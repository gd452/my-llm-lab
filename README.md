# ğŸš€ My LLM Lab: ë‚˜ë§Œì˜ Tiny LLM ë§Œë“¤ê¸°

> "The best way to understand something is to build it from scratch" - Andrej Karpathy

## ğŸ¯ ëª©í‘œ
ë‚˜ë§Œì˜ ì•„ì£¼ ê°„ë‹¨í•œ tiny LLMì„ í†µí•´ ì£¼ìš” LLMì„ ì´í•´í•˜ê³ , LLM ì „ë°˜ì„ ì´í•´í•˜ëŠ” expertê°€ ë˜ê¸° ìœ„í•œ ê¸°ì´ˆ ê³¼ì •

## ğŸ“š ì „ì²´ í•™ìŠµ ê²½ë¡œ

### Week 1: ê¸°ì´ˆ ë‹¤ì§€ê¸°

#### âœ… Day 1: Tiny Autograd Package
- **ìƒíƒœ**: ì™„ë£Œ
- **í´ë”**: `tiny_autograd_project/`
- **ë‚´ìš©**: 
  - ìë™ ë¯¸ë¶„ êµ¬í˜„
  - ê³„ì‚° ê·¸ë˜í”„ì™€ ì—­ì „íŒŒ
  - Value í´ë˜ìŠ¤ êµ¬í˜„
  - ìœ„ìƒ ì •ë ¬ê³¼ Chain Rule

#### âœ… Day 1.5: Neural Network ê¸°ì´ˆ
- **ìƒíƒœ**: ì™„ë£Œ
- **í´ë”**: `projects/day1_5_neural_net/`
- **ë‚´ìš©**:
  - Neuron, Layer, MLP êµ¬í˜„
  - XOR ë¬¸ì œ í•´ê²°
  - ê²½ì‚¬í•˜ê°•ë²•ê³¼ ìµœì í™”
  - í™œì„±í™” í•¨ìˆ˜ì™€ ì†ì‹¤ í•¨ìˆ˜

#### âœ… Day 2: ë²¡í„°/í–‰ë ¬ ì—°ì‚°
- **ìƒíƒœ**: ì™„ë£Œ
- **í´ë”**: `projects/day2_vector_matrix/`
- **ë‚´ìš©**:
  - NumPyë¡œ íš¨ìœ¨ì ì¸ ì—°ì‚°
  - Batch ì²˜ë¦¬
  - Softmaxì™€ CrossEntropy
  - í–‰ë ¬ê³±ê³¼ Broadcasting
  - Mini MNIST ë¶„ë¥˜ê¸°

#### âœ… Day 3: Attention ë©”ì»¤ë‹ˆì¦˜
- **ìƒíƒœ**: ì™„ë£Œ
- **í´ë”**: `projects/day3_attention/`
- **ë‚´ìš©**:
  - Self-Attention êµ¬í˜„
  - Query, Key, Value ì´í•´
  - Scaled Dot-Product Attention
  - Multi-Head Attention
  - Positional Encoding

#### âœ… Day 4: Transformer Architecture
- **ìƒíƒœ**: ì™„ë£Œ
- **í´ë”**: `projects/day4_transformer/`
- **ë‚´ìš©**:
  - ì „ì²´ Transformer ì•„í‚¤í…ì²˜ êµ¬í˜„
  - Encoderì™€ Decoder êµ¬ì¡°
  - Multi-Head Attention í†µí•©
  - Positional Encoding
  - Layer Normalization
  - Feed-Forward Networks
  - PyTorchë¡œ ì‹¤ì œ êµ¬í˜„

### Week 2: ì‹¤ì „ LLM êµ¬í˜„

#### ğŸš€ miniGPT - nanoGPT ìŠ¤íƒ€ì¼ êµ¬í˜„
- **ìƒíƒœ**: ì§„í–‰ì¤‘
- **í´ë”**: `projects/mini_gpt/`
- **í•µì‹¬ êµ¬í˜„**:
  - Karpathy's nanoGPT ìŠ¤íƒ€ì¼ (~200ì¤„)
  - Self-Attention, Causal Masking
  - Character-level ì–¸ì–´ ëª¨ë¸
  - ì…°ìµìŠ¤í”¼ì–´ í…ìŠ¤íŠ¸ í•™ìŠµ
  - Attention íŒ¨í„´ ì‹œê°í™”
- **ì‹¤í–‰**: `cd projects/mini_gpt && python train.py`

#### ğŸ¤– ì˜¤í”ˆì†ŒìŠ¤ LLM ì‹¤ìŠµ - Qwen2
- **ìƒíƒœ**: ì§„í–‰ì¤‘
- **í´ë”**: `projects/llm_practice/`
- **ì‹¤ìŠµ ë‚´ìš©**:
  - Ollamaë¡œ ë¡œì»¬ LLM ì‹¤í–‰
  - Streaming ì‘ë‹µ êµ¬í˜„
  - RAG ì‹œìŠ¤í…œ ê¸°ì´ˆ
  - Fine-tuning with LoRA
  - ì½”ë“œ ë¦¬ë·°/ë²ˆì—­/ìš”ì•½ ë´‡
- **ì‹¤í–‰**: `ollama run qwen2:7b`

### Week 3: ì‹¤ë¬´ í”„ë¡œì íŠ¸

#### ğŸ“ ë‚˜ë§Œì˜ ë©”ëª¨ ë¹„ì„œ
- **ê³„íšì¤‘**: ê°œì¸ ë…¸íŠ¸ + Qwen = AI ì–´ì‹œìŠ¤í„´íŠ¸
- Vector DBë¡œ ì§€ì‹ ê´€ë¦¬
- Context-aware ì‘ë‹µ
- ì‹¤ì‹œê°„ ë©”ëª¨ ê²€ìƒ‰ & ìš”ì•½

#### â³ Day 16-20: ë„ë©”ì¸ íŠ¹í™” LLM
- **ì˜ˆì • ë‚´ìš©**:
  - íŠ¹ì • ë„ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘
  - ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €
  - ë„ë©”ì¸ íŠ¹í™” íŒŒì¸íŠœë‹
  - ì„±ëŠ¥ í‰ê°€ ë° ê°œì„ 

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ
- Python 3.8+
- NumPy (ë²¡í„° ì—°ì‚°)
- PyTorch (Week 2ë¶€í„° ì„ íƒì )
- Jupyter Notebook (ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ)

## ğŸ“– í•™ìŠµ ë°©ë²•

### ê° Dayë³„ ì§„í–‰ ìˆœì„œ:
1. **ê°œë… í•™ìŠµ**: `study_notes/` ì½ê¸°
2. **íŠœí† ë¦¬ì–¼**: `notebooks/` ë”°ë¼í•˜ê¸°
3. **êµ¬í˜„**: ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ ì™„ì„±
4. **í…ŒìŠ¤íŠ¸**: pytestë¡œ ê²€ì¦
5. **ì‹¤ìŠµ**: ë°ëª¨ ì‹¤í–‰ ë° ì‹¤í—˜

### ê¶Œì¥ í•™ìŠµ ì‹œê°„:
- í•˜ë£¨ 2-4ì‹œê°„
- ê° Day ì™„ë£Œ í›„ ì¶©ë¶„í•œ ë³µìŠµ
- ì´í•´ê°€ ì•ˆ ë˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì§€ ë§ ê²ƒ

## ğŸ“ ì„ ìˆ˜ ì§€ì‹
- Python ê¸°ì´ˆ ë¬¸ë²•
- ê³ ë“±í•™êµ ìˆ˜ì¤€ ìˆ˜í•™ (ë¯¸ë¶„)
- í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ê°œë…

## ğŸ’¡ í•µì‹¬ ì›ì¹™
1. **ì²˜ìŒë¶€í„° êµ¬í˜„**: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ìµœì†Œí™”
2. **ì´í•´ ì¤‘ì‹¬**: ì•”ê¸°ë³´ë‹¤ ì›ë¦¬ ì´í•´
3. **ì ì§„ì  í•™ìŠµ**: ì‘ì€ ê²ƒë¶€í„° ì°¨ê·¼ì°¨ê·¼
4. **ì‹¤ìŠµ ìœ„ì£¼**: ì½”ë“œë¡œ ì§ì ‘ í™•ì¸

## ğŸ”— ì°¸ê³  ìë£Œ
- [Andrej Karpathy - Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## ğŸ“ ì§„í–‰ ìƒí™©

### âœ… Week 1: ê¸°ì´ˆ ì™„ë£Œ
- [x] Day 1: Autograd êµ¬í˜„
- [x] Day 1.5: Neural Network ê¸°ì´ˆ
- [x] Day 2: Vector/Matrix Operations
- [x] Day 3: Attention Mechanism
- [x] Day 4: Transformer Architecture (PyTorch)

### ğŸš€ Week 2: ì‹¤ì „ êµ¬í˜„ (ì§„í–‰ì¤‘)
- [x] miniGPT êµ¬í˜„ (nanoGPT style)
- [ ] Qwen2 ëª¨ë¸ ì‹¤ìŠµ
- [ ] Fine-tuning & LoRA
- [ ] RAG ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸ“… Week 3: í”„ë¡œì íŠ¸ (ì˜ˆì •)
- [ ] ë‚˜ë§Œì˜ ë©”ëª¨ ë¹„ì„œ ë§Œë“¤ê¸°

## ğŸš¦ í˜„ì¬ ì§„í–‰
**miniGPT í•™ìŠµì¤‘** â†’ Qwen2 ì‹¤ìŠµ ì˜ˆì •
```bash
# miniGPT í•™ìŠµ
cd projects/mini_gpt && python train.py

# Qwen2 ì‹¤ìŠµ
ollama pull qwen2:7b
cd projects/llm_practice/01_ollama
python basic_chat.py
```

---

**"ì‘ì€ ê±¸ìŒì´ ëª¨ì—¬ í° ë„ì•½ì´ ë©ë‹ˆë‹¤. í¬ê¸°í•˜ì§€ ë§ˆì„¸ìš”!"**