# π§  Day 1.5: Neural Network κΈ°μ΄ - From Neuron to MLP

> "μ°λ¦¬λ” λ‡μ λ‰΄λ°μ²λΌ μ‘λ™ν•λ” μΈκ³µ λ‰΄λ°μ„ λ§λ“¤μ–΄λ΄…λ‹λ‹¤" - Andrej Karpathy μ¤νƒ€μΌ

## π― ν•™μµ λ©ν‘

μ΄ ν”„λ΅μ νΈλ¥Ό μ™„λ£ν•λ©΄ λ‹¤μμ„ μ΄ν•΄ν•κ² λ©λ‹λ‹¤:
- π”Έ **λ‰΄λ°**: κ°€μ¥ μ‘μ€ ν•™μµ λ‹¨μ„
- π”Έ **λ μ΄μ–΄**: λ‰΄λ°λ“¤μ μ΅°μ§ν™”
- π”Έ **MLP**: λ‹¤μΈµ νΌμ…‰νΈλ΅ μ κµ¬μ΅°
- π”Έ **ν•™μµ**: κ²½μ‚¬ν•κ°•λ²•κ³Ό μ—­μ „ν
- π”Έ **XOR λ¬Έμ **: μ‹ κ²½λ§μ Hello World

## π“ μ„ μ μ§€μ‹

- β… Tiny Autograd μ™„λ£ (μ—­μ „ν μ΄ν•΄)
- β… Python κΈ°μ΄
- β… κ³ λ“±ν•™κµ μν•™ (λ―Έλ¶„)

## π—‚οΈ ν”„λ΅μ νΈ κµ¬μ΅°

```
tiny_neural_net/
β”β”€β”€ _10_core/              # ν•µμ‹¬ κµ¬ν„ (λ°‘μ¤„λ΅ μ‹μ‘ν•΄μ„ import μ©μ΄)
β”‚   β””β”€β”€ nn_tiny/
β”‚       β”β”€β”€ neuron.py      # λ‹¨μΌ λ‰΄λ° κµ¬ν„
β”‚       β”β”€β”€ layer.py       # λ μ΄μ–΄ (λ‰΄λ° μ§‘ν•©)
β”‚       β”β”€β”€ mlp.py         # Multi-Layer Perceptron
β”‚       β”β”€β”€ losses.py      # μ†μ‹¤ ν•¨μ (MSE, CrossEntropy)
β”‚       β””β”€β”€ optimizer.py   # μµμ ν™” μ•κ³ λ¦¬μ¦ (SGD)
β”β”€β”€ notebooks/
β”‚   β””β”€β”€ nn_tutorial.ipynb # μƒμ„Έ νν† λ¦¬μ–Ό
β”β”€β”€ tests/
β”‚   β””β”€β”€ test_nn.py        # λ‹¨μ„ ν…μ¤νΈ
β”β”€β”€ 50_eval/
β”‚   β””β”€β”€ xor_demo.py       # XOR λ¬Έμ  ν•΄κ²°
β””β”€β”€ study_notes/
    β””β”€β”€ nn_concepts.md    # ν•µμ‹¬ κ°λ… μ •λ¦¬
```

## π€ λΉ λ¥Έ μ‹μ‘

### 1. ν™κ²½ μ„¤μ •
```bash
cd tiny_neural_net
pip install -r requirements.txt
```

### 2. ν…μ¤νΈ μ‹¤ν–‰ (ν„μ¬ μ‹¤ν¨ - TODO κµ¬ν„ ν•„μ”)
```bash
python -m pytest tests/ -v
```

### 3. λ…ΈνΈλ¶μΌλ΅ ν•™μµ
```bash
jupyter notebook notebooks/nn_tutorial.ipynb
```

### 4. XOR λ¬Έμ  λ„μ „
```bash
python 50_eval/xor_demo.py
```

## π“ κµ¬ν„ μ²΄ν¬λ¦¬μ¤νΈ

### Stage 1: λ‰΄λ° (1μ‹κ°„)
- [ ] `Neuron.__init__`: κ°€μ¤‘μΉμ™€ νΈν–¥ μ΄κΈ°ν™”
- [ ] `Neuron.__call__`: forward pass (wx + b)
- [ ] `Neuron.parameters()`: ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° λ°ν™

### Stage 2: λ μ΄μ–΄ (30λ¶„)
- [ ] `Layer.__init__`: nκ°μ λ‰΄λ° μƒμ„±
- [ ] `Layer.__call__`: λ¨λ“  λ‰΄λ°μ— μ…λ ¥ μ „λ‹¬
- [ ] `Layer.parameters()`: λ¨λ“  λ‰΄λ°μ νλΌλ―Έν„°

### Stage 3: MLP (1μ‹κ°„)
- [ ] `MLP.__init__`: λ‹¤μΈµ κµ¬μ΅° μƒμ„±
- [ ] `MLP.__call__`: μμ°¨μ  forward pass
- [ ] `MLP.parameters()`: μ „μ²΄ λ„¤νΈμ›ν¬ νλΌλ―Έν„°

### Stage 4: ν•™μµ (1μ‹κ°„)
- [ ] `mse_loss`: Mean Squared Error
- [ ] `SGD.step()`: νλΌλ―Έν„° μ—…λ°μ΄νΈ
- [ ] `SGD.zero_grad()`: gradient μ΄κΈ°ν™”

### Stage 5: XOR ν•΄κ²° (1μ‹κ°„)
- [ ] λ°μ΄ν„° μ¤€λΉ„
- [ ] λ„¤νΈμ›ν¬ μƒμ„± (2-4-1 κµ¬μ΅°)
- [ ] ν•™μµ λ£¨ν”„
- [ ] κ²°κ³Ό μ‹κ°ν™”

## π“ ν•µμ‹¬ κ°λ…

### 1. λ‰΄λ°μ μν•™
```python
# λ‹¨μΌ λ‰΄λ°μ κ³„μ‚°
output = activation(w1*x1 + w2*x2 + ... + b)

# μ°λ¦¬μ κµ¬ν„ (Value ν΄λμ¤ μ‚¬μ©)
output = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b
output = output.tanh()  # ν™μ„±ν™” ν•¨μ
```

### 2. XOR λ¬Έμ 
```
μ…λ ¥: (0,0) β†’ μ¶λ ¥: 0
μ…λ ¥: (0,1) β†’ μ¶λ ¥: 1
μ…λ ¥: (1,0) β†’ μ¶λ ¥: 1
μ…λ ¥: (1,1) β†’ μ¶λ ¥: 0

μ„ ν•μΌλ΅ λ¶„λ¦¬ λ¶κ°€λ¥ β†’ μ€λ‹‰μΈµ ν•„μ”!
```

### 3. ν•™μµ κ³Όμ •
```python
for epoch in range(1000):
    # Forward pass
    y_pred = model(X)
    
    # Compute loss
    loss = mse_loss(y_pred, y_true)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    
    # Update parameters
    optimizer.step()
```

## π” λ””λ²„κΉ… ν

1. **Gradient ν™•μΈ**: λ¨λ“  νλΌλ―Έν„°μ gradκ°€ 0μ΄ μ•„λ‹μ§€ ν™•μΈ
2. **Loss μ¶”μ **: ν•™μµ μ¤‘ lossκ°€ κ°μ†ν•λ”μ§€ λ¨λ‹ν„°λ§
3. **ν•™μµλ¥ **: λ„λ¬΄ ν¬λ©΄ λ°μ‚°, λ„λ¬΄ μ‘μΌλ©΄ μλ ΄ λλ¦Ό
4. **μ΄κΈ°ν™”**: κ°€μ¤‘μΉ μ΄κΈ°ν™”κ°€ μ¤‘μ” (Xavier, He μ΄κΈ°ν™”)

## π“ κΈ°λ€ κ²°κ³Ό

### XOR ν•™μµ κ³΅μ„ 
```
Epoch 0: Loss = 0.25
Epoch 100: Loss = 0.18
Epoch 500: Loss = 0.05
Epoch 1000: Loss = 0.001
```

### μ •ν™•λ„
```
(0, 0) β†’ 0.02 β‰ 0 β“
(0, 1) β†’ 0.98 β‰ 1 β“
(1, 0) β†’ 0.97 β‰ 1 β“
(1, 1) β†’ 0.03 β‰ 0 β“
```

## π’΅ λ„μ „ κ³Όμ 

### Level 1: κΈ°λ³Έ
- [ ] AND, OR κ²μ΄νΈ ν•™μµ
- [ ] λ‹¤λ¥Έ ν™μ„±ν™” ν•¨μ (ReLU, Sigmoid)

### Level 2: μ¤‘κΈ‰
- [ ] 3-bit parity λ¬Έμ 
- [ ] μ›ν• λ°μ΄ν„° λ¶„λ¥

### Level 3: κ³ κΈ‰
- [ ] MNIST μ«μ μΈμ‹ (784-128-10)
- [ ] μ •κ·ν™” κΈ°λ²• (L2, Dropout)

## π”— μ°Έκ³  μλ£

- [Andrej Karpathy - micrograd](https://github.com/karpathy/micrograd)
- [Neural Networks from Scratch](https://www.youtube.com/watch?v=VMj-3S1tku0)
- [XOR Problem Visualization](https://playground.tensorflow.org/)

## β° μμƒ μ†μ” μ‹κ°„

- **μ΄ μ‹κ°„**: 4-6μ‹κ°„
- **κ¶μ¥ μ§„ν–‰**:
  - 1μ‹κ°„: λ…ΈνΈλ¶ λ”°λΌν•κΈ°
  - 2μ‹κ°„: μ§μ ‘ κµ¬ν„
  - 1μ‹κ°„: XOR λ¬Έμ  ν•΄κ²°
  - 1-2μ‹κ°„: λ„μ „ κ³Όμ 

## π― λ‹¤μ λ‹¨κ³„

μ΄ ν”„λ΅μ νΈλ¥Ό μ™„λ£ν•λ©΄:
- **Day 2**: λ²΅ν„°/ν–‰λ ¬ μ—°μ‚°μΌλ΅ ν™•μ¥
- **Day 3**: Attention λ©”μ»¤λ‹μ¦ μ΄ν•΄
- **Day 4**: Transformer κµ¬ν„
- **Day 5**: μ‹¤μ  ν…μ¤νΈ μƒμ„±

---

**"The key to understanding deep learning is to build it from scratch!"** - Andrej Karpathy