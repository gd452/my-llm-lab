{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Neural Network Tutorial: From Neuron to MLP\n",
    "\n",
    "**\"ìš°ë¦¬ëŠ” ë‡Œë¥¼ ëª¨ë°©í•œ ì¸ê³µ ì‹ ê²½ë§ì„ ì²˜ìŒë¶€í„° ë§Œë“¤ì–´ë´…ë‹ˆë‹¤\"**\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ”:\n",
    "1. ğŸ”¸ **Neuron**: ê°€ì¥ ì‘ì€ í•™ìŠµ ë‹¨ìœ„\n",
    "2. ğŸ”² **Layer**: ë‰´ëŸ°ë“¤ì˜ ì§‘í•©\n",
    "3. ğŸ—ï¸ **MLP**: ë‹¤ì¸µ ì‹ ê²½ë§\n",
    "4. ğŸ¯ **XOR**: ì‹ ê²½ë§ì˜ Hello World\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Part 0: ì¤€ë¹„\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ ëª¨ë“ˆì„ importí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# Tiny Autograd import\n",
    "from tiny_autograd_project._10_core.autograd_tiny.value import Value\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¸ Part 1: Neuron - ê°€ì¥ ì‘ì€ ë‹¨ìœ„\n",
    "\n",
    "### ìƒë¬¼í•™ì  ë‰´ëŸ° vs ì¸ê³µ ë‰´ëŸ°\n",
    "\n",
    "ìƒë¬¼í•™ì  ë‰´ëŸ°:\n",
    "- ìˆ˜ìƒëŒê¸°(Dendrites): ì…ë ¥ ë°›ê¸°\n",
    "- ì„¸í¬ì²´(Cell Body): ì‹ í˜¸ ì²˜ë¦¬\n",
    "- ì¶•ì‚­(Axon): ì¶œë ¥ ì „ë‹¬\n",
    "\n",
    "ì¸ê³µ ë‰´ëŸ°:\n",
    "- ì…ë ¥: xâ‚, xâ‚‚, ..., xâ‚™\n",
    "- ê°€ì¤‘ì¹˜: wâ‚, wâ‚‚, ..., wâ‚™\n",
    "- í¸í–¥: b\n",
    "- í™œì„±í™”: f(Î£wáµ¢xáµ¢ + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    \"\"\"ê°„ë‹¨í•œ ë‰´ëŸ° êµ¬í˜„ (êµìœ¡ìš©)\"\"\"\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        \"\"\"ninê°œì˜ ì…ë ¥ì„ ë°›ëŠ” ë‰´ëŸ° ìƒì„±\"\"\"\n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (-1 ~ 1 ì‚¬ì´ ëœë¤)\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        # í¸í–¥ ì´ˆê¸°í™” (0ìœ¼ë¡œ ì‹œì‘)\n",
    "        self.b = Value(0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass: ì…ë ¥ì„ ë°›ì•„ ì¶œë ¥ ìƒì„±\"\"\"\n",
    "        # ê°€ì¤‘í•© ê³„ì‚°: Î£(wi * xi) + b\n",
    "        activation = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # í™œì„±í™” í•¨ìˆ˜ (tanh) ì ìš©\n",
    "        output = activation.tanh()\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ë°˜í™˜\"\"\"\n",
    "        return self.w + [self.b]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "neuron = SimpleNeuron(2)\n",
    "x = [Value(0.5), Value(-0.5)]\n",
    "y = neuron(x)\n",
    "\n",
    "print(f\"ì…ë ¥: {[xi.data for xi in x]}\")\n",
    "print(f\"ì¶œë ¥: {y.data:.4f}\")\n",
    "print(f\"íŒŒë¼ë¯¸í„° ê°œìˆ˜: {len(neuron.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ë‰´ëŸ°ì˜ í•™ìŠµ ê³¼ì • ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í•™ìŠµ ì˜ˆì œ: ë‰´ëŸ°ì´ AND ê²Œì´íŠ¸ í•™ìŠµ\n",
    "def train_single_neuron():\n",
    "    # AND ê²Œì´íŠ¸ ë°ì´í„°\n",
    "    X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "    y = [0, 0, 0, 1]  # AND ì¶œë ¥\n",
    "    \n",
    "    # ë‰´ëŸ° ìƒì„±\n",
    "    neuron = SimpleNeuron(2)\n",
    "    \n",
    "    # í•™ìŠµ ê¸°ë¡\n",
    "    losses = []\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, target in zip(X, y):\n",
    "            # Forward\n",
    "            x_vals = [Value(x) for x in inputs]\n",
    "            pred = neuron(x_vals)\n",
    "            \n",
    "            # Loss\n",
    "            loss = (pred - Value(target)) ** 2\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Backward\n",
    "            for p in neuron.parameters():\n",
    "                p.grad = 0\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update (SGD)\n",
    "            for p in neuron.parameters():\n",
    "                p.data -= 0.1 * p.grad\n",
    "        \n",
    "        losses.append(total_loss / 4)\n",
    "    \n",
    "    # ê²°ê³¼ ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('í•™ìŠµ ê³¡ì„ ')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # ìµœì¢… ì˜ˆì¸¡\n",
    "    for inputs, target in zip(X, y):\n",
    "        x_vals = [Value(x) for x in inputs]\n",
    "        pred = neuron(x_vals)\n",
    "        plt.scatter(inputs[0], inputs[1], \n",
    "                   c='green' if pred.data > 0.5 else 'red',\n",
    "                   s=100, edgecolor='black')\n",
    "        plt.text(inputs[0], inputs[1], f'{pred.data:.2f}', \n",
    "                ha='center', va='center')\n",
    "    \n",
    "    plt.title('AND ê²Œì´íŠ¸ ì˜ˆì¸¡')\n",
    "    plt.xlabel('Input 1')\n",
    "    plt.ylabel('Input 2')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return neuron\n",
    "\n",
    "# ì‹¤í–‰\n",
    "trained_neuron = train_single_neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”² Part 2: Layer - ë‰´ëŸ°ë“¤ì˜ ì§‘í•©\n",
    "\n",
    "LayerëŠ” ì—¬ëŸ¬ ë‰´ëŸ°ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "ê° ë‰´ëŸ°ì€ ê°™ì€ ì…ë ¥ì„ ë°›ì§€ë§Œ, ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLayer:\n",
    "    \"\"\"ë‰´ëŸ°ë“¤ì˜ ì§‘í•©\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"nin ì…ë ¥ì„ ë°›ì•„ nout ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ì¸µ\"\"\"\n",
    "        self.neurons = [SimpleNeuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ëª¨ë“  ë‰´ëŸ°ì— ì…ë ¥ ì „ë‹¬\"\"\"\n",
    "        outputs = [neuron(x) for neuron in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"ëª¨ë“  ë‰´ëŸ°ì˜ íŒŒë¼ë¯¸í„°\"\"\"\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "layer = SimpleLayer(3, 2)  # 3ì…ë ¥ â†’ 2ì¶œë ¥\n",
    "x = [Value(1), Value(2), Value(3)]\n",
    "y = layer(x)\n",
    "\n",
    "print(f\"ì…ë ¥ ì°¨ì›: 3\")\n",
    "print(f\"ì¶œë ¥ ì°¨ì›: 2\")\n",
    "print(f\"ì¶œë ¥ê°’: {[yi.data for yi in y]}\")\n",
    "print(f\"ì´ íŒŒë¼ë¯¸í„°: {len(layer.parameters())}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Part 3: MLP - Multi-Layer Perceptron\n",
    "\n",
    "ì—¬ëŸ¬ ì¸µì„ ì—°ê²°í•˜ì—¬ ê¹Šì€ ì‹ ê²½ë§ì„ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    \"\"\"ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ \"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"nin ì…ë ¥, nouts=[n1, n2, ...] ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜\"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(nouts)):\n",
    "            self.layers.append(SimpleLayer(sz[i], sz[i+1]))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ì¸µ í†µê³¼\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"ëª¨ë“  ì¸µì˜ íŒŒë¼ë¯¸í„°\"\"\"\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸: 2-4-4-1 êµ¬ì¡°\n",
    "mlp = SimpleMLP(2, [4, 4, 1])\n",
    "x = [Value(0.5), Value(0.5)]\n",
    "y = mlp(x)\n",
    "\n",
    "print(f\"ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°: 2 â†’ 4 â†’ 4 â†’ 1\")\n",
    "print(f\"ì…ë ¥: {[xi.data for xi in x]}\")\n",
    "print(f\"ì¶œë ¥: {y.data}\")\n",
    "print(f\"ì´ íŒŒë¼ë¯¸í„°: {len(mlp.parameters())}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 4: XOR Problem - ì‹ ê²½ë§ì˜ Hello World\n",
    "\n",
    "XORì€ ì„ í˜•ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\n",
    "ì€ë‹‰ì¸µì´ ìˆëŠ” ì‹ ê²½ë§ì´ í•„ìš”í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR ë°ì´í„°\n",
    "X_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_xor = [0, 1, 1, 0]\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(6, 6))\n",
    "for inputs, target in zip(X_xor, y_xor):\n",
    "    color = 'blue' if target == 1 else 'red'\n",
    "    plt.scatter(inputs[0], inputs[1], c=color, s=200, edgecolor='black', linewidth=2)\n",
    "    plt.text(inputs[0], inputs[1], f'{target}', \n",
    "            ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "\n",
    "plt.title('XOR Problem', fontsize=16)\n",
    "plt.xlabel('Input 1', fontsize=14)\n",
    "plt.ylabel('Input 2', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"ë¹¨ê°•(0)ê³¼ íŒŒë‘(1)ì„ ì„  í•˜ë‚˜ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?\")\n",
    "print(\"â†’ ë¶ˆê°€ëŠ¥! ì€ë‹‰ì¸µì´ í•„ìš”í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ XOR í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_network(epochs=1000, lr=0.5, hidden_size=4):\n",
    "    \"\"\"XOR ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì‹ ê²½ë§ í•™ìŠµ\"\"\"\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = SimpleMLP(2, [hidden_size, 1])\n",
    "    print(f\"ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°: 2 â†’ {hidden_size} â†’ 1\")\n",
    "    print(f\"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {len(model.parameters())}ê°œ\\n\")\n",
    "    \n",
    "    # í•™ìŠµ ê¸°ë¡\n",
    "    history = []\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, target in zip(X_xor, y_xor):\n",
    "            # Forward pass\n",
    "            x_vals = [Value(x) for x in inputs]\n",
    "            pred = model(x_vals)\n",
    "            \n",
    "            # Loss ê³„ì‚°\n",
    "            loss = (pred - Value(target)) ** 2\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Backward pass\n",
    "            for p in model.parameters():\n",
    "                p.grad = 0\n",
    "            loss.backward()\n",
    "            \n",
    "            # Parameter update (SGD)\n",
    "            for p in model.parameters():\n",
    "                p.data -= lr * p.grad\n",
    "        \n",
    "        avg_loss = total_loss / len(X_xor)\n",
    "        history.append(avg_loss)\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d}: Loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "print(\"ğŸ¯ XOR í•™ìŠµ ì‹œì‘...\\n\")\n",
    "xor_model, loss_history = train_xor_network(epochs=1000, lr=0.5, hidden_size=4)\n",
    "print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. í•™ìŠµ ê³¡ì„ \n",
    "axes[0].plot(loss_history, linewidth=2)\n",
    "axes[0].set_title('í•™ìŠµ ê³¡ì„ ', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# 2. ìµœì¢… ì˜ˆì¸¡\n",
    "predictions = []\n",
    "for inputs in X_xor:\n",
    "    x_vals = [Value(x) for x in inputs]\n",
    "    pred = xor_model(x_vals)\n",
    "    predictions.append(pred.data)\n",
    "\n",
    "for i, (inputs, target, pred) in enumerate(zip(X_xor, y_xor, predictions)):\n",
    "    color = 'green' if abs(pred - target) < 0.5 else 'red'\n",
    "    axes[1].scatter(inputs[0], inputs[1], c=color, s=200, edgecolor='black', linewidth=2)\n",
    "    axes[1].text(inputs[0], inputs[1], f'{pred:.2f}', \n",
    "                ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].set_title('ëª¨ë¸ ì˜ˆì¸¡', fontsize=14)\n",
    "axes[1].set_xlabel('Input 1')\n",
    "axes[1].set_ylabel('Input 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-0.5, 1.5)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "# 3. ê²°ì • ê²½ê³„\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 50),\n",
    "                     np.linspace(-0.5, 1.5, 50))\n",
    "Z = []\n",
    "for i in range(len(xx.ravel())):\n",
    "    x_val = [Value(xx.ravel()[i]), Value(yy.ravel()[i])]\n",
    "    pred = xor_model(x_val)\n",
    "    Z.append(pred.data)\n",
    "\n",
    "Z = np.array(Z).reshape(xx.shape)\n",
    "\n",
    "contour = axes[2].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "axes[2].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "for inputs, target in zip(X_xor, y_xor):\n",
    "    color = 'blue' if target == 1 else 'red'\n",
    "    axes[2].scatter(inputs[0], inputs[1], c=color, s=100, \n",
    "                   edgecolor='white', linewidth=2, zorder=5)\n",
    "\n",
    "axes[2].set_title('ê²°ì • ê²½ê³„', fontsize=14)\n",
    "axes[2].set_xlabel('Input 1')\n",
    "axes[2].set_ylabel('Input 2')\n",
    "axes[2].set_xlim(-0.5, 1.5)\n",
    "axes[2].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "correct = sum(1 for pred, target in zip(predictions, y_xor) \n",
    "             if (pred > 0.5) == (target == 1))\n",
    "accuracy = correct / len(y_xor) * 100\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
    "print(\"=\"*40)\n",
    "for inputs, target, pred in zip(X_xor, y_xor, predictions):\n",
    "    pred_class = 1 if pred > 0.5 else 0\n",
    "    symbol = \"âœ“\" if pred_class == target else \"âœ—\"\n",
    "    print(f\"{inputs} â†’ ì˜ˆì¸¡: {pred:.4f} (í´ë˜ìŠ¤: {pred_class}), ì •ë‹µ: {target} {symbol}\")\n",
    "print(\"=\"*40)\n",
    "print(f\"ğŸ¯ ì •í™•ë„: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Part 5: í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "### ì™œ XORì´ ì¤‘ìš”í•œê°€?\n",
    "\n",
    "1. **ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥**: ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” í•´ê²° ë¶ˆê°€\n",
    "2. **ì€ë‹‰ì¸µì˜ í•„ìš”ì„±**: ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•œ íŠ¹ì§• í•™ìŠµ\n",
    "3. **í‘œí˜„ë ¥**: ì€ë‹‰ì¸µì´ ìˆìœ¼ë©´ ì–´ë–¤ í•¨ìˆ˜ë„ ê·¼ì‚¬ ê°€ëŠ¥ (Universal Approximation)\n",
    "\n",
    "### í•™ìŠµì˜ í•µì‹¬ ìš”ì†Œ\n",
    "\n",
    "1. **Forward Pass**: ì…ë ¥ â†’ ì¶œë ¥ ê³„ì‚°\n",
    "2. **Loss Function**: ì˜ˆì¸¡ê³¼ ì •ë‹µì˜ ì°¨ì´ ì¸¡ì •\n",
    "3. **Backward Pass**: Chain Ruleë¡œ gradient ê³„ì‚°\n",
    "4. **Parameter Update**: Gradient descentë¡œ ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Part 6: ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "ì´ì œ `10_core/nn_tiny/` í´ë”ì˜ íŒŒì¼ë“¤ì„ êµ¬í˜„í•´ë³´ì„¸ìš”!\n",
    "\n",
    "### êµ¬í˜„ ìˆœì„œ:\n",
    "1. âœ… `neuron.py` - Neuron í´ë˜ìŠ¤\n",
    "2. âœ… `layer.py` - Layer í´ë˜ìŠ¤\n",
    "3. âœ… `mlp.py` - MLP í´ë˜ìŠ¤\n",
    "4. âœ… `losses.py` - ì†ì‹¤ í•¨ìˆ˜\n",
    "5. âœ… `optimizer.py` - SGD ìµœì í™”ê¸°\n",
    "\n",
    "### í…ŒìŠ¤íŠ¸:\n",
    "```bash\n",
    "# êµ¬í˜„ í›„ í…ŒìŠ¤íŠ¸\n",
    "python -m pytest tests/test_nn.py -v\n",
    "\n",
    "# XOR ë°ëª¨ ì‹¤í–‰\n",
    "python 50_eval/xor_demo.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ† ë„ì „ ê³¼ì œ\n",
    "\n",
    "### Level 1: ë‹¤ë¥¸ ë…¼ë¦¬ ê²Œì´íŠ¸\n",
    "- AND, OR, NAND ê²Œì´íŠ¸ í•™ìŠµ\n",
    "- í•„ìš”í•œ ìµœì†Œ ë‰´ëŸ° ìˆ˜ëŠ”?\n",
    "\n",
    "### Level 2: ë” ë³µì¡í•œ íŒ¨í„´\n",
    "- 3-bit parity ë¬¸ì œ\n",
    "- ì›í˜• ë°ì´í„° ë¶„ë¥˜\n",
    "\n",
    "### Level 3: ì‹¤ì œ ë°ì´í„°\n",
    "- Iris ë°ì´í„°ì…‹\n",
    "- MNIST (784-128-10 êµ¬ì¡°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„ì „ ê³¼ì œ ì˜ˆì‹œ: ì›í˜• ë°ì´í„° ë¶„ë¥˜\n",
    "def create_circular_data(n_samples=100):\n",
    "    \"\"\"ì›í˜•ìœ¼ë¡œ ë¶„í¬í•œ 2í´ë˜ìŠ¤ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # ëœë¤ ì  ìƒì„±\n",
    "        x1 = random.uniform(-2, 2)\n",
    "        x2 = random.uniform(-2, 2)\n",
    "        \n",
    "        # ì›ì˜ ì•ˆ/ë°–ìœ¼ë¡œ ë¶„ë¥˜\n",
    "        distance = math.sqrt(x1**2 + x2**2)\n",
    "        label = 1 if distance < 1.0 else 0\n",
    "        \n",
    "        X.append([x1, x2])\n",
    "        y.append(label)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# ë°ì´í„° ìƒì„± ë° ì‹œê°í™”\n",
    "X_circle, y_circle = create_circular_data(200)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for x, label in zip(X_circle, y_circle):\n",
    "    color = 'blue' if label == 1 else 'red'\n",
    "    plt.scatter(x[0], x[1], c=color, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='green', linewidth=2, linestyle='--')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.title('ì›í˜• ë¶„ë¥˜ ë¬¸ì œ', fontsize=16)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"ë„ì „: ì´ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!\")\n",
    "print(\"íŒíŠ¸: ì€ë‹‰ì¸µì˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (ì˜ˆ: 2-16-16-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
    "\n",
    "ì‹ ê²½ë§ì˜ ê¸°ì´ˆë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤! ğŸŠ\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "- **Day 2**: ë²¡í„°/í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ í™•ì¥\n",
    "- **Day 3**: Attention ë©”ì»¤ë‹ˆì¦˜\n",
    "- **Day 4**: Transformer êµ¬í˜„\n",
    "- **Day 5**: ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "**\"The best way to understand neural networks is to implement them from scratch!\"**\n",
    "- Andrej Karpathy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}