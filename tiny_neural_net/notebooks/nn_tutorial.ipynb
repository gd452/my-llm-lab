{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Neural Network Tutorial: From Neuron to MLP\n",
    "\n",
    "**\"우리는 뇌를 모방한 인공 신경망을 처음부터 만들어봅니다\"**\n",
    "\n",
    "이 노트북에서는:\n",
    "1. 🔸 **Neuron**: 가장 작은 학습 단위\n",
    "2. 🔲 **Layer**: 뉴런들의 집합\n",
    "3. 🏗️ **MLP**: 다층 신경망\n",
    "4. 🎯 **XOR**: 신경망의 Hello World\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Part 0: 준비\n",
    "\n",
    "먼저 필요한 모듈을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 프로젝트 경로 추가\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# Tiny Autograd import\n",
    "from tiny_autograd_project._10_core.autograd_tiny.value import Value\n",
    "\n",
    "# 시드 고정\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔸 Part 1: Neuron - 가장 작은 단위\n",
    "\n",
    "### 생물학적 뉴런 vs 인공 뉴런\n",
    "\n",
    "생물학적 뉴런:\n",
    "- 수상돌기(Dendrites): 입력 받기\n",
    "- 세포체(Cell Body): 신호 처리\n",
    "- 축삭(Axon): 출력 전달\n",
    "\n",
    "인공 뉴런:\n",
    "- 입력: x₁, x₂, ..., xₙ\n",
    "- 가중치: w₁, w₂, ..., wₙ\n",
    "- 편향: b\n",
    "- 활성화: f(Σwᵢxᵢ + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    \"\"\"간단한 뉴런 구현 (교육용)\"\"\"\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        \"\"\"nin개의 입력을 받는 뉴런 생성\"\"\"\n",
    "        # 가중치 초기화 (-1 ~ 1 사이 랜덤)\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        # 편향 초기화 (0으로 시작)\n",
    "        self.b = Value(0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass: 입력을 받아 출력 생성\"\"\"\n",
    "        # 가중합 계산: Σ(wi * xi) + b\n",
    "        activation = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # 활성화 함수 (tanh) 적용\n",
    "        output = activation.tanh()\n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"학습 가능한 파라미터 반환\"\"\"\n",
    "        return self.w + [self.b]\n",
    "\n",
    "# 테스트\n",
    "neuron = SimpleNeuron(2)\n",
    "x = [Value(0.5), Value(-0.5)]\n",
    "y = neuron(x)\n",
    "\n",
    "print(f\"입력: {[xi.data for xi in x]}\")\n",
    "print(f\"출력: {y.data:.4f}\")\n",
    "print(f\"파라미터 개수: {len(neuron.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 뉴런의 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 학습 예제: 뉴런이 AND 게이트 학습\n",
    "def train_single_neuron():\n",
    "    # AND 게이트 데이터\n",
    "    X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "    y = [0, 0, 0, 1]  # AND 출력\n",
    "    \n",
    "    # 뉴런 생성\n",
    "    neuron = SimpleNeuron(2)\n",
    "    \n",
    "    # 학습 기록\n",
    "    losses = []\n",
    "    \n",
    "    # 학습\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, target in zip(X, y):\n",
    "            # Forward\n",
    "            x_vals = [Value(x) for x in inputs]\n",
    "            pred = neuron(x_vals)\n",
    "            \n",
    "            # Loss\n",
    "            loss = (pred - Value(target)) ** 2\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Backward\n",
    "            for p in neuron.parameters():\n",
    "                p.grad = 0\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update (SGD)\n",
    "            for p in neuron.parameters():\n",
    "                p.data -= 0.1 * p.grad\n",
    "        \n",
    "        losses.append(total_loss / 4)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('학습 곡선')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # 최종 예측\n",
    "    for inputs, target in zip(X, y):\n",
    "        x_vals = [Value(x) for x in inputs]\n",
    "        pred = neuron(x_vals)\n",
    "        plt.scatter(inputs[0], inputs[1], \n",
    "                   c='green' if pred.data > 0.5 else 'red',\n",
    "                   s=100, edgecolor='black')\n",
    "        plt.text(inputs[0], inputs[1], f'{pred.data:.2f}', \n",
    "                ha='center', va='center')\n",
    "    \n",
    "    plt.title('AND 게이트 예측')\n",
    "    plt.xlabel('Input 1')\n",
    "    plt.ylabel('Input 2')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return neuron\n",
    "\n",
    "# 실행\n",
    "trained_neuron = train_single_neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔲 Part 2: Layer - 뉴런들의 집합\n",
    "\n",
    "Layer는 여러 뉴런을 포함합니다.\n",
    "각 뉴런은 같은 입력을 받지만, 다른 가중치를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLayer:\n",
    "    \"\"\"뉴런들의 집합\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"nin 입력을 받아 nout 출력을 생성하는 층\"\"\"\n",
    "        self.neurons = [SimpleNeuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"모든 뉴런에 입력 전달\"\"\"\n",
    "        outputs = [neuron(x) for neuron in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"모든 뉴런의 파라미터\"\"\"\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "# 테스트\n",
    "layer = SimpleLayer(3, 2)  # 3입력 → 2출력\n",
    "x = [Value(1), Value(2), Value(3)]\n",
    "y = layer(x)\n",
    "\n",
    "print(f\"입력 차원: 3\")\n",
    "print(f\"출력 차원: 2\")\n",
    "print(f\"출력값: {[yi.data for yi in y]}\")\n",
    "print(f\"총 파라미터: {len(layer.parameters())}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Part 3: MLP - Multi-Layer Perceptron\n",
    "\n",
    "여러 층을 연결하여 깊은 신경망을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    \"\"\"다층 퍼셉트론\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"nin 입력, nouts=[n1, n2, ...] 각 층의 뉴런 수\"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(nouts)):\n",
    "            self.layers.append(SimpleLayer(sz[i], sz[i+1]))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"순차적으로 모든 층 통과\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"모든 층의 파라미터\"\"\"\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# 테스트: 2-4-4-1 구조\n",
    "mlp = SimpleMLP(2, [4, 4, 1])\n",
    "x = [Value(0.5), Value(0.5)]\n",
    "y = mlp(x)\n",
    "\n",
    "print(f\"네트워크 구조: 2 → 4 → 4 → 1\")\n",
    "print(f\"입력: {[xi.data for xi in x]}\")\n",
    "print(f\"출력: {y.data}\")\n",
    "print(f\"총 파라미터: {len(mlp.parameters())}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Part 4: XOR Problem - 신경망의 Hello World\n",
    "\n",
    "XOR은 선형으로 분리할 수 없는 문제입니다.\n",
    "은닉층이 있는 신경망이 필요합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터\n",
    "X_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_xor = [0, 1, 1, 0]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(6, 6))\n",
    "for inputs, target in zip(X_xor, y_xor):\n",
    "    color = 'blue' if target == 1 else 'red'\n",
    "    plt.scatter(inputs[0], inputs[1], c=color, s=200, edgecolor='black', linewidth=2)\n",
    "    plt.text(inputs[0], inputs[1], f'{target}', \n",
    "            ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "\n",
    "plt.title('XOR Problem', fontsize=16)\n",
    "plt.xlabel('Input 1', fontsize=14)\n",
    "plt.ylabel('Input 2', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"빨강(0)과 파랑(1)을 선 하나로 분리할 수 있나요?\")\n",
    "print(\"→ 불가능! 은닉층이 필요합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 XOR 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_network(epochs=1000, lr=0.5, hidden_size=4):\n",
    "    \"\"\"XOR 문제를 해결하는 신경망 학습\"\"\"\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = SimpleMLP(2, [hidden_size, 1])\n",
    "    print(f\"🏗️ 모델 구조: 2 → {hidden_size} → 1\")\n",
    "    print(f\"📊 총 파라미터: {len(model.parameters())}개\\n\")\n",
    "    \n",
    "    # 학습 기록\n",
    "    history = []\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, target in zip(X_xor, y_xor):\n",
    "            # Forward pass\n",
    "            x_vals = [Value(x) for x in inputs]\n",
    "            pred = model(x_vals)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = (pred - Value(target)) ** 2\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Backward pass\n",
    "            for p in model.parameters():\n",
    "                p.grad = 0\n",
    "            loss.backward()\n",
    "            \n",
    "            # Parameter update (SGD)\n",
    "            for p in model.parameters():\n",
    "                p.data -= lr * p.grad\n",
    "        \n",
    "        avg_loss = total_loss / len(X_xor)\n",
    "        history.append(avg_loss)\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d}: Loss = {avg_loss:.6f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 학습 실행\n",
    "print(\"🎯 XOR 학습 시작...\\n\")\n",
    "xor_model, loss_history = train_xor_network(epochs=1000, lr=0.5, hidden_size=4)\n",
    "print(\"\\n✅ 학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 학습 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. 학습 곡선\n",
    "axes[0].plot(loss_history, linewidth=2)\n",
    "axes[0].set_title('학습 곡선', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# 2. 최종 예측\n",
    "predictions = []\n",
    "for inputs in X_xor:\n",
    "    x_vals = [Value(x) for x in inputs]\n",
    "    pred = xor_model(x_vals)\n",
    "    predictions.append(pred.data)\n",
    "\n",
    "for i, (inputs, target, pred) in enumerate(zip(X_xor, y_xor, predictions)):\n",
    "    color = 'green' if abs(pred - target) < 0.5 else 'red'\n",
    "    axes[1].scatter(inputs[0], inputs[1], c=color, s=200, edgecolor='black', linewidth=2)\n",
    "    axes[1].text(inputs[0], inputs[1], f'{pred:.2f}', \n",
    "                ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].set_title('모델 예측', fontsize=14)\n",
    "axes[1].set_xlabel('Input 1')\n",
    "axes[1].set_ylabel('Input 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-0.5, 1.5)\n",
    "axes[1].set_ylim(-0.5, 1.5)\n",
    "\n",
    "# 3. 결정 경계\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 50),\n",
    "                     np.linspace(-0.5, 1.5, 50))\n",
    "Z = []\n",
    "for i in range(len(xx.ravel())):\n",
    "    x_val = [Value(xx.ravel()[i]), Value(yy.ravel()[i])]\n",
    "    pred = xor_model(x_val)\n",
    "    Z.append(pred.data)\n",
    "\n",
    "Z = np.array(Z).reshape(xx.shape)\n",
    "\n",
    "contour = axes[2].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "axes[2].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "for inputs, target in zip(X_xor, y_xor):\n",
    "    color = 'blue' if target == 1 else 'red'\n",
    "    axes[2].scatter(inputs[0], inputs[1], c=color, s=100, \n",
    "                   edgecolor='white', linewidth=2, zorder=5)\n",
    "\n",
    "axes[2].set_title('결정 경계', fontsize=14)\n",
    "axes[2].set_xlabel('Input 1')\n",
    "axes[2].set_ylabel('Input 2')\n",
    "axes[2].set_xlim(-0.5, 1.5)\n",
    "axes[2].set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 정확도 계산\n",
    "correct = sum(1 for pred, target in zip(predictions, y_xor) \n",
    "             if (pred > 0.5) == (target == 1))\n",
    "accuracy = correct / len(y_xor) * 100\n",
    "\n",
    "print(\"\\n📊 최종 결과:\")\n",
    "print(\"=\"*40)\n",
    "for inputs, target, pred in zip(X_xor, y_xor, predictions):\n",
    "    pred_class = 1 if pred > 0.5 else 0\n",
    "    symbol = \"✓\" if pred_class == target else \"✗\"\n",
    "    print(f\"{inputs} → 예측: {pred:.4f} (클래스: {pred_class}), 정답: {target} {symbol}\")\n",
    "print(\"=\"*40)\n",
    "print(f\"🎯 정확도: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Part 5: 핵심 개념 정리\n",
    "\n",
    "### 왜 XOR이 중요한가?\n",
    "\n",
    "1. **선형 분리 불가능**: 단층 퍼셉트론으로는 해결 불가\n",
    "2. **은닉층의 필요성**: 비선형 변환을 통한 특징 학습\n",
    "3. **표현력**: 은닉층이 있으면 어떤 함수도 근사 가능 (Universal Approximation)\n",
    "\n",
    "### 학습의 핵심 요소\n",
    "\n",
    "1. **Forward Pass**: 입력 → 출력 계산\n",
    "2. **Loss Function**: 예측과 정답의 차이 측정\n",
    "3. **Backward Pass**: Chain Rule로 gradient 계산\n",
    "4. **Parameter Update**: Gradient descent로 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Part 6: 실습 과제\n",
    "\n",
    "이제 `10_core/nn_tiny/` 폴더의 파일들을 구현해보세요!\n",
    "\n",
    "### 구현 순서:\n",
    "1. ✅ `neuron.py` - Neuron 클래스\n",
    "2. ✅ `layer.py` - Layer 클래스\n",
    "3. ✅ `mlp.py` - MLP 클래스\n",
    "4. ✅ `losses.py` - 손실 함수\n",
    "5. ✅ `optimizer.py` - SGD 최적화기\n",
    "\n",
    "### 테스트:\n",
    "```bash\n",
    "# 구현 후 테스트\n",
    "python -m pytest tests/test_nn.py -v\n",
    "\n",
    "# XOR 데모 실행\n",
    "python 50_eval/xor_demo.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 도전 과제\n",
    "\n",
    "### Level 1: 다른 논리 게이트\n",
    "- AND, OR, NAND 게이트 학습\n",
    "- 필요한 최소 뉴런 수는?\n",
    "\n",
    "### Level 2: 더 복잡한 패턴\n",
    "- 3-bit parity 문제\n",
    "- 원형 데이터 분류\n",
    "\n",
    "### Level 3: 실제 데이터\n",
    "- Iris 데이터셋\n",
    "- MNIST (784-128-10 구조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도전 과제 예시: 원형 데이터 분류\n",
    "def create_circular_data(n_samples=100):\n",
    "    \"\"\"원형으로 분포한 2클래스 데이터 생성\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # 랜덤 점 생성\n",
    "        x1 = random.uniform(-2, 2)\n",
    "        x2 = random.uniform(-2, 2)\n",
    "        \n",
    "        # 원의 안/밖으로 분류\n",
    "        distance = math.sqrt(x1**2 + x2**2)\n",
    "        label = 1 if distance < 1.0 else 0\n",
    "        \n",
    "        X.append([x1, x2])\n",
    "        y.append(label)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 데이터 생성 및 시각화\n",
    "X_circle, y_circle = create_circular_data(200)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for x, label in zip(X_circle, y_circle):\n",
    "    color = 'blue' if label == 1 else 'red'\n",
    "    plt.scatter(x[0], x[1], c=color, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='green', linewidth=2, linestyle='--')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.title('원형 분류 문제', fontsize=16)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"도전: 이 데이터를 분류하는 신경망을 만들어보세요!\")\n",
    "print(\"힌트: 은닉층의 크기를 늘려보세요 (예: 2-16-16-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 축하합니다!\n",
    "\n",
    "신경망의 기초를 완성했습니다! 🎊\n",
    "\n",
    "### 다음 단계:\n",
    "- **Day 2**: 벡터/행렬 연산으로 확장\n",
    "- **Day 3**: Attention 메커니즘\n",
    "- **Day 4**: Transformer 구현\n",
    "- **Day 5**: 실제 텍스트 생성\n",
    "\n",
    "**\"The best way to understand neural networks is to implement them from scratch!\"**\n",
    "- Andrej Karpathy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}