# ğŸ§  Neural Network í•µì‹¬ ê°œë…

## 1. ì‹ ê²½ë§ì˜ êµ¬ì„± ìš”ì†Œ

### ğŸ”¸ Neuron (ë‰´ëŸ°)
```python
output = activation(Î£(wi * xi) + b)
```

**êµ¬ì„± ìš”ì†Œ:**
- **ì…ë ¥ (x)**: ì´ì „ ì¸µì—ì„œ ì˜¤ëŠ” ì‹ í˜¸
- **ê°€ì¤‘ì¹˜ (w)**: ê° ì…ë ¥ì˜ ì¤‘ìš”ë„
- **í¸í–¥ (b)**: í™œì„±í™” ì„ê³„ê°’ ì¡°ì •
- **í™œì„±í™” í•¨ìˆ˜**: ë¹„ì„ í˜• ë³€í™˜

**ìƒë¬¼í•™ì  ë¹„ìœ :**
- ìˆ˜ìƒëŒê¸° â†’ ì…ë ¥
- ì‹œëƒ…ìŠ¤ ê°•ë„ â†’ ê°€ì¤‘ì¹˜
- ì„¸í¬ì²´ â†’ ê°€ì¤‘í•© ê³„ì‚°
- ì¶•ì‚­ â†’ ì¶œë ¥

### ğŸ”² Layer (ì¸µ)
- ê°™ì€ ì…ë ¥ì„ ê³µìœ í•˜ëŠ” ë‰´ëŸ°ë“¤ì˜ ì§‘í•©
- ê° ë‰´ëŸ°ì€ ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜
- ë²¡í„° â†’ ë²¡í„° ë³€í™˜

### ğŸ—ï¸ Network (ë„¤íŠ¸ì›Œí¬)
- ì—¬ëŸ¬ ì¸µì˜ ìˆœì°¨ì  ì—°ê²°
- ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ(ë“¤) â†’ ì¶œë ¥ì¸µ

## 2. Forward Propagation

### ìˆ˜í•™ì  í‘œí˜„
```
ì¸µ lì—ì„œ:
z[l] = W[l] Ã— a[l-1] + b[l]  # ì„ í˜• ë³€í™˜
a[l] = f(z[l])                # í™œì„±í™”
```

### ì½”ë“œ êµ¬í˜„ íŒ¨í„´
```python
def forward(self, x):
    for layer in self.layers:
        x = layer(x)
    return x
```

## 3. í™œì„±í™” í•¨ìˆ˜

### Tanh
```python
f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
f'(x) = 1 - tanhÂ²(x)
```
- ë²”ìœ„: [-1, 1]
- ì¤‘ì‹¬: 0
- ìš©ë„: ì€ë‹‰ì¸µ

### ReLU
```python
f(x) = max(0, x)
f'(x) = 1 if x > 0 else 0
```
- ë²”ìœ„: [0, âˆ)
- ì¥ì : ê³„ì‚° ê°„ë‹¨, gradient vanishing ì™„í™”
- ë‹¨ì : dying ReLU ë¬¸ì œ

### Sigmoid
```python
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x) Ã— (1 - f(x))
```
- ë²”ìœ„: [0, 1]
- ìš©ë„: ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ

## 4. ì†ì‹¤ í•¨ìˆ˜

### MSE (Mean Squared Error)
```python
L = (1/n) Ã— Î£(y_pred - y_true)Â²
âˆ‚L/âˆ‚y_pred = (2/n) Ã— (y_pred - y_true)
```
- ìš©ë„: íšŒê·€ ë¬¸ì œ
- íŠ¹ì§•: í° ì˜¤ì°¨ì— ë¯¼ê°

### Cross-Entropy
```python
L = -Î£(y_true Ã— log(y_pred))
âˆ‚L/âˆ‚y_pred = -y_true/y_pred
```
- ìš©ë„: ë¶„ë¥˜ ë¬¸ì œ
- íŠ¹ì§•: í™•ë¥  ë¶„í¬ ë¹„êµ

## 5. Backpropagation

### Chain Rule
```
âˆ‚L/âˆ‚w[l] = âˆ‚L/âˆ‚a[l] Ã— âˆ‚a[l]/âˆ‚z[l] Ã— âˆ‚z[l]/âˆ‚w[l]
```

### êµ¬í˜„ íŒ¨í„´
```python
def backward(self):
    # 1. ì¶œë ¥ì¸µ gradient
    self.grad = 1.0
    
    # 2. ì—­ìˆœìœ¼ë¡œ ì „íŒŒ
    for layer in reversed(self.layers):
        layer.backward()
```

## 6. ìµœì í™” (Optimization)

### SGD (Stochastic Gradient Descent)
```python
w = w - learning_rate Ã— âˆ‚L/âˆ‚w
```

### Momentum
```python
v = Î² Ã— v - learning_rate Ã— âˆ‚L/âˆ‚w
w = w + v
```

### Adam
```python
m = Î²1 Ã— m + (1-Î²1) Ã— grad      # 1ì°¨ ëª¨ë©˜íŠ¸
v = Î²2 Ã— v + (1-Î²2) Ã— gradÂ²     # 2ì°¨ ëª¨ë©˜íŠ¸
w = w - lr Ã— m / (âˆšv + Îµ)
```

## 7. XOR ë¬¸ì œì˜ ì˜ë¯¸

### ì™œ ì¤‘ìš”í•œê°€?
1. **ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥**: ë‹¨ì¸µìœ¼ë¡œ í•´ê²° ë¶ˆê°€
2. **ì€ë‹‰ì¸µ í•„ìš”ì„±**: ë¹„ì„ í˜• ë³€í™˜ í•„ìš”
3. **ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥**: Universal Approximation

### XOR ì§„ë¦¬í‘œ
```
X1  X2  |  Y
--------|----
0   0   |  0
0   1   |  1
1   0   |  1
1   1   |  0
```

### í•´ê²° ë°©ë²•
```python
# ìµœì†Œ êµ¬ì¡°: 2-2-1
# ì•ˆì •ì : 2-4-1
model = MLP(2, [4, 1])
```

## 8. í•™ìŠµ ê³¼ì •

### 1. ì´ˆê¸°í™”
- Xavier: `w ~ N(0, 2/(nin + nout))`
- He: `w ~ N(0, 2/nin)`

### 2. í•™ìŠµ ë£¨í”„
```python
for epoch in range(epochs):
    # Forward
    pred = model(x)
    loss = loss_fn(pred, y)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    
    # Update
    optimizer.step()
```

### 3. ìˆ˜ë ´ í™•ì¸
- Loss ê°ì†Œ
- Gradient norm ê°ì†Œ
- Validation ì„±ëŠ¥

## 9. ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°

### Gradient Vanishing
- ì›ì¸: ê¹Šì€ ë„¤íŠ¸ì›Œí¬, sigmoid/tanh
- í•´ê²°: ReLU, BatchNorm, ResNet

### Gradient Exploding
- ì›ì¸: í° ê°€ì¤‘ì¹˜, ê¹Šì€ ë„¤íŠ¸ì›Œí¬
- í•´ê²°: Gradient clipping, ì‘ì€ ì´ˆê¸°í™”

### Overfitting
- ì›ì¸: ëª¨ë¸ ë³µì¡ë„ > ë°ì´í„°
- í•´ê²°: Dropout, L2 ì •ê·œí™”, ë°ì´í„° ì¦ê°•

## 10. ì‹¤ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ìˆœì„œ
1. âœ… Neuron: ê°€ì¤‘í•© + í™œì„±í™”
2. âœ… Layer: ë‰´ëŸ° ì§‘í•©
3. âœ… MLP: ì¸µ ì—°ê²°
4. âœ… Loss: MSE
5. âœ… Optimizer: SGD
6. âœ… Training Loop

### í…ŒìŠ¤íŠ¸ ìˆœì„œ
1. ë‹¨ì¼ ë‰´ëŸ° â†’ AND ê²Œì´íŠ¸
2. 2ì¸µ ë„¤íŠ¸ì›Œí¬ â†’ XOR
3. 3ì¸µ ë„¤íŠ¸ì›Œí¬ â†’ ì›í˜• ë°ì´í„°

## 11. ì½”ë“œ ìŠ¤ë‹ˆí«

### ë‰´ëŸ° êµ¬í˜„
```python
class Neuron:
    def __init__(self, nin):
        self.w = [Value(random.uniform(-1,1)) 
                  for _ in range(nin)]
        self.b = Value(0)
    
    def __call__(self, x):
        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)
        return act.tanh()
```

### í•™ìŠµ ë£¨í”„
```python
for epoch in range(1000):
    # Batch ì²˜ë¦¬
    for x_batch, y_batch in dataloader:
        pred = model(x_batch)
        loss = mse_loss(pred, y_batch)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

## 12. ë‹¤ìŒ ë‹¨ê³„

### Day 2 ì¤€ë¹„
- NumPy ë°°ì—´ ì—°ì‚°
- í–‰ë ¬ê³± ì´í•´
- Broadcasting

### í™•ì¥ ì£¼ì œ
- Batch ì²˜ë¦¬
- ì •ê·œí™” (BatchNorm, LayerNorm)
- Dropout
- Skip Connection

---

**"ë‰´ëŸ° í•˜ë‚˜í•˜ë‚˜ê°€ ëª¨ì—¬ ì§€ëŠ¥ì„ ë§Œë“­ë‹ˆë‹¤"**