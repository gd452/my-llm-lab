"""
âš™ï¸ Optimizer: ìµœì í™” ì•Œê³ ë¦¬ì¦˜

ì´ íŒŒì¼ì—ì„œ êµ¬í˜„í•  ê²ƒ:
1. SGD (Stochastic Gradient Descent) - í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•
2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
3. Gradient ì´ˆê¸°í™”

ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ gradientë¥¼ ì‚¬ìš©í•´
íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""


class SGD:
    """
    Stochastic Gradient Descent Optimizer
    
    ê°€ì¥ ê¸°ë³¸ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
    ê° íŒŒë¼ë¯¸í„°ë¥¼ gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    
    ì—…ë°ì´íŠ¸ ê·œì¹™:
        param = param - learning_rate * gradient
    
    Example:
        >>> optimizer = SGD(model.parameters(), lr=0.01)
        >>> loss.backward()  # gradient ê³„ì‚°
        >>> optimizer.step()  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        >>> optimizer.zero_grad()  # gradient ì´ˆê¸°í™”
    """
    
    def __init__(self, parameters, lr=0.01):
        """
        SGD ì´ˆê¸°í™”
        
        Args:
            parameters: ìµœì í™”í•  íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸ (Value ê°ì²´ë“¤)
            lr: í•™ìŠµë¥  (learning rate)
            
        TODO:
        1. self.parametersì— íŒŒë¼ë¯¸í„° ì €ì¥
        2. self.lrì— í•™ìŠµë¥  ì €ì¥
        """
        # TODO: íŒŒë¼ë¯¸í„°ì™€ í•™ìŠµë¥  ì €ì¥
        self.parameters = parameters
        self.lr = lr
    
    def step(self):
        """
        íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        
        ê° íŒŒë¼ë¯¸í„°ë¥¼ gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™:
        param.data = param.data - lr * param.grad
        
        TODO:
        1. ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´
        2. param.data -= self.lr * param.grad ìˆ˜í–‰
        
        íŒíŠ¸:
            for param in self.parameters:
                param.data -= self.lr * param.grad
        """
        # TODO: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        pass  # TODO: êµ¬í˜„
    
    def zero_grad(self):
        """
        ëª¨ë“  gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        
        ì—­ì „íŒŒ ì „ì— ì´ì „ gradientë¥¼ ì§€ì›Œì•¼ í•©ë‹ˆë‹¤.
        (gradientëŠ” ëˆ„ì ë˜ê¸° ë•Œë¬¸)
        
        TODO:
        1. ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradë¥¼ 0ìœ¼ë¡œ ì„¤ì •
        
        íŒíŠ¸:
            for param in self.parameters:
                param.grad = 0.0
        """
        # TODO: gradient ì´ˆê¸°í™”
        pass  # TODO: êµ¬í˜„


class Adam:
    """
    Adam Optimizer (ì„ íƒ êµ¬í˜„)
    
    Adaptive Moment Estimation
    - Momentumê³¼ RMSPropì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    - ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ì ì‘ì  í•™ìŠµë¥  ì‚¬ìš©
    
    ë” ê³ ê¸‰ ìµœì í™”ë¥¼ ì›í•œë‹¤ë©´ êµ¬í˜„í•´ë³´ì„¸ìš”!
    """
    
    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8):
        """
        Adam ì´ˆê¸°í™” (ì„ íƒ êµ¬í˜„)
        
        Args:
            parameters: ìµœì í™”í•  íŒŒë¼ë¯¸í„°
            lr: í•™ìŠµë¥ 
            betas: (Î²1, Î²2) ëª¨ë©˜í…€ ê³„ìˆ˜
            eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’
        """
        self.parameters = parameters
        self.lr = lr
        self.betas = betas
        self.eps = eps
        
        # TODO: ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ m, v ì´ˆê¸°í™” (ì„ íƒ)
        # self.m = [0] * len(parameters)  # 1ì°¨ ëª¨ë©˜íŠ¸
        # self.v = [0] * len(parameters)  # 2ì°¨ ëª¨ë©˜íŠ¸
        # self.t = 0  # ì‹œê°„ ìŠ¤í…
    
    def step(self):
        """Adam ì—…ë°ì´íŠ¸ (ì„ íƒ êµ¬í˜„)"""
        # TODO: Adam ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (ì„ íƒ)
        pass
    
    def zero_grad(self):
        """gradient ì´ˆê¸°í™”"""
        for param in self.parameters:
            param.grad = 0.0


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ§ª Optimizer í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    # TODO êµ¬í˜„ í›„ í…ŒìŠ¤íŠ¸
    """
    from tiny_autograd_project._10_core.autograd_tiny.value import Value
    
    # íŒŒë¼ë¯¸í„° ìƒì„±
    params = [Value(1.0), Value(2.0), Value(3.0)]
    
    # Gradient ì‹œë®¬ë ˆì´ì…˜
    params[0].grad = 0.1
    params[1].grad = -0.2
    params[2].grad = 0.3
    
    print("ì´ˆê¸° íŒŒë¼ë¯¸í„°:")
    print([p.data for p in params])
    print("Gradients:")
    print([p.grad for p in params])
    
    # SGD ì ìš©
    optimizer = SGD(params, lr=0.1)
    optimizer.step()
    
    print("\nì—…ë°ì´íŠ¸ í›„ íŒŒë¼ë¯¸í„°:")
    print([p.data for p in params])
    # ì˜ˆìƒ: [0.99, 2.02, 2.97]
    
    # Gradient ì´ˆê¸°í™”
    optimizer.zero_grad()
    print("\nGradient ì´ˆê¸°í™” í›„:")
    print([p.grad for p in params])
    # ì˜ˆìƒ: [0.0, 0.0, 0.0]
    """
    print("TODO: optimizer.py êµ¬í˜„ í•„ìš”!")