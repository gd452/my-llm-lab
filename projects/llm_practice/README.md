# Week 2: ì˜¤í”ˆì†ŒìŠ¤ LLM ì‹¤ìŠµ - Qwen ëª¨ë¸

## ğŸ¯ ëª©í‘œ
**ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ LLM í™œìš© ëŠ¥ë ¥ ìŠµë“**

## ğŸ“Š Qwen ëª¨ë¸ ì„ íƒ ì´ìœ 

1. **ìµœì‹  ì„±ëŠ¥**: GPT-3.5 ìˆ˜ì¤€ ë˜ëŠ” ê·¸ ì´ìƒ
2. **í•œêµ­ì–´ ì§€ì›**: ë›°ì–´ë‚œ ë‹¤êµ­ì–´ ëŠ¥ë ¥
3. **ë‹¤ì–‘í•œ í¬ê¸°**: 0.5B ~ 72B (ìš©ë„ë³„ ì„ íƒ ê°€ëŠ¥)
4. **ìƒì—…ì  ì‚¬ìš©**: Apache 2.0 ë¼ì´ì„ ìŠ¤

## ğŸ”§ ì‹¤ìŠµ êµ¬ì„±

### Phase 1: Local LLM ì‹¤í–‰ (Ollama)
```bash
# Qwen ëª¨ë¸ ì„¤ì¹˜ ë° ì‹¤í–‰
ollama pull qwen2:0.5b    # ê°€ì¥ ì‘ì€ ëª¨ë¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
ollama pull qwen2:7b      # ì‹¤ìš©ì  í¬ê¸°
ollama run qwen2:7b
```

### Phase 2: Hugging Face í™œìš©
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Qwen2-1.5B ëª¨ë¸ (ì½”ë”© ì‘ì—…ìš©)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-1.5B")
```

### Phase 3: Fine-tuning with LoRA
```python
# íš¨ìœ¨ì ì¸ fine-tuning
from peft import LoraConfig, get_peft_model

# 4-bit quantization + LoRA
# 7B ëª¨ë¸ë„ ì¼ë°˜ GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥
```

### Phase 4: RAG ì‹œìŠ¤í…œ êµ¬ì¶•
```python
# ë‚˜ë§Œì˜ ì§€ì‹ ë² ì´ìŠ¤ ì—°ë™
from langchain import Qwen2LLM
from langchain.vectorstores import Chroma

# PDF, ë…¸íŠ¸, ì½”ë“œ â†’ Vector DB â†’ Qwen
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llm_practice/
â”œâ”€â”€ 01_ollama/
â”‚   â”œâ”€â”€ basic_chat.py        # ê¸°ë³¸ ëŒ€í™”
â”‚   â”œâ”€â”€ streaming.py         # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
â”‚   â””â”€â”€ api_server.py        # REST API ì„œë²„
â”‚
â”œâ”€â”€ 02_huggingface/
â”‚   â”œâ”€â”€ inference.py         # ì¶”ë¡  ìµœì í™”
â”‚   â”œâ”€â”€ quantization.py      # 4-bit, 8-bit ì–‘ìí™”
â”‚   â””â”€â”€ batch_processing.py  # ë°°ì¹˜ ì²˜ë¦¬
â”‚
â”œâ”€â”€ 03_fine_tuning/
â”‚   â”œâ”€â”€ prepare_data.py      # ë°ì´í„° ì¤€ë¹„
â”‚   â”œâ”€â”€ lora_training.py     # LoRA í•™ìŠµ
â”‚   â””â”€â”€ merge_weights.py     # ê°€ì¤‘ì¹˜ ë³‘í•©
â”‚
â””â”€â”€ 04_rag_system/
    â”œâ”€â”€ document_loader.py   # ë¬¸ì„œ ë¡œë”©
    â”œâ”€â”€ vector_store.py      # ë²¡í„° DB
    â””â”€â”€ rag_chat.py          # RAG ì±—ë´‡
```

## ğŸš€ ì‹¤ìŠµ ìŠ¤ì¼€ì¤„

### Day 1-2: Ollama + Qwen ê¸°ì´ˆ
- ë¡œì»¬ ì‹¤í–‰ í™˜ê²½ êµ¬ì¶•
- ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸° ë¹„êµ
- Prompt engineering ì‹¤ìŠµ

### Day 3-4: Hugging Face ìƒíƒœê³„
- ëª¨ë¸ ë¡œë”© ìµœì í™”
- Quantization (ë©”ëª¨ë¦¬ ì ˆì•½)
- Batch inference

### Day 5-6: Fine-tuning
- ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„
- LoRA/QLoRA í•™ìŠµ
- í•™ìŠµ ëª¨ë‹ˆí„°ë§

### Day 7: RAG ì‹œìŠ¤í…œ
- ë¬¸ì„œ ì„ë² ë”©
- ë²¡í„° ê²€ìƒ‰
- Context-aware ì‘ë‹µ

## ğŸ’¡ ì‹¤ë¬´ í™œìš© ì˜ˆì‹œ

### 1. ì½”ë“œ ë¦¬ë·° ë´‡
```python
# ì½”ë“œ ë¶„ì„ ë° ê°œì„  ì œì•ˆ
def review_code(code: str):
    prompt = f"Review this code and suggest improvements:\n{code}"
    return qwen_model.generate(prompt)
```

### 2. ë¬¸ì„œ ìš”ì•½ê¸°
```python
# ê¸´ ë¬¸ì„œ â†’ í•µì‹¬ ìš”ì•½
def summarize_document(doc: str):
    prompt = f"Summarize in 3 bullet points:\n{doc}"
    return qwen_model.generate(prompt)
```

### 3. SQL ìƒì„±ê¸°
```python
# ìì—°ì–´ â†’ SQL ì¿¼ë¦¬
def text_to_sql(question: str, schema: str):
    prompt = f"Schema: {schema}\nQuestion: {question}\nSQL:"
    return qwen_model.generate(prompt)
```

## ğŸ“ í•™ìŠµ ëª©í‘œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Ollamaë¡œ ë¡œì»¬ LLM ì‹¤í–‰
- [ ] Streaming response êµ¬í˜„
- [ ] 4-bit quantizationìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
- [ ] LoRA fine-tuning ì‹¤í–‰
- [ ] Custom dataset ì¤€ë¹„
- [ ] RAG ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì‹¤ì œ ì—…ë¬´ ì ìš© ì‚¬ë¡€ 1ê°œ êµ¬í˜„

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ëª¨ë¸ | í¬ê¸° | ë©”ëª¨ë¦¬ | ì†ë„ | í’ˆì§ˆ | ìš©ë„ |
|------|------|--------|------|------|------|
| Qwen2-0.5B | 0.5B | 1GB | ë§¤ìš°ë¹ ë¦„ | ë³´í†µ | í…ŒìŠ¤íŠ¸, ê°„ë‹¨ì‘ì—… |
| Qwen2-1.5B | 1.5B | 3GB | ë¹ ë¦„ | ì¢‹ìŒ | ì½”ë”©, ë²ˆì—­ |
| Qwen2-7B | 7B | 14GB | ë³´í†µ | ë§¤ìš°ì¢‹ìŒ | ì „ë¬¸ì‘ì—… |
| Qwen2-72B | 72B | 140GB | ëŠë¦¼ | ìµœê³  | ì—°êµ¬, ê³ ê¸‰ì‘ì—… |

## ğŸ” Week 3 Preview: ë‚˜ë§Œì˜ ë©”ëª¨ ë¹„ì„œ

```python
# ë‚´ ë…¸íŠ¸ + Qwen = ê°œì¸ AI ë¹„ì„œ
class PersonalAssistant:
    def __init__(self):
        self.model = load_qwen_model()
        self.notes = load_my_notes()
        self.vector_db = create_vector_store(self.notes)
    
    def answer(self, question):
        # 1. ê´€ë ¨ ë…¸íŠ¸ ê²€ìƒ‰
        context = self.vector_db.search(question)
        # 2. Qwenìœ¼ë¡œ ë‹µë³€ ìƒì„±
        return self.model.generate_with_context(question, context)
```

ì¤€ë¹„ë˜ì…¨ë‚˜ìš”? ì‹¤ìŠµì„ ì‹œì‘í•´ë³¼ê¹Œìš”?