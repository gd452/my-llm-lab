"""
ì„ë² ë”© ê¸°ì´ˆ - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
Sentence Transformersë¥¼ ì‚¬ìš©í•œ ì‹¤ìš©ì ì¸ ì˜ˆì œ
"""

import numpy as np
from typing import List, Tuple, Dict, Any
from sentence_transformers import SentenceTransformer
import torch
from sklearn.metrics.pairwise import cosine_similarity
import json

class EmbeddingBasics:
    """ì„ë² ë”© ê¸°ì´ˆ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        ê°€ë²¼ìš´ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        all-MiniLM-L6-v2: ë¹ ë¥´ê³  íš¨ìœ¨ì  (384 ì°¨ì›)
        all-mpnet-base-v2: ë” ì •í™•í•¨ (768 ì°¨ì›)
        """
        self.model = SentenceTransformer(model_name)
        self.embeddings_cache = {}
        
    def get_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        if text in self.embeddings_cache:
            return self.embeddings_cache[text]
        
        embedding = self.model.encode(text)
        self.embeddings_cache[text] = embedding
        return embedding
    
    def get_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ì„ë² ë”©"""
        return self.model.encode(texts)
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        emb1 = self.get_embedding(text1)
        emb2 = self.get_embedding(text2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity([emb1], [emb2])[0][0]
        return float(similarity)
    
    def find_most_similar(self, query: str, documents: List[str], top_k: int = 3) -> List[Tuple[str, float]]:
        """ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°"""
        query_emb = self.get_embedding(query)
        doc_embs = self.get_embeddings_batch(documents)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity([query_emb], doc_embs)[0]
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append((documents[idx], float(similarities[idx])))
        
        return results
    
    def semantic_search(self, query: str, knowledge_base: Dict[str, str]) -> List[Tuple[str, str, float]]:
        """ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"""
        # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
        titles = list(knowledge_base.keys())
        contents = list(knowledge_base.values())
        
        # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©í•œ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”©
        combined_texts = [f"{title}: {content}" for title, content in zip(titles, contents)]
        
        # ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
        similar_docs = self.find_most_similar(query, combined_texts, top_k=3)
        
        results = []
        for doc, score in similar_docs:
            # ì›ë³¸ ì œëª© ì°¾ê¸°
            for title, content in knowledge_base.items():
                if f"{title}: {content}" == doc:
                    results.append((title, content, score))
                    break
        
        return results
    
    def cluster_texts(self, texts: List[str], n_clusters: int = 3) -> Dict[int, List[str]]:
        """í…ìŠ¤íŠ¸ í´ëŸ¬ìŠ¤í„°ë§"""
        from sklearn.cluster import KMeans
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.get_embeddings_batch(texts)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê·¸ë£¹í™”
        clusters = {}
        for text, label in zip(texts, labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(text)
        
        return clusters
    
    def detect_duplicates(self, texts: List[str], threshold: float = 0.9) -> List[Tuple[str, str, float]]:
        """ì¤‘ë³µ ë˜ëŠ” ë§¤ìš° ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        duplicates = []
        embeddings = self.get_embeddings_batch(texts)
        
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
                if similarity >= threshold:
                    duplicates.append((texts[i], texts[j], float(similarity)))
        
        return duplicates
    
    def zero_shot_classification(self, text: str, labels: List[str]) -> Dict[str, float]:
        """Zero-shot ë¶„ë¥˜ (ë¼ë²¨ ì—†ì´ ë¶„ë¥˜)"""
        text_emb = self.get_embedding(text)
        label_embs = self.get_embeddings_batch(labels)
        
        # ê° ë¼ë²¨ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity([text_emb], label_embs)[0]
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
        exp_scores = np.exp(similarities)
        probabilities = exp_scores / exp_scores.sum()
        
        results = {}
        for label, prob in zip(labels, probabilities):
            results[label] = float(prob)
        
        return results

class AdvancedEmbeddings:
    """ê³ ê¸‰ ì„ë² ë”© ê¸°ë²•"""
    
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def weighted_embedding(self, texts: List[str], weights: List[float]) -> np.ndarray:
        """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì„ë² ë”© í‰ê· """
        embeddings = self.model.encode(texts)
        weights = np.array(weights).reshape(-1, 1)
        weighted_emb = np.sum(embeddings * weights, axis=0) / np.sum(weights)
        return weighted_emb
    
    def hierarchical_embedding(self, paragraphs: List[str]) -> Dict[str, Any]:
        """ê³„ì¸µì  ì„ë² ë”© (ë¬¸ë‹¨ -> ë¬¸ì¥ -> ë‹¨ì–´)"""
        result = {
            "document_embedding": None,
            "paragraph_embeddings": [],
            "sentence_embeddings": []
        }
        
        paragraph_embs = []
        all_sentences = []
        
        for para in paragraphs:
            # ë¬¸ì¥ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ë²•)
            sentences = para.split('. ')
            all_sentences.extend(sentences)
            
            # ë¬¸ë‹¨ ì„ë² ë”©
            para_emb = self.model.encode(para)
            paragraph_embs.append(para_emb)
        
        # ì „ì²´ ë¬¸ì„œ ì„ë² ë”© (ë¬¸ë‹¨ ì„ë² ë”©ì˜ í‰ê· )
        result["document_embedding"] = np.mean(paragraph_embs, axis=0)
        result["paragraph_embeddings"] = paragraph_embs
        result["sentence_embeddings"] = self.model.encode(all_sentences)
        
        return result
    
    def sliding_window_embedding(self, text: str, window_size: int = 100, stride: int = 50) -> List[np.ndarray]:
        """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì„ë² ë”© (ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬)"""
        words = text.split()
        embeddings = []
        
        for i in range(0, len(words) - window_size + 1, stride):
            window_text = ' '.join(words[i:i + window_size])
            embedding = self.model.encode(window_text)
            embeddings.append(embedding)
        
        return embeddings

def demo_basic_embeddings():
    """ê¸°ë³¸ ì„ë² ë”© ë°ëª¨"""
    
    print("ğŸ”¤ ì„ë² ë”© ê¸°ì´ˆ ë°ëª¨")
    print("=" * 60)
    
    emb = EmbeddingBasics()
    
    # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
    print("\n1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°:")
    pairs = [
        ("cat", "dog"),
        ("cat", "kitten"),
        ("car", "automobile"),
        ("car", "banana")
    ]
    
    for text1, text2 in pairs:
        similarity = emb.calculate_similarity(text1, text2)
        print(f"  '{text1}' vs '{text2}': {similarity:.3f}")
    
    # 2. ì˜ë¯¸ ê²€ìƒ‰
    print("\n2. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰:")
    documents = [
        "Python is a programming language",
        "Machine learning uses algorithms to learn from data",
        "Dogs are loyal pets",
        "Neural networks are inspired by the brain",
        "Coffee is a popular morning beverage"
    ]
    
    query = "artificial intelligence and deep learning"
    results = emb.find_most_similar(query, documents, top_k=3)
    
    print(f"  Query: '{query}'")
    print("  Top matches:")
    for doc, score in results:
        print(f"    - {doc[:50]}... (score: {score:.3f})")
    
    # 3. Zero-shot ë¶„ë¥˜
    print("\n3. Zero-shot Classification:")
    text = "The new smartphone has an amazing camera and long battery life"
    categories = ["Technology", "Sports", "Food", "Politics"]
    
    classification = emb.zero_shot_classification(text, categories)
    print(f"  Text: '{text[:50]}...'")
    print("  Categories:")
    for category, prob in sorted(classification.items(), key=lambda x: x[1], reverse=True):
        print(f"    - {category}: {prob:.2%}")
    
    # 4. ì¤‘ë³µ ê°ì§€
    print("\n4. ì¤‘ë³µ í…ìŠ¤íŠ¸ ê°ì§€:")
    texts = [
        "The weather is nice today",
        "Today the weather is nice",
        "It's a beautiful day",
        "Python is great for data science",
        "Data science is great with Python"
    ]
    
    duplicates = emb.detect_duplicates(texts, threshold=0.8)
    if duplicates:
        print("  ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ìŒ:")
        for text1, text2, score in duplicates:
            print(f"    - '{text1[:30]}...' â†” '{text2[:30]}...' (ìœ ì‚¬ë„: {score:.3f})")
    else:
        print("  ì¤‘ë³µ ì—†ìŒ")

def demo_semantic_search():
    """ì˜ë¯¸ ê²€ìƒ‰ ë°ëª¨"""
    
    print("\nğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ë°ëª¨")
    print("=" * 60)
    
    emb = EmbeddingBasics()
    
    # ì§€ì‹ ë² ì´ìŠ¤
    knowledge_base = {
        "Python ê¸°ì´ˆ": "Pythonì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê°„ê²°í•œ ë¬¸ë²•ê³¼ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹": "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” AI ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ì›¹ ê°œë°œ": "ì›¹ ê°œë°œì€ HTML, CSS, JavaScriptë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì‚¬ì´íŠ¸ì™€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
        "ë°ì´í„°ë² ì´ìŠ¤": "ë°ì´í„°ë² ì´ìŠ¤ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ, SQLë¡œ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "í´ë¼ìš°ë“œ ì»´í“¨íŒ…": "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•´ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."
    }
    
    queries = [
        "í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë°°ìš°ê¸°",
        "AIì™€ ë”¥ëŸ¬ë‹",
        "ë°ì´í„° ì €ì¥ ë°©ë²•"
    ]
    
    for query in queries:
        print(f"\n  Query: '{query}'")
        results = emb.semantic_search(query, knowledge_base)
        print("  Results:")
        for title, content, score in results:
            print(f"    [{score:.3f}] {title}: {content[:50]}...")

def demo_clustering():
    """í´ëŸ¬ìŠ¤í„°ë§ ë°ëª¨"""
    
    print("\nğŸ“Š í…ìŠ¤íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ë°ëª¨")
    print("=" * 60)
    
    emb = EmbeddingBasics()
    
    texts = [
        "Python programming tutorial",
        "Machine learning with Python",
        "How to cook pasta",
        "Italian cuisine recipes",
        "Deep learning fundamentals",
        "Neural network architecture",
        "Best pasta restaurants",
        "Data science with R",
        "Traditional Italian dishes"
    ]
    
    clusters = emb.cluster_texts(texts, n_clusters=3)
    
    print("Clusters:")
    for cluster_id, cluster_texts in clusters.items():
        print(f"\n  Cluster {cluster_id + 1}:")
        for text in cluster_texts:
            print(f"    - {text}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "search":
            demo_semantic_search()
        elif mode == "cluster":
            demo_clustering()
        else:
            print("Usage: python embedding_basics.py [search|cluster]")
    else:
        demo_basic_embeddings()
        
        print("\n" + "=" * 60)
        print("ğŸ’¡ ë” ë§ì€ ë°ëª¨:")
        print("  python embedding_basics.py search   # ì˜ë¯¸ ê²€ìƒ‰")
        print("  python embedding_basics.py cluster  # í´ëŸ¬ìŠ¤í„°ë§")