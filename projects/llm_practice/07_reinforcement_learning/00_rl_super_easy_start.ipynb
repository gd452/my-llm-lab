{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎮 RL 완전 쉽게 시작하기: 1차원 보물찾기\n",
    "\n",
    "## 목표\n",
    "- 복잡한 수학 없이 RL의 핵심 개념 체험\n",
    "- 직관적인 예제로 Q-learning 이해\n",
    "- 01_rl_basics.ipynb를 위한 워밍업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 강화학습이 뭔가요?\n",
    "\n",
    "**한 줄 요약**: 시행착오를 통해 배우는 AI\n",
    "\n",
    "일상 예시:\n",
    "- 🎮 게임: 죽으면서 배우는 다크소울\n",
    "- 🚗 운전: 연습하면서 실력이 늘어감\n",
    "- 🍳 요리: 맛없으면 다음엔 다르게 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"🚀 준비 완료! 이제 시작해봅시다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ 초간단 세계 만들기 (1차원!)\n",
    "\n",
    "복잡한 2D 격자 대신, 일직선 세계부터!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리의 작은 세계 (5칸짜리)\n",
    "world = ['🏠', '⬜', '🕳️', '⬜', '💎']\n",
    "#        시작   빈칸   함정   빈칸  보물\n",
    "\n",
    "# 시각화 함수\n",
    "def show_world(position):\n",
    "    \"\"\"현재 위치를 보여주는 함수\"\"\"\n",
    "    display = []\n",
    "    for i, cell in enumerate(world):\n",
    "        if i == position:\n",
    "            display.append(f'[😊{cell}]')\n",
    "        else:\n",
    "            display.append(f' {cell} ')\n",
    "    print(' '.join(display))\n",
    "    print(' 0    1    2    3    4  (위치 번호)')\n",
    "\n",
    "print(\"🗺️ 우리의 게임 맵:\")\n",
    "show_world(0)\n",
    "print(\"\\n목표: 🏠에서 출발해서 💎을 찾아가세요!\")\n",
    "print(\"주의: 🕳️ 함정을 피하세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ 게임 규칙 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동: 정말 간단하게 2개만!\n",
    "actions = {\n",
    "    0: '⬅️ 왼쪽',\n",
    "    1: '➡️ 오른쪽'\n",
    "}\n",
    "\n",
    "# 보상 시스템 (점수)\n",
    "rewards = {\n",
    "    '🏠': 0,     # 시작점: 0점\n",
    "    '⬜': -1,    # 빈칸: -1점 (빨리 가도록)\n",
    "    '🕳️': -10,  # 함정: -10점 (피해야 함!)\n",
    "    '💎': +10    # 보물: +10점 (목표!)\n",
    "}\n",
    "\n",
    "print(\"🎮 게임 규칙:\")\n",
    "print(\"\\n행동:\")\n",
    "for key, value in actions.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n보상:\")\n",
    "for symbol, reward in rewards.items():\n",
    "    print(f\"  {symbol}: {reward:+3d}점\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ 에이전트 만들기 (랜덤부터)\n",
    "\n",
    "먼저 아무것도 모르는 에이전트를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_play():\n",
    "    \"\"\"랜덤하게 행동하는 에이전트\"\"\"\n",
    "    position = 0  # 시작 위치\n",
    "    total_reward = 0\n",
    "    path = [position]\n",
    "    \n",
    "    print(\"🎲 랜덤 에이전트의 여정:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for step in range(10):  # 최대 10걸음\n",
    "        # 랜덤 행동 선택\n",
    "        action = np.random.randint(2)\n",
    "        \n",
    "        # 이동\n",
    "        if action == 0:  # 왼쪽\n",
    "            new_position = max(0, position - 1)\n",
    "        else:  # 오른쪽\n",
    "            new_position = min(4, position + 1)\n",
    "        \n",
    "        # 보상 받기\n",
    "        reward = rewards[world[new_position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Step {step+1}: {world[position]} → {actions[action]} → {world[new_position]} (보상: {reward:+d})\")\n",
    "        \n",
    "        position = new_position\n",
    "        path.append(position)\n",
    "        \n",
    "        # 목표 도달 또는 함정\n",
    "        if position == 4:  # 보물 찾음!\n",
    "            print(\"\\n🎉 성공! 보물을 찾았습니다!\")\n",
    "            break\n",
    "        elif position == 2:  # 함정에 빠짐\n",
    "            print(\"\\n😱 함정에 빠졌습니다!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n총 보상: {total_reward:+d}점\")\n",
    "    return total_reward, path\n",
    "\n",
    "# 랜덤 에이전트 실행\n",
    "reward, path = random_agent_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Q-Learning: 경험으로 배우기\n",
    "\n",
    "### Q-테이블이란?\n",
    "각 위치에서 각 행동이 얼마나 좋은지 점수를 매긴 표\n",
    "\n",
    "예: Q[0][1] = 시작점(0)에서 오른쪽(1)으로 가는 것의 가치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화\n",
    "Q = np.zeros((5, 2))  # 5개 위치, 2개 행동\n",
    "\n",
    "def show_q_table(Q):\n",
    "    \"\"\"Q-테이블을 예쁘게 출력\"\"\"\n",
    "    print(\"\\n📊 Q-테이블 (각 상태-행동의 가치):\")\n",
    "    print(\"위치     ⬅️왼쪽   ➡️오른쪽\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, symbol in enumerate(world):\n",
    "        print(f\"{i}:{symbol}    {Q[i][0]:6.2f}   {Q[i][1]:6.2f}\")\n",
    "\n",
    "show_q_table(Q)\n",
    "print(\"\\n💡 처음엔 모든 값이 0 (아무것도 모름)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ 학습 시작! (100번 연습)\n",
    "\n",
    "### 핵심 공식 (겁내지 마세요!)\n",
    "```\n",
    "새로운 지식 = 기존 지식 + 학습률 × (새로 배운 것 - 기존 지식)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 (조절 가능한 설정값)\n",
    "learning_rate = 0.1   # 새 정보를 얼마나 빨리 받아들일까? (0~1)\n",
    "discount = 0.9        # 미래 보상을 얼마나 중요하게 볼까? (0~1)\n",
    "epsilon = 0.2         # 얼마나 자주 새로운 시도를 할까? (0~1)\n",
    "\n",
    "print(\"🎓 Q-Learning 시작!\")\n",
    "print(f\"학습률: {learning_rate}, 할인율: {discount}, 탐험율: {epsilon}\")\n",
    "print(\"\\n100번 게임을 하면서 배워봅시다...\\n\")\n",
    "\n",
    "# 학습 기록\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(100):\n",
    "    position = 0  # 시작 위치\n",
    "    total_reward = 0\n",
    "    \n",
    "    # 한 에피소드 (한 번의 게임)\n",
    "    for step in range(20):  # 최대 20걸음\n",
    "        # ε-greedy 행동 선택\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(2)  # 탐험: 랜덤\n",
    "        else:\n",
    "            action = np.argmax(Q[position])  # 활용: 최선의 선택\n",
    "        \n",
    "        # 행동 실행\n",
    "        old_position = position\n",
    "        if action == 0:  # 왼쪽\n",
    "            position = max(0, position - 1)\n",
    "        else:  # 오른쪽\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        # 보상 받기\n",
    "        reward = rewards[world[position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Q-테이블 업데이트 (핵심!)\n",
    "        if position == 4:  # 목표 도달\n",
    "            Q[old_position][action] += learning_rate * (reward - Q[old_position][action])\n",
    "            break\n",
    "        elif position == 2:  # 함정\n",
    "            Q[old_position][action] += learning_rate * (reward - Q[old_position][action])\n",
    "            break\n",
    "        else:\n",
    "            # Q-learning 공식\n",
    "            best_next_action = np.max(Q[position])\n",
    "            Q[old_position][action] += learning_rate * (\n",
    "                reward + discount * best_next_action - Q[old_position][action]\n",
    "            )\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    # 진행 상황 출력\n",
    "    if (episode + 1) % 20 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-20:])\n",
    "        print(f\"에피소드 {episode+1:3d}: 평균 보상 = {avg_reward:+.2f}\")\n",
    "\n",
    "print(\"\\n✅ 학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 Q-테이블 보기\n",
    "show_q_table(Q)\n",
    "\n",
    "print(\"\\n💡 해석:\")\n",
    "print(\"- 양수(+): 좋은 행동\")\n",
    "print(\"- 음수(-): 나쁜 행동\")\n",
    "print(\"- 큰 값: 더 확실한 선택\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 그리기\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards, alpha=0.5, label='에피소드별 보상')\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "         linewidth=2, label='이동 평균 (10 에피소드)')\n",
    "plt.xlabel('에피소드')\n",
    "plt.ylabel('총 보상')\n",
    "plt.title('학습 진행 상황')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 보상이 점점 증가 = 학습이 되고 있음!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ 학습된 에이전트 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_agent_play(Q, visualize=True):\n",
    "    \"\"\"학습된 Q-테이블을 사용하는 똑똑한 에이전트\"\"\"\n",
    "    position = 0\n",
    "    path = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"🤖 학습된 에이전트의 최적 경로:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for step in range(10):\n",
    "        if visualize:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"\\nStep {step}:\")\n",
    "            show_world(position)\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Q-테이블에서 최선의 행동 선택\n",
    "        action = np.argmax(Q[position])\n",
    "        \n",
    "        # 이동\n",
    "        old_position = position\n",
    "        if action == 0:\n",
    "            position = max(0, position - 1)\n",
    "        else:\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        path.append((old_position, action, position))\n",
    "        reward = rewards[world[position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        if position == 4:\n",
    "            if visualize:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"\\nStep {step+1}:\")\n",
    "                show_world(position)\n",
    "            print(\"\\n🎉 성공! 보물을 찾았습니다!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n경로:\")\n",
    "    for old_pos, act, new_pos in path:\n",
    "        print(f\"  {world[old_pos]} → {actions[act]} → {world[new_pos]}\")\n",
    "    \n",
    "    print(f\"\\n총 보상: {total_reward:+d}점\")\n",
    "    print(f\"총 걸음: {len(path)}걸음\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "# 실행!\n",
    "path = smart_agent_play(Q, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ 핵심 개념 정리\n",
    "\n",
    "### RL의 5가지 핵심 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = {\n",
    "    \"1. Agent (에이전트)\": \"보물을 찾는 😊 (의사결정자)\",\n",
    "    \"2. Environment (환경)\": \"5칸짜리 세계 🏠⬜🕳️⬜💎\",\n",
    "    \"3. State (상태)\": \"현재 위치 (0~4)\",\n",
    "    \"4. Action (행동)\": \"왼쪽/오른쪽 이동 ⬅️➡️\",\n",
    "    \"5. Reward (보상)\": \"각 위치의 점수 (+10, -10, -1, 0)\"\n",
    "}\n",
    "\n",
    "print(\"🎯 강화학습 핵심 개념:\")\n",
    "print(\"=\" * 40)\n",
    "for concept, explanation in concepts.items():\n",
    "    print(f\"{concept:20s} = {explanation}\")\n",
    "\n",
    "print(\"\\n📚 Q-Learning이란?\")\n",
    "print(\"- Q(상태, 행동) = 그 상태에서 그 행동의 예상 가치\")\n",
    "print(\"- 경험을 통해 Q값을 업데이트\")\n",
    "print(\"- 최종적으로 각 상태에서 최선의 행동을 알게 됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ 실험해보기\n",
    "\n",
    "### 🧪 실험 1: 함정을 더 위험하게 만들면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함정 패널티를 -50으로 변경\n",
    "rewards_extreme = rewards.copy()\n",
    "rewards_extreme['🕳️'] = -50\n",
    "\n",
    "print(\"🔥 극한 모드: 함정 = -50점\")\n",
    "print(\"이제 에이전트가 어떻게 행동할까요?\")\n",
    "\n",
    "# 새로운 Q-테이블로 학습\n",
    "Q_extreme = np.zeros((5, 2))\n",
    "\n",
    "# 빠른 학습 (50 에피소드만)\n",
    "for episode in range(50):\n",
    "    position = 0\n",
    "    for step in range(20):\n",
    "        if np.random.random() < 0.1:  # 적은 탐험\n",
    "            action = np.random.randint(2)\n",
    "        else:\n",
    "            action = np.argmax(Q_extreme[position])\n",
    "        \n",
    "        old_position = position\n",
    "        if action == 0:\n",
    "            position = max(0, position - 1)\n",
    "        else:\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        reward = rewards_extreme[world[position]]\n",
    "        \n",
    "        if position in [2, 4]:  # 함정 또는 목표\n",
    "            Q_extreme[old_position][action] += 0.1 * (reward - Q_extreme[old_position][action])\n",
    "            break\n",
    "        else:\n",
    "            best_next = np.max(Q_extreme[position])\n",
    "            Q_extreme[old_position][action] += 0.1 * (\n",
    "                reward + 0.9 * best_next - Q_extreme[old_position][action]\n",
    "            )\n",
    "\n",
    "show_q_table(Q_extreme)\n",
    "print(\"\\n💡 함정(위치 2)으로 가는 행동의 Q값이 매우 낮음!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 다음 단계\n",
    "\n",
    "### 이제 이해한 것:\n",
    "✅ Agent, Environment, State, Action, Reward  \n",
    "✅ Q-테이블과 Q-learning  \n",
    "✅ 탐험 vs 활용 (ε-greedy)  \n",
    "✅ 학습률과 할인율  \n",
    "\n",
    "### 다음 도전:\n",
    "1. **2D GridWorld** (01_rl_basics.ipynb)\n",
    "   - 이제 2차원으로 확장!\n",
    "   - 같은 원리, 더 큰 세계\n",
    "\n",
    "2. **OpenAI Gym 환경**\n",
    "   ```python\n",
    "   import gym\n",
    "   env = gym.make('FrozenLake-v1')\n",
    "   ```\n",
    "\n",
    "3. **추천 학습 자료**\n",
    "   - 🎥 David Silver RL Course (Lecture 1-3)\n",
    "   - 📖 Sutton & Barto Chapter 1-3\n",
    "   - 🇰🇷 모두를 위한 RL (김성훈 교수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 축하합니다! RL의 핵심을 이해하셨습니다!\")\n",
    "print(\"\\n💪 이제 01_rl_basics.ipynb가 훨씬 쉬워질 거예요!\")\n",
    "print(\"\\n다음 명령어로 계속하세요:\")\n",
    "print(\"jupyter notebook 01_rl_basics.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}