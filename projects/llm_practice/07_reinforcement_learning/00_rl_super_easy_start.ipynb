{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ® RL ì™„ì „ ì‰½ê²Œ ì‹œì‘í•˜ê¸°: 1ì°¨ì› ë³´ë¬¼ì°¾ê¸°\n",
    "\n",
    "## ëª©í‘œ\n",
    "- ë³µì¡í•œ ìˆ˜í•™ ì—†ì´ RLì˜ í•µì‹¬ ê°œë… ì²´í—˜\n",
    "- ì§ê´€ì ì¸ ì˜ˆì œë¡œ Q-learning ì´í•´\n",
    "- 01_rl_basics.ipynbë¥¼ ìœ„í•œ ì›Œë°ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ê°•í™”í•™ìŠµì´ ë­”ê°€ìš”?\n",
    "\n",
    "**í•œ ì¤„ ìš”ì•½**: ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ë°°ìš°ëŠ” AI\n",
    "\n",
    "ì¼ìƒ ì˜ˆì‹œ:\n",
    "- ğŸ® ê²Œì„: ì£½ìœ¼ë©´ì„œ ë°°ìš°ëŠ” ë‹¤í¬ì†Œìš¸\n",
    "- ğŸš— ìš´ì „: ì—°ìŠµí•˜ë©´ì„œ ì‹¤ë ¥ì´ ëŠ˜ì–´ê°\n",
    "- ğŸ³ ìš”ë¦¬: ë§›ì—†ìœ¼ë©´ ë‹¤ìŒì—” ë‹¤ë¥´ê²Œ í•´ë´„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì‹œì‘í•´ë´…ì‹œë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ì´ˆê°„ë‹¨ ì„¸ê³„ ë§Œë“¤ê¸° (1ì°¨ì›!)\n",
    "\n",
    "ë³µì¡í•œ 2D ê²©ì ëŒ€ì‹ , ì¼ì§ì„  ì„¸ê³„ë¶€í„°!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš°ë¦¬ì˜ ì‘ì€ ì„¸ê³„ (5ì¹¸ì§œë¦¬)\n",
    "world = ['ğŸ ', 'â¬œ', 'ğŸ•³ï¸', 'â¬œ', 'ğŸ’']\n",
    "#        ì‹œì‘   ë¹ˆì¹¸   í•¨ì •   ë¹ˆì¹¸  ë³´ë¬¼\n",
    "\n",
    "# ì‹œê°í™” í•¨ìˆ˜\n",
    "def show_world(position):\n",
    "    \"\"\"í˜„ì¬ ìœ„ì¹˜ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    display = []\n",
    "    for i, cell in enumerate(world):\n",
    "        if i == position:\n",
    "            display.append(f'[ğŸ˜Š{cell}]')\n",
    "        else:\n",
    "            display.append(f' {cell} ')\n",
    "    print(' '.join(display))\n",
    "    print(' 0    1    2    3    4  (ìœ„ì¹˜ ë²ˆí˜¸)')\n",
    "\n",
    "print(\"ğŸ—ºï¸ ìš°ë¦¬ì˜ ê²Œì„ ë§µ:\")\n",
    "show_world(0)\n",
    "print(\"\\nëª©í‘œ: ğŸ ì—ì„œ ì¶œë°œí•´ì„œ ğŸ’ì„ ì°¾ì•„ê°€ì„¸ìš”!\")\n",
    "print(\"ì£¼ì˜: ğŸ•³ï¸ í•¨ì •ì„ í”¼í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ê²Œì„ ê·œì¹™ ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í–‰ë™: ì •ë§ ê°„ë‹¨í•˜ê²Œ 2ê°œë§Œ!\n",
    "actions = {\n",
    "    0: 'â¬…ï¸ ì™¼ìª½',\n",
    "    1: 'â¡ï¸ ì˜¤ë¥¸ìª½'\n",
    "}\n",
    "\n",
    "# ë³´ìƒ ì‹œìŠ¤í…œ (ì ìˆ˜)\n",
    "rewards = {\n",
    "    'ğŸ ': 0,     # ì‹œì‘ì : 0ì \n",
    "    'â¬œ': -1,    # ë¹ˆì¹¸: -1ì  (ë¹¨ë¦¬ ê°€ë„ë¡)\n",
    "    'ğŸ•³ï¸': -10,  # í•¨ì •: -10ì  (í”¼í•´ì•¼ í•¨!)\n",
    "    'ğŸ’': +10    # ë³´ë¬¼: +10ì  (ëª©í‘œ!)\n",
    "}\n",
    "\n",
    "print(\"ğŸ® ê²Œì„ ê·œì¹™:\")\n",
    "print(\"\\ní–‰ë™:\")\n",
    "for key, value in actions.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\në³´ìƒ:\")\n",
    "for symbol, reward in rewards.items():\n",
    "    print(f\"  {symbol}: {reward:+3d}ì \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ì—ì´ì „íŠ¸ ë§Œë“¤ê¸° (ëœë¤ë¶€í„°)\n",
    "\n",
    "ë¨¼ì € ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_play():\n",
    "    \"\"\"ëœë¤í•˜ê²Œ í–‰ë™í•˜ëŠ” ì—ì´ì „íŠ¸\"\"\"\n",
    "    position = 0  # ì‹œì‘ ìœ„ì¹˜\n",
    "    total_reward = 0\n",
    "    path = [position]\n",
    "    \n",
    "    print(\"ğŸ² ëœë¤ ì—ì´ì „íŠ¸ì˜ ì—¬ì •:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for step in range(10):  # ìµœëŒ€ 10ê±¸ìŒ\n",
    "        # ëœë¤ í–‰ë™ ì„ íƒ\n",
    "        action = np.random.randint(2)\n",
    "        \n",
    "        # ì´ë™\n",
    "        if action == 0:  # ì™¼ìª½\n",
    "            new_position = max(0, position - 1)\n",
    "        else:  # ì˜¤ë¥¸ìª½\n",
    "            new_position = min(4, position + 1)\n",
    "        \n",
    "        # ë³´ìƒ ë°›ê¸°\n",
    "        reward = rewards[world[new_position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Step {step+1}: {world[position]} â†’ {actions[action]} â†’ {world[new_position]} (ë³´ìƒ: {reward:+d})\")\n",
    "        \n",
    "        position = new_position\n",
    "        path.append(position)\n",
    "        \n",
    "        # ëª©í‘œ ë„ë‹¬ ë˜ëŠ” í•¨ì •\n",
    "        if position == 4:  # ë³´ë¬¼ ì°¾ìŒ!\n",
    "            print(\"\\nğŸ‰ ì„±ê³µ! ë³´ë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "            break\n",
    "        elif position == 2:  # í•¨ì •ì— ë¹ ì§\n",
    "            print(\"\\nğŸ˜± í•¨ì •ì— ë¹ ì¡ŒìŠµë‹ˆë‹¤!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nì´ ë³´ìƒ: {total_reward:+d}ì \")\n",
    "    return total_reward, path\n",
    "\n",
    "# ëœë¤ ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "reward, path = random_agent_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Q-Learning: ê²½í—˜ìœ¼ë¡œ ë°°ìš°ê¸°\n",
    "\n",
    "### Q-í…Œì´ë¸”ì´ë€?\n",
    "ê° ìœ„ì¹˜ì—ì„œ ê° í–‰ë™ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ì ìˆ˜ë¥¼ ë§¤ê¸´ í‘œ\n",
    "\n",
    "ì˜ˆ: Q[0][1] = ì‹œì‘ì (0)ì—ì„œ ì˜¤ë¥¸ìª½(1)ìœ¼ë¡œ ê°€ëŠ” ê²ƒì˜ ê°€ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-í…Œì´ë¸” ì´ˆê¸°í™”\n",
    "Q = np.zeros((5, 2))  # 5ê°œ ìœ„ì¹˜, 2ê°œ í–‰ë™\n",
    "\n",
    "def show_q_table(Q):\n",
    "    \"\"\"Q-í…Œì´ë¸”ì„ ì˜ˆì˜ê²Œ ì¶œë ¥\"\"\"\n",
    "    print(\"\\nğŸ“Š Q-í…Œì´ë¸” (ê° ìƒíƒœ-í–‰ë™ì˜ ê°€ì¹˜):\")\n",
    "    print(\"ìœ„ì¹˜     â¬…ï¸ì™¼ìª½   â¡ï¸ì˜¤ë¥¸ìª½\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, symbol in enumerate(world):\n",
    "        print(f\"{i}:{symbol}    {Q[i][0]:6.2f}   {Q[i][1]:6.2f}\")\n",
    "\n",
    "show_q_table(Q)\n",
    "print(\"\\nğŸ’¡ ì²˜ìŒì—” ëª¨ë“  ê°’ì´ 0 (ì•„ë¬´ê²ƒë„ ëª¨ë¦„)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ í•™ìŠµ ì‹œì‘! (100ë²ˆ ì—°ìŠµ)\n",
    "\n",
    "### í•µì‹¬ ê³µì‹ (ê²ë‚´ì§€ ë§ˆì„¸ìš”!)\n",
    "```\n",
    "ìƒˆë¡œìš´ ì§€ì‹ = ê¸°ì¡´ ì§€ì‹ + í•™ìŠµë¥  Ã— (ìƒˆë¡œ ë°°ìš´ ê²ƒ - ê¸°ì¡´ ì§€ì‹)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì¡°ì ˆ ê°€ëŠ¥í•œ ì„¤ì •ê°’)\n",
    "learning_rate = 0.1   # ìƒˆ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë°›ì•„ë“¤ì¼ê¹Œ? (0~1)\n",
    "discount = 0.9        # ë¯¸ë˜ ë³´ìƒì„ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ë³¼ê¹Œ? (0~1)\n",
    "epsilon = 0.2         # ì–¼ë§ˆë‚˜ ìì£¼ ìƒˆë¡œìš´ ì‹œë„ë¥¼ í• ê¹Œ? (0~1)\n",
    "\n",
    "print(\"ğŸ“ Q-Learning ì‹œì‘!\")\n",
    "print(f\"í•™ìŠµë¥ : {learning_rate}, í• ì¸ìœ¨: {discount}, íƒí—˜ìœ¨: {epsilon}\")\n",
    "print(\"\\n100ë²ˆ ê²Œì„ì„ í•˜ë©´ì„œ ë°°ì›Œë´…ì‹œë‹¤...\\n\")\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(100):\n",
    "    position = 0  # ì‹œì‘ ìœ„ì¹˜\n",
    "    total_reward = 0\n",
    "    \n",
    "    # í•œ ì—í”¼ì†Œë“œ (í•œ ë²ˆì˜ ê²Œì„)\n",
    "    for step in range(20):  # ìµœëŒ€ 20ê±¸ìŒ\n",
    "        # Îµ-greedy í–‰ë™ ì„ íƒ\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(2)  # íƒí—˜: ëœë¤\n",
    "        else:\n",
    "            action = np.argmax(Q[position])  # í™œìš©: ìµœì„ ì˜ ì„ íƒ\n",
    "        \n",
    "        # í–‰ë™ ì‹¤í–‰\n",
    "        old_position = position\n",
    "        if action == 0:  # ì™¼ìª½\n",
    "            position = max(0, position - 1)\n",
    "        else:  # ì˜¤ë¥¸ìª½\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        # ë³´ìƒ ë°›ê¸°\n",
    "        reward = rewards[world[position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸ (í•µì‹¬!)\n",
    "        if position == 4:  # ëª©í‘œ ë„ë‹¬\n",
    "            Q[old_position][action] += learning_rate * (reward - Q[old_position][action])\n",
    "            break\n",
    "        elif position == 2:  # í•¨ì •\n",
    "            Q[old_position][action] += learning_rate * (reward - Q[old_position][action])\n",
    "            break\n",
    "        else:\n",
    "            # Q-learning ê³µì‹\n",
    "            best_next_action = np.max(Q[position])\n",
    "            Q[old_position][action] += learning_rate * (\n",
    "                reward + discount * best_next_action - Q[old_position][action]\n",
    "            )\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "    if (episode + 1) % 20 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-20:])\n",
    "        print(f\"ì—í”¼ì†Œë“œ {episode+1:3d}: í‰ê·  ë³´ìƒ = {avg_reward:+.2f}\")\n",
    "\n",
    "print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ í•™ìŠµ ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ Q-í…Œì´ë¸” ë³´ê¸°\n",
    "show_q_table(Q)\n",
    "\n",
    "print(\"\\nğŸ’¡ í•´ì„:\")\n",
    "print(\"- ì–‘ìˆ˜(+): ì¢‹ì€ í–‰ë™\")\n",
    "print(\"- ìŒìˆ˜(-): ë‚˜ìœ í–‰ë™\")\n",
    "print(\"- í° ê°’: ë” í™•ì‹¤í•œ ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards, alpha=0.5, label='ì—í”¼ì†Œë“œë³„ ë³´ìƒ')\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "         linewidth=2, label='ì´ë™ í‰ê·  (10 ì—í”¼ì†Œë“œ)')\n",
    "plt.xlabel('ì—í”¼ì†Œë“œ')\n",
    "plt.ylabel('ì´ ë³´ìƒ')\n",
    "plt.title('í•™ìŠµ ì§„í–‰ ìƒí™©')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“ˆ ë³´ìƒì´ ì ì  ì¦ê°€ = í•™ìŠµì´ ë˜ê³  ìˆìŒ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ í•™ìŠµëœ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_agent_play(Q, visualize=True):\n",
    "    \"\"\"í•™ìŠµëœ Q-í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ëŠ” ë˜‘ë˜‘í•œ ì—ì´ì „íŠ¸\"\"\"\n",
    "    position = 0\n",
    "    path = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"ğŸ¤– í•™ìŠµëœ ì—ì´ì „íŠ¸ì˜ ìµœì  ê²½ë¡œ:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for step in range(10):\n",
    "        if visualize:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"\\nStep {step}:\")\n",
    "            show_world(position)\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Q-í…Œì´ë¸”ì—ì„œ ìµœì„ ì˜ í–‰ë™ ì„ íƒ\n",
    "        action = np.argmax(Q[position])\n",
    "        \n",
    "        # ì´ë™\n",
    "        old_position = position\n",
    "        if action == 0:\n",
    "            position = max(0, position - 1)\n",
    "        else:\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        path.append((old_position, action, position))\n",
    "        reward = rewards[world[position]]\n",
    "        total_reward += reward\n",
    "        \n",
    "        if position == 4:\n",
    "            if visualize:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"\\nStep {step+1}:\")\n",
    "                show_world(position)\n",
    "            print(\"\\nğŸ‰ ì„±ê³µ! ë³´ë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nê²½ë¡œ:\")\n",
    "    for old_pos, act, new_pos in path:\n",
    "        print(f\"  {world[old_pos]} â†’ {actions[act]} â†’ {world[new_pos]}\")\n",
    "    \n",
    "    print(f\"\\nì´ ë³´ìƒ: {total_reward:+d}ì \")\n",
    "    print(f\"ì´ ê±¸ìŒ: {len(path)}ê±¸ìŒ\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "# ì‹¤í–‰!\n",
    "path = smart_agent_play(Q, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "### RLì˜ 5ê°€ì§€ í•µì‹¬ ìš”ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = {\n",
    "    \"1. Agent (ì—ì´ì „íŠ¸)\": \"ë³´ë¬¼ì„ ì°¾ëŠ” ğŸ˜Š (ì˜ì‚¬ê²°ì •ì)\",\n",
    "    \"2. Environment (í™˜ê²½)\": \"5ì¹¸ì§œë¦¬ ì„¸ê³„ ğŸ â¬œğŸ•³ï¸â¬œğŸ’\",\n",
    "    \"3. State (ìƒíƒœ)\": \"í˜„ì¬ ìœ„ì¹˜ (0~4)\",\n",
    "    \"4. Action (í–‰ë™)\": \"ì™¼ìª½/ì˜¤ë¥¸ìª½ ì´ë™ â¬…ï¸â¡ï¸\",\n",
    "    \"5. Reward (ë³´ìƒ)\": \"ê° ìœ„ì¹˜ì˜ ì ìˆ˜ (+10, -10, -1, 0)\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ ê°•í™”í•™ìŠµ í•µì‹¬ ê°œë…:\")\n",
    "print(\"=\" * 40)\n",
    "for concept, explanation in concepts.items():\n",
    "    print(f\"{concept:20s} = {explanation}\")\n",
    "\n",
    "print(\"\\nğŸ“š Q-Learningì´ë€?\")\n",
    "print(\"- Q(ìƒíƒœ, í–‰ë™) = ê·¸ ìƒíƒœì—ì„œ ê·¸ í–‰ë™ì˜ ì˜ˆìƒ ê°€ì¹˜\")\n",
    "print(\"- ê²½í—˜ì„ í†µí•´ Qê°’ì„ ì—…ë°ì´íŠ¸\")\n",
    "print(\"- ìµœì¢…ì ìœ¼ë¡œ ê° ìƒíƒœì—ì„œ ìµœì„ ì˜ í–‰ë™ì„ ì•Œê²Œ ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ ì‹¤í—˜í•´ë³´ê¸°\n",
    "\n",
    "### ğŸ§ª ì‹¤í—˜ 1: í•¨ì •ì„ ë” ìœ„í—˜í•˜ê²Œ ë§Œë“¤ë©´?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ì • íŒ¨ë„í‹°ë¥¼ -50ìœ¼ë¡œ ë³€ê²½\n",
    "rewards_extreme = rewards.copy()\n",
    "rewards_extreme['ğŸ•³ï¸'] = -50\n",
    "\n",
    "print(\"ğŸ”¥ ê·¹í•œ ëª¨ë“œ: í•¨ì • = -50ì \")\n",
    "print(\"ì´ì œ ì—ì´ì „íŠ¸ê°€ ì–´ë–»ê²Œ í–‰ë™í• ê¹Œìš”?\")\n",
    "\n",
    "# ìƒˆë¡œìš´ Q-í…Œì´ë¸”ë¡œ í•™ìŠµ\n",
    "Q_extreme = np.zeros((5, 2))\n",
    "\n",
    "# ë¹ ë¥¸ í•™ìŠµ (50 ì—í”¼ì†Œë“œë§Œ)\n",
    "for episode in range(50):\n",
    "    position = 0\n",
    "    for step in range(20):\n",
    "        if np.random.random() < 0.1:  # ì ì€ íƒí—˜\n",
    "            action = np.random.randint(2)\n",
    "        else:\n",
    "            action = np.argmax(Q_extreme[position])\n",
    "        \n",
    "        old_position = position\n",
    "        if action == 0:\n",
    "            position = max(0, position - 1)\n",
    "        else:\n",
    "            position = min(4, position + 1)\n",
    "        \n",
    "        reward = rewards_extreme[world[position]]\n",
    "        \n",
    "        if position in [2, 4]:  # í•¨ì • ë˜ëŠ” ëª©í‘œ\n",
    "            Q_extreme[old_position][action] += 0.1 * (reward - Q_extreme[old_position][action])\n",
    "            break\n",
    "        else:\n",
    "            best_next = np.max(Q_extreme[position])\n",
    "            Q_extreme[old_position][action] += 0.1 * (\n",
    "                reward + 0.9 * best_next - Q_extreme[old_position][action]\n",
    "            )\n",
    "\n",
    "show_q_table(Q_extreme)\n",
    "print(\"\\nğŸ’¡ í•¨ì •(ìœ„ì¹˜ 2)ìœ¼ë¡œ ê°€ëŠ” í–‰ë™ì˜ Qê°’ì´ ë§¤ìš° ë‚®ìŒ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì´ì œ ì´í•´í•œ ê²ƒ:\n",
    "âœ… Agent, Environment, State, Action, Reward  \n",
    "âœ… Q-í…Œì´ë¸”ê³¼ Q-learning  \n",
    "âœ… íƒí—˜ vs í™œìš© (Îµ-greedy)  \n",
    "âœ… í•™ìŠµë¥ ê³¼ í• ì¸ìœ¨  \n",
    "\n",
    "### ë‹¤ìŒ ë„ì „:\n",
    "1. **2D GridWorld** (01_rl_basics.ipynb)\n",
    "   - ì´ì œ 2ì°¨ì›ìœ¼ë¡œ í™•ì¥!\n",
    "   - ê°™ì€ ì›ë¦¬, ë” í° ì„¸ê³„\n",
    "\n",
    "2. **OpenAI Gym í™˜ê²½**\n",
    "   ```python\n",
    "   import gym\n",
    "   env = gym.make('FrozenLake-v1')\n",
    "   ```\n",
    "\n",
    "3. **ì¶”ì²œ í•™ìŠµ ìë£Œ**\n",
    "   - ğŸ¥ David Silver RL Course (Lecture 1-3)\n",
    "   - ğŸ“– Sutton & Barto Chapter 1-3\n",
    "   - ğŸ‡°ğŸ‡· ëª¨ë‘ë¥¼ ìœ„í•œ RL (ê¹€ì„±í›ˆ êµìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! RLì˜ í•µì‹¬ì„ ì´í•´í•˜ì…¨ìŠµë‹ˆë‹¤!\")\n",
    "print(\"\\nğŸ’ª ì´ì œ 01_rl_basics.ipynbê°€ í›¨ì”¬ ì‰¬ì›Œì§ˆ ê±°ì˜ˆìš”!\")\n",
    "print(\"\\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ê³„ì†í•˜ì„¸ìš”:\")\n",
    "print(\"jupyter notebook 01_rl_basics.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}