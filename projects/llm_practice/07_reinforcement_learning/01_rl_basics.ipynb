{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: RL 기초 - MDP와 가치 함수\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- 강화학습의 핵심 개념 이해\n",
    "- MDP (Markov Decision Process) 구조 파악\n",
    "- 가치 함수와 Bellman 방정식 학습\n",
    "- GridWorld에서 정책 평가와 가치 반복 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 강화학습이란?\n",
    "\n",
    "강화학습(Reinforcement Learning)은 **에이전트(Agent)**가 **환경(Environment)**과 상호작용하며 **보상(Reward)**을 최대화하는 **행동(Action)**을 학습하는 방법입니다.\n",
    "\n",
    "### 핵심 구성 요소\n",
    "- **Agent**: 학습하고 결정을 내리는 주체\n",
    "- **Environment**: 에이전트가 상호작용하는 세계\n",
    "- **State (s)**: 현재 상황을 나타내는 정보\n",
    "- **Action (a)**: 에이전트가 취할 수 있는 행동\n",
    "- **Reward (r)**: 행동의 결과로 받는 즉각적 보상\n",
    "- **Policy (π)**: 상태에서 행동을 선택하는 전략\n",
    "- **Value (V)**: 장기적 기대 보상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"RL 기초 학습 환경 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Markov Decision Process (MDP)\n",
    "\n",
    "MDP는 강화학습 문제를 수학적으로 정의하는 프레임워크입니다.\n",
    "\n",
    "### MDP 구성 요소\n",
    "- **S**: 상태 공간 (State space)\n",
    "- **A**: 행동 공간 (Action space)  \n",
    "- **P(s'|s,a)**: 상태 전이 확률 (Transition probability)\n",
    "- **R(s,a,s')**: 보상 함수 (Reward function)\n",
    "- **γ**: 할인 인자 (Discount factor) ∈ [0,1]\n",
    "\n",
    "### Markov 속성\n",
    "미래는 현재 상태에만 의존하고, 과거 이력과는 무관합니다:\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"가능한 행동들\"\"\"\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def all(cls):\n",
    "        return list(cls)\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    \"\"\"상태를 나타내는 클래스\"\"\"\n",
    "    x: int\n",
    "    y: int\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.x, self.y))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "class GridWorldMDP:\n",
    "    \"\"\"간단한 GridWorld MDP 환경\"\"\"\n",
    "    \n",
    "    def __init__(self, width: int = 5, height: int = 5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.states = [State(x, y) for x in range(width) for y in range(height)]\n",
    "        self.actions = Action.all()\n",
    "        \n",
    "        # 특수 상태 정의\n",
    "        self.goal_state = State(width-1, height-1)  # 우하단 목표\n",
    "        self.trap_state = State(2, 2)  # 중앙 함정\n",
    "        self.start_state = State(0, 0)  # 좌상단 시작\n",
    "        \n",
    "        # 할인 인자\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        \"\"\"종료 상태 확인\"\"\"\n",
    "        return state == self.goal_state or state == self.trap_state\n",
    "    \n",
    "    def get_next_state(self, state: State, action: Action) -> State:\n",
    "        \"\"\"결정적 상태 전이 (deterministic transition)\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        x, y = state.x, state.y\n",
    "        \n",
    "        if action == Action.UP:\n",
    "            y = max(0, y - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            y = min(self.height - 1, y + 1)\n",
    "        elif action == Action.LEFT:\n",
    "            x = max(0, x - 1)\n",
    "        elif action == Action.RIGHT:\n",
    "            x = min(self.width - 1, x + 1)\n",
    "            \n",
    "        return State(x, y)\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action, next_state: State) -> float:\n",
    "        \"\"\"보상 함수\"\"\"\n",
    "        if next_state == self.goal_state:\n",
    "            return 10.0  # 목표 도달\n",
    "        elif next_state == self.trap_state:\n",
    "            return -10.0  # 함정에 빠짐\n",
    "        else:\n",
    "            return -0.1  # 각 스텝마다 작은 패널티\n",
    "    \n",
    "    def visualize(self, values: Optional[Dict[State, float]] = None, \n",
    "                  policy: Optional[Dict[State, Action]] = None):\n",
    "        \"\"\"환경 시각화\"\"\"\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for state, value in values.items():\n",
    "                grid[state.y, state.x] = value\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # 가치 함수 히트맵\n",
    "        im = ax.imshow(grid, cmap='RdYlGn', vmin=-10, vmax=10)\n",
    "        \n",
    "        # 정책 화살표\n",
    "        if policy:\n",
    "            for state, action in policy.items():\n",
    "                if not self.is_terminal(state):\n",
    "                    dx, dy = 0, 0\n",
    "                    if action == Action.UP: dy = -0.3\n",
    "                    elif action == Action.DOWN: dy = 0.3\n",
    "                    elif action == Action.LEFT: dx = -0.3\n",
    "                    elif action == Action.RIGHT: dx = 0.3\n",
    "                    \n",
    "                    ax.arrow(state.x, state.y, dx, dy, \n",
    "                            head_width=0.1, head_length=0.1, \n",
    "                            fc='black', ec='black')\n",
    "        \n",
    "        # 특수 상태 표시\n",
    "        ax.text(self.goal_state.x, self.goal_state.y, 'GOAL', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        ax.text(self.trap_state.x, self.trap_state.y, 'TRAP', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "        ax.text(self.start_state.x, self.start_state.y, 'START', \n",
    "               ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # 격자 표시\n",
    "        ax.set_xticks(np.arange(self.width))\n",
    "        ax.set_yticks(np.arange(self.height))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Value')\n",
    "        plt.title('GridWorld MDP')\n",
    "        plt.show()\n",
    "\n",
    "# MDP 환경 생성 및 시각화\n",
    "mdp = GridWorldMDP()\n",
    "print(f\"MDP 생성: {mdp.width}x{mdp.height} 그리드\")\n",
    "print(f\"시작: (0,0), 목표: ({mdp.width-1},{mdp.height-1}), 함정: (2,2)\")\n",
    "mdp.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 가치 함수 (Value Functions)\n",
    "\n",
    "### 상태 가치 함수 V(s)\n",
    "상태 s에서 시작하여 정책 π를 따를 때의 기대 누적 보상:\n",
    "$$V^\\pi(s) = E_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_{t+1} | s_0 = s]$$\n",
    "\n",
    "### 행동 가치 함수 Q(s,a)\n",
    "상태 s에서 행동 a를 취한 후 정책 π를 따를 때의 기대 누적 보상:\n",
    "$$Q^\\pi(s,a) = E_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_{t+1} | s_0 = s, a_0 = a]$$\n",
    "\n",
    "### Bellman 방정식\n",
    "가치 함수는 재귀적 관계를 만족합니다:\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"정책 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp: GridWorldMDP):\n",
    "        self.mdp = mdp\n",
    "        self.policy_dict = {}\n",
    "    \n",
    "    def get_action(self, state: State) -> Action:\n",
    "        \"\"\"상태에 대한 행동 반환\"\"\"\n",
    "        return self.policy_dict.get(state, random.choice(self.mdp.actions))\n",
    "    \n",
    "    def get_action_prob(self, state: State, action: Action) -> float:\n",
    "        \"\"\"π(a|s) - 결정적 정책\"\"\"\n",
    "        return 1.0 if self.get_action(state) == action else 0.0\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"랜덤 정책\"\"\"\n",
    "    \n",
    "    def get_action_prob(self, state: State, action: Action) -> float:\n",
    "        \"\"\"모든 행동에 동일한 확률\"\"\"\n",
    "        return 1.0 / len(self.mdp.actions)\n",
    "\n",
    "def policy_evaluation(mdp: GridWorldMDP, policy: Policy, \n",
    "                     theta: float = 0.01, max_iters: int = 1000) -> Dict[State, float]:\n",
    "    \"\"\"\n",
    "    정책 평가: 주어진 정책의 가치 함수 계산\n",
    "    Iterative Policy Evaluation 알고리즘 사용\n",
    "    \"\"\"\n",
    "    # 가치 함수 초기화\n",
    "    V = {state: 0.0 for state in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0\n",
    "        \n",
    "        # 모든 상태에 대해 업데이트\n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            v = V[state]\n",
    "            new_v = 0\n",
    "            \n",
    "            # Bellman Expectation Equation\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                \n",
    "                # π(a|s) * [R + γV(s')]\n",
    "                new_v += policy.get_action_prob(state, action) * \\\n",
    "                        (reward + mdp.gamma * V[next_state])\n",
    "            \n",
    "            V[state] = new_v\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"정책 평가 수렴 (반복: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# 랜덤 정책 평가\n",
    "random_policy = RandomPolicy(mdp)\n",
    "V_random = policy_evaluation(mdp, random_policy)\n",
    "\n",
    "print(\"\\n랜덤 정책의 가치 함수:\")\n",
    "print(f\"시작 상태 V(0,0) = {V_random[State(0,0)]:.2f}\")\n",
    "print(f\"목표 근처 V(3,3) = {V_random[State(3,3)]:.2f}\")\n",
    "print(f\"함정 근처 V(2,1) = {V_random[State(2,1)]:.2f}\")\n",
    "\n",
    "mdp.visualize(values=V_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 가치 반복 (Value Iteration)\n",
    "\n",
    "가치 반복은 **최적 가치 함수** $V^*$를 직접 계산하는 동적 프로그래밍 방법입니다.\n",
    "\n",
    "### Bellman Optimality Equation\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "### 알고리즘\n",
    "1. 모든 상태의 가치를 0으로 초기화\n",
    "2. 수렴할 때까지 반복:\n",
    "   - 각 상태에 대해 모든 행동의 가치를 계산\n",
    "   - 최대 가치를 선택하여 업데이트\n",
    "3. 최적 정책 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: GridWorldMDP, theta: float = 0.01, \n",
    "                   max_iters: int = 1000) -> Tuple[Dict[State, float], Dict[State, Action]]:\n",
    "    \"\"\"\n",
    "    가치 반복: 최적 가치 함수와 최적 정책 계산\n",
    "    \"\"\"\n",
    "    # 가치 함수 초기화\n",
    "    V = {state: 0.0 for state in mdp.states}\n",
    "    \n",
    "    # 가치 반복\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            v = V[state]\n",
    "            \n",
    "            # Bellman Optimality Equation\n",
    "            action_values = []\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                action_value = reward + mdp.gamma * V[next_state]\n",
    "                action_values.append(action_value)\n",
    "            \n",
    "            V[state] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"가치 반복 수렴 (반복: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    # 최적 정책 추출\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        if mdp.is_terminal(state):\n",
    "            continue\n",
    "        \n",
    "        action_values = []\n",
    "        for action in mdp.actions:\n",
    "            next_state = mdp.get_next_state(state, action)\n",
    "            reward = mdp.get_reward(state, action, next_state)\n",
    "            action_value = reward + mdp.gamma * V[next_state]\n",
    "            action_values.append((action_value, action))\n",
    "        \n",
    "        policy[state] = max(action_values)[1]\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# 가치 반복 실행\n",
    "V_optimal, policy_optimal = value_iteration(mdp)\n",
    "\n",
    "print(\"\\n최적 가치 함수:\")\n",
    "print(f\"시작 상태 V*(0,0) = {V_optimal[State(0,0)]:.2f}\")\n",
    "print(f\"목표 근처 V*(3,3) = {V_optimal[State(3,3)]:.2f}\")\n",
    "print(f\"함정 근처 V*(2,1) = {V_optimal[State(2,1)]:.2f}\")\n",
    "\n",
    "print(\"\\n최적 정책 (화살표로 표시):\")\n",
    "mdp.visualize(values=V_optimal, policy=policy_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 정책 반복 (Policy Iteration)\n",
    "\n",
    "정책 반복은 정책 평가와 정책 개선을 번갈아가며 수행합니다.\n",
    "\n",
    "### 알고리즘\n",
    "1. 임의의 정책으로 시작\n",
    "2. 수렴할 때까지 반복:\n",
    "   - **정책 평가**: 현재 정책의 가치 함수 계산\n",
    "   - **정책 개선**: 가치 함수를 기반으로 더 나은 정책 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: GridWorldMDP, max_iters: int = 100) -> Tuple[Dict[State, float], Dict[State, Action]]:\n",
    "    \"\"\"\n",
    "    정책 반복: 정책 평가와 개선을 반복하여 최적 정책 찾기\n",
    "    \"\"\"\n",
    "    # 임의의 정책으로 초기화 (모두 위로 가기)\n",
    "    policy = Policy(mdp)\n",
    "    for state in mdp.states:\n",
    "        policy.policy_dict[state] = Action.UP\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # 1. 정책 평가\n",
    "        V = policy_evaluation(mdp, policy, theta=0.01)\n",
    "        \n",
    "        # 2. 정책 개선\n",
    "        policy_stable = True\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            old_action = policy.get_action(state)\n",
    "            \n",
    "            # 각 행동의 가치 계산\n",
    "            action_values = []\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                action_value = reward + mdp.gamma * V[next_state]\n",
    "                action_values.append((action_value, action))\n",
    "            \n",
    "            # 최선의 행동 선택 (greedy)\n",
    "            best_action = max(action_values)[1]\n",
    "            policy.policy_dict[state] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"정책 반복 수렴 (반복: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    return V, policy.policy_dict\n",
    "\n",
    "# 정책 반복 실행\n",
    "V_pi, policy_pi = policy_iteration(mdp)\n",
    "\n",
    "print(\"\\n정책 반복 결과:\")\n",
    "print(f\"시작 상태 V(0,0) = {V_pi[State(0,0)]:.2f}\")\n",
    "\n",
    "# 가치 반복과 정책 반복 결과 비교\n",
    "print(\"\\n=== 가치 반복 vs 정책 반복 비교 ===\")\n",
    "print(f\"가치 함수 차이: {sum(abs(V_optimal[s] - V_pi[s]) for s in mdp.states):.6f}\")\n",
    "print(\"동일한 최적 정책 도출:\", policy_optimal == policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 탐색 vs 활용 (Exploration vs Exploitation)\n",
    "\n",
    "강화학습의 핵심 딜레마: 알려진 좋은 행동을 계속할 것인가(활용), 더 나은 행동을 찾을 것인가(탐색)?\n",
    "\n",
    "### ε-greedy 정책\n",
    "- 확률 ε로 랜덤 행동 (탐색)\n",
    "- 확률 1-ε로 최선의 행동 (활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\"ε-greedy 정책\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp: GridWorldMDP, Q: Dict[Tuple[State, Action], float], \n",
    "                 epsilon: float = 0.1):\n",
    "        super().__init__(mdp)\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, state: State) -> Action:\n",
    "        \"\"\"ε-greedy 행동 선택\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # 탐색: 랜덤 행동\n",
    "            return random.choice(self.mdp.actions)\n",
    "        else:\n",
    "            # 활용: 최선의 행동\n",
    "            q_values = [(self.Q.get((state, a), 0), a) for a in self.mdp.actions]\n",
    "            return max(q_values)[1]\n",
    "\n",
    "def simulate_episode(mdp: GridWorldMDP, policy: Policy, max_steps: int = 100) -> float:\n",
    "    \"\"\"에피소드 시뮬레이션\"\"\"\n",
    "    state = mdp.start_state\n",
    "    total_reward = 0\n",
    "    discount = 1.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if mdp.is_terminal(state):\n",
    "            break\n",
    "        \n",
    "        action = policy.get_action(state)\n",
    "        next_state = mdp.get_next_state(state, action)\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        \n",
    "        total_reward += discount * reward\n",
    "        discount *= mdp.gamma\n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# 다양한 epsilon 값으로 성능 비교\n",
    "epsilons = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
    "results = []\n",
    "\n",
    "# 간단한 Q 테이블 (최적 정책 기반)\n",
    "Q_table = {}\n",
    "for state in mdp.states:\n",
    "    for action in mdp.actions:\n",
    "        next_state = mdp.get_next_state(state, action)\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        Q_table[(state, action)] = reward + mdp.gamma * V_optimal.get(next_state, 0)\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    policy = EpsilonGreedyPolicy(mdp, Q_table, epsilon)\n",
    "    rewards = [simulate_episode(mdp, policy) for _ in range(100)]\n",
    "    avg_reward = np.mean(rewards)\n",
    "    results.append(avg_reward)\n",
    "    print(f\"ε={epsilon:.1f}: 평균 보상 = {avg_reward:.2f}\")\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, results, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (탐색 확률)')\n",
    "plt.ylabel('평균 누적 보상')\n",
    "plt.title('탐색 vs 활용 트레이드오프')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실습: 복잡한 GridWorld\n",
    "\n",
    "이제 벽과 여러 함정이 있는 더 복잡한 환경에서 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexGridWorld(GridWorldMDP):\n",
    "    \"\"\"복잡한 GridWorld 환경\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(7, 7)\n",
    "        \n",
    "        # 벽 위치\n",
    "        self.walls = [\n",
    "            State(1, 1), State(1, 2), State(1, 3),\n",
    "            State(3, 3), State(3, 4), State(3, 5),\n",
    "            State(5, 1), State(5, 2)\n",
    "        ]\n",
    "        \n",
    "        # 여러 함정\n",
    "        self.traps = [State(2, 2), State(4, 4), State(6, 1)]\n",
    "        \n",
    "        # 보상 지점\n",
    "        self.rewards_dict = {\n",
    "            State(3, 1): 5.0,  # 보너스 지점\n",
    "            State(6, 6): 20.0  # 목표\n",
    "        }\n",
    "        \n",
    "        self.goal_state = State(6, 6)\n",
    "    \n",
    "    def get_next_state(self, state: State, action: Action) -> State:\n",
    "        \"\"\"벽을 고려한 상태 전이\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        next_state = super().get_next_state(state, action)\n",
    "        \n",
    "        # 벽으로는 이동 불가\n",
    "        if next_state in self.walls:\n",
    "            return state\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        \"\"\"함정이나 목표에 도달하면 종료\"\"\"\n",
    "        return state in self.traps or state == self.goal_state\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action, next_state: State) -> float:\n",
    "        \"\"\"복잡한 보상 구조\"\"\"\n",
    "        if next_state in self.traps:\n",
    "            return -20.0\n",
    "        elif next_state in self.rewards_dict:\n",
    "            return self.rewards_dict[next_state]\n",
    "        else:\n",
    "            return -0.1\n",
    "    \n",
    "    def visualize(self, values: Optional[Dict[State, float]] = None, \n",
    "                  policy: Optional[Dict[State, Action]] = None):\n",
    "        \"\"\"복잡한 환경 시각화\"\"\"\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for state, value in values.items():\n",
    "                grid[state.y, state.x] = value\n",
    "        \n",
    "        # 벽은 NaN으로 표시 (회색으로 보임)\n",
    "        for wall in self.walls:\n",
    "            grid[wall.y, wall.x] = np.nan\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        \n",
    "        # 가치 함수 히트맵\n",
    "        masked_grid = np.ma.masked_invalid(grid)\n",
    "        im = ax.imshow(masked_grid, cmap='RdYlGn', vmin=-20, vmax=20)\n",
    "        \n",
    "        # 정책 화살표\n",
    "        if policy:\n",
    "            for state, action in policy.items():\n",
    "                if not self.is_terminal(state) and state not in self.walls:\n",
    "                    dx, dy = 0, 0\n",
    "                    if action == Action.UP: dy = -0.25\n",
    "                    elif action == Action.DOWN: dy = 0.25\n",
    "                    elif action == Action.LEFT: dx = -0.25\n",
    "                    elif action == Action.RIGHT: dx = 0.25\n",
    "                    \n",
    "                    ax.arrow(state.x, state.y, dx, dy, \n",
    "                            head_width=0.08, head_length=0.08, \n",
    "                            fc='black', ec='black', alpha=0.7)\n",
    "        \n",
    "        # 특수 지점 표시\n",
    "        for trap in self.traps:\n",
    "            ax.text(trap.x, trap.y, '☠', ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        for reward_state, reward in self.rewards_dict.items():\n",
    "            if reward_state == self.goal_state:\n",
    "                ax.text(reward_state.x, reward_state.y, '★', \n",
    "                       ha='center', va='center', fontsize=25, color='gold')\n",
    "            else:\n",
    "                ax.text(reward_state.x, reward_state.y, '◆', \n",
    "                       ha='center', va='center', fontsize=20, color='blue')\n",
    "        \n",
    "        ax.text(self.start_state.x, self.start_state.y, 'S', \n",
    "               ha='center', va='center', fontsize=15, fontweight='bold')\n",
    "        \n",
    "        # 격자\n",
    "        ax.set_xticks(np.arange(self.width))\n",
    "        ax.set_yticks(np.arange(self.height))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Value')\n",
    "        plt.title('Complex GridWorld')\n",
    "        plt.show()\n",
    "\n",
    "# 복잡한 환경에서 학습\n",
    "complex_mdp = ComplexGridWorld()\n",
    "print(\"복잡한 GridWorld 생성 완료\")\n",
    "print(f\"벽: {len(complex_mdp.walls)}개, 함정: {len(complex_mdp.traps)}개\")\n",
    "\n",
    "# 가치 반복\n",
    "print(\"\\n가치 반복 실행 중...\")\n",
    "V_complex, policy_complex = value_iteration(complex_mdp, theta=0.001)\n",
    "\n",
    "print(\"\\n학습된 최적 정책:\")\n",
    "complex_mdp.visualize(values=V_complex, policy=policy_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 요약 및 핵심 개념\n",
    "\n",
    "### 이번 노트북에서 배운 내용\n",
    "\n",
    "1. **MDP (Markov Decision Process)**\n",
    "   - 강화학습 문제의 수학적 프레임워크\n",
    "   - 상태, 행동, 보상, 전이 확률로 구성\n",
    "\n",
    "2. **가치 함수**\n",
    "   - 상태 가치 V(s): 특정 상태의 장기적 가치\n",
    "   - 행동 가치 Q(s,a): 상태-행동 쌍의 가치\n",
    "   - Bellman 방정식: 재귀적 관계\n",
    "\n",
    "3. **동적 프로그래밍**\n",
    "   - 정책 평가: 주어진 정책의 가치 계산\n",
    "   - 가치 반복: 최적 가치 함수 직접 계산\n",
    "   - 정책 반복: 평가와 개선 반복\n",
    "\n",
    "4. **탐색 vs 활용**\n",
    "   - ε-greedy: 균형잡힌 탐색 전략\n",
    "   - 초기에는 탐색, 후반에는 활용\n",
    "\n",
    "### 다음 노트북 예고\n",
    "**Notebook 2: Q-learning과 SARSA**\n",
    "- Model-free 학습 (환경 모델 없이)\n",
    "- Temporal Difference 학습\n",
    "- On-policy vs Off-policy\n",
    "- 실제 에이전트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 체크포인트\n",
    "print(\"🎯 학습 완료 체크리스트:\")\n",
    "print(\"✅ MDP 구조 이해\")\n",
    "print(\"✅ 가치 함수 계산\")\n",
    "print(\"✅ Bellman 방정식 적용\")\n",
    "print(\"✅ 정책 평가 구현\")\n",
    "print(\"✅ 가치 반복 구현\")\n",
    "print(\"✅ 정책 반복 구현\")\n",
    "print(\"✅ 탐색-활용 트레이드오프 이해\")\n",
    "print(\"\\n🚀 다음 단계: Q-learning으로 모델 없이 학습하기!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}