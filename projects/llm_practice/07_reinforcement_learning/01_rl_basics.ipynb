{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: RL ê¸°ì´ˆ - MDPì™€ ê°€ì¹˜ í•¨ìˆ˜\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- ê°•í™”í•™ìŠµì˜ í•µì‹¬ ê°œë… ì´í•´\n",
    "- MDP (Markov Decision Process) êµ¬ì¡° íŒŒì•…\n",
    "- ê°€ì¹˜ í•¨ìˆ˜ì™€ Bellman ë°©ì •ì‹ í•™ìŠµ\n",
    "- GridWorldì—ì„œ ì •ì±… í‰ê°€ì™€ ê°€ì¹˜ ë°˜ë³µ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ê°•í™”í•™ìŠµì´ë€?\n",
    "\n",
    "ê°•í™”í•™ìŠµ(Reinforcement Learning)ì€ **ì—ì´ì „íŠ¸(Agent)**ê°€ **í™˜ê²½(Environment)**ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° **ë³´ìƒ(Reward)**ì„ ìµœëŒ€í™”í•˜ëŠ” **í–‰ë™(Action)**ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
    "- **Agent**: í•™ìŠµí•˜ê³  ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì£¼ì²´\n",
    "- **Environment**: ì—ì´ì „íŠ¸ê°€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì„¸ê³„\n",
    "- **State (s)**: í˜„ì¬ ìƒí™©ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´\n",
    "- **Action (a)**: ì—ì´ì „íŠ¸ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™\n",
    "- **Reward (r)**: í–‰ë™ì˜ ê²°ê³¼ë¡œ ë°›ëŠ” ì¦‰ê°ì  ë³´ìƒ\n",
    "- **Policy (Ï€)**: ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ì „ëµ\n",
    "- **Value (V)**: ì¥ê¸°ì  ê¸°ëŒ€ ë³´ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"RL ê¸°ì´ˆ í•™ìŠµ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Markov Decision Process (MDP)\n",
    "\n",
    "MDPëŠ” ê°•í™”í•™ìŠµ ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### MDP êµ¬ì„± ìš”ì†Œ\n",
    "- **S**: ìƒíƒœ ê³µê°„ (State space)\n",
    "- **A**: í–‰ë™ ê³µê°„ (Action space)  \n",
    "- **P(s'|s,a)**: ìƒíƒœ ì „ì´ í™•ë¥  (Transition probability)\n",
    "- **R(s,a,s')**: ë³´ìƒ í•¨ìˆ˜ (Reward function)\n",
    "- **Î³**: í• ì¸ ì¸ì (Discount factor) âˆˆ [0,1]\n",
    "\n",
    "### Markov ì†ì„±\n",
    "ë¯¸ë˜ëŠ” í˜„ì¬ ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ê³ , ê³¼ê±° ì´ë ¥ê³¼ëŠ” ë¬´ê´€í•©ë‹ˆë‹¤:\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"ê°€ëŠ¥í•œ í–‰ë™ë“¤\"\"\"\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def all(cls):\n",
    "        return list(cls)\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    \"\"\"ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    x: int\n",
    "    y: int\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.x, self.y))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "class GridWorldMDP:\n",
    "    \"\"\"ê°„ë‹¨í•œ GridWorld MDP í™˜ê²½\"\"\"\n",
    "    \n",
    "    def __init__(self, width: int = 5, height: int = 5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.states = [State(x, y) for x in range(width) for y in range(height)]\n",
    "        self.actions = Action.all()\n",
    "        \n",
    "        # íŠ¹ìˆ˜ ìƒíƒœ ì •ì˜\n",
    "        self.goal_state = State(width-1, height-1)  # ìš°í•˜ë‹¨ ëª©í‘œ\n",
    "        self.trap_state = State(2, 2)  # ì¤‘ì•™ í•¨ì •\n",
    "        self.start_state = State(0, 0)  # ì¢Œìƒë‹¨ ì‹œì‘\n",
    "        \n",
    "        # í• ì¸ ì¸ì\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        \"\"\"ì¢…ë£Œ ìƒíƒœ í™•ì¸\"\"\"\n",
    "        return state == self.goal_state or state == self.trap_state\n",
    "    \n",
    "    def get_next_state(self, state: State, action: Action) -> State:\n",
    "        \"\"\"ê²°ì •ì  ìƒíƒœ ì „ì´ (deterministic transition)\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        x, y = state.x, state.y\n",
    "        \n",
    "        if action == Action.UP:\n",
    "            y = max(0, y - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            y = min(self.height - 1, y + 1)\n",
    "        elif action == Action.LEFT:\n",
    "            x = max(0, x - 1)\n",
    "        elif action == Action.RIGHT:\n",
    "            x = min(self.width - 1, x + 1)\n",
    "            \n",
    "        return State(x, y)\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action, next_state: State) -> float:\n",
    "        \"\"\"ë³´ìƒ í•¨ìˆ˜\"\"\"\n",
    "        if next_state == self.goal_state:\n",
    "            return 10.0  # ëª©í‘œ ë„ë‹¬\n",
    "        elif next_state == self.trap_state:\n",
    "            return -10.0  # í•¨ì •ì— ë¹ ì§\n",
    "        else:\n",
    "            return -0.1  # ê° ìŠ¤í…ë§ˆë‹¤ ì‘ì€ íŒ¨ë„í‹°\n",
    "    \n",
    "    def visualize(self, values: Optional[Dict[State, float]] = None, \n",
    "                  policy: Optional[Dict[State, Action]] = None):\n",
    "        \"\"\"í™˜ê²½ ì‹œê°í™”\"\"\"\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for state, value in values.items():\n",
    "                grid[state.y, state.x] = value\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # ê°€ì¹˜ í•¨ìˆ˜ íˆíŠ¸ë§µ\n",
    "        im = ax.imshow(grid, cmap='RdYlGn', vmin=-10, vmax=10)\n",
    "        \n",
    "        # ì •ì±… í™”ì‚´í‘œ\n",
    "        if policy:\n",
    "            for state, action in policy.items():\n",
    "                if not self.is_terminal(state):\n",
    "                    dx, dy = 0, 0\n",
    "                    if action == Action.UP: dy = -0.3\n",
    "                    elif action == Action.DOWN: dy = 0.3\n",
    "                    elif action == Action.LEFT: dx = -0.3\n",
    "                    elif action == Action.RIGHT: dx = 0.3\n",
    "                    \n",
    "                    ax.arrow(state.x, state.y, dx, dy, \n",
    "                            head_width=0.1, head_length=0.1, \n",
    "                            fc='black', ec='black')\n",
    "        \n",
    "        # íŠ¹ìˆ˜ ìƒíƒœ í‘œì‹œ\n",
    "        ax.text(self.goal_state.x, self.goal_state.y, 'GOAL', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        ax.text(self.trap_state.x, self.trap_state.y, 'TRAP', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
    "        ax.text(self.start_state.x, self.start_state.y, 'START', \n",
    "               ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # ê²©ì í‘œì‹œ\n",
    "        ax.set_xticks(np.arange(self.width))\n",
    "        ax.set_yticks(np.arange(self.height))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Value')\n",
    "        plt.title('GridWorld MDP')\n",
    "        plt.show()\n",
    "\n",
    "# MDP í™˜ê²½ ìƒì„± ë° ì‹œê°í™”\n",
    "mdp = GridWorldMDP()\n",
    "print(f\"MDP ìƒì„±: {mdp.width}x{mdp.height} ê·¸ë¦¬ë“œ\")\n",
    "print(f\"ì‹œì‘: (0,0), ëª©í‘œ: ({mdp.width-1},{mdp.height-1}), í•¨ì •: (2,2)\")\n",
    "mdp.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê°€ì¹˜ í•¨ìˆ˜ (Value Functions)\n",
    "\n",
    "### ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s)\n",
    "ìƒíƒœ sì—ì„œ ì‹œì‘í•˜ì—¬ ì •ì±… Ï€ë¥¼ ë”°ë¥¼ ë•Œì˜ ê¸°ëŒ€ ëˆ„ì  ë³´ìƒ:\n",
    "$$V^\\pi(s) = E_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_{t+1} | s_0 = s]$$\n",
    "\n",
    "### í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ Q(s,a)\n",
    "ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì·¨í•œ í›„ ì •ì±… Ï€ë¥¼ ë”°ë¥¼ ë•Œì˜ ê¸°ëŒ€ ëˆ„ì  ë³´ìƒ:\n",
    "$$Q^\\pi(s,a) = E_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_{t+1} | s_0 = s, a_0 = a]$$\n",
    "\n",
    "### Bellman ë°©ì •ì‹\n",
    "ê°€ì¹˜ í•¨ìˆ˜ëŠ” ì¬ê·€ì  ê´€ê³„ë¥¼ ë§Œì¡±í•©ë‹ˆë‹¤:\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"ì •ì±… í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp: GridWorldMDP):\n",
    "        self.mdp = mdp\n",
    "        self.policy_dict = {}\n",
    "    \n",
    "    def get_action(self, state: State) -> Action:\n",
    "        \"\"\"ìƒíƒœì— ëŒ€í•œ í–‰ë™ ë°˜í™˜\"\"\"\n",
    "        return self.policy_dict.get(state, random.choice(self.mdp.actions))\n",
    "    \n",
    "    def get_action_prob(self, state: State, action: Action) -> float:\n",
    "        \"\"\"Ï€(a|s) - ê²°ì •ì  ì •ì±…\"\"\"\n",
    "        return 1.0 if self.get_action(state) == action else 0.0\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    \"\"\"ëœë¤ ì •ì±…\"\"\"\n",
    "    \n",
    "    def get_action_prob(self, state: State, action: Action) -> float:\n",
    "        \"\"\"ëª¨ë“  í–‰ë™ì— ë™ì¼í•œ í™•ë¥ \"\"\"\n",
    "        return 1.0 / len(self.mdp.actions)\n",
    "\n",
    "def policy_evaluation(mdp: GridWorldMDP, policy: Policy, \n",
    "                     theta: float = 0.01, max_iters: int = 1000) -> Dict[State, float]:\n",
    "    \"\"\"\n",
    "    ì •ì±… í‰ê°€: ì£¼ì–´ì§„ ì •ì±…ì˜ ê°€ì¹˜ í•¨ìˆ˜ ê³„ì‚°\n",
    "    Iterative Policy Evaluation ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    # ê°€ì¹˜ í•¨ìˆ˜ ì´ˆê¸°í™”\n",
    "    V = {state: 0.0 for state in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0\n",
    "        \n",
    "        # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ì—…ë°ì´íŠ¸\n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            v = V[state]\n",
    "            new_v = 0\n",
    "            \n",
    "            # Bellman Expectation Equation\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                \n",
    "                # Ï€(a|s) * [R + Î³V(s')]\n",
    "                new_v += policy.get_action_prob(state, action) * \\\n",
    "                        (reward + mdp.gamma * V[next_state])\n",
    "            \n",
    "            V[state] = new_v\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"ì •ì±… í‰ê°€ ìˆ˜ë ´ (ë°˜ë³µ: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# ëœë¤ ì •ì±… í‰ê°€\n",
    "random_policy = RandomPolicy(mdp)\n",
    "V_random = policy_evaluation(mdp, random_policy)\n",
    "\n",
    "print(\"\\nëœë¤ ì •ì±…ì˜ ê°€ì¹˜ í•¨ìˆ˜:\")\n",
    "print(f\"ì‹œì‘ ìƒíƒœ V(0,0) = {V_random[State(0,0)]:.2f}\")\n",
    "print(f\"ëª©í‘œ ê·¼ì²˜ V(3,3) = {V_random[State(3,3)]:.2f}\")\n",
    "print(f\"í•¨ì • ê·¼ì²˜ V(2,1) = {V_random[State(2,1)]:.2f}\")\n",
    "\n",
    "mdp.visualize(values=V_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ê°€ì¹˜ ë°˜ë³µ (Value Iteration)\n",
    "\n",
    "ê°€ì¹˜ ë°˜ë³µì€ **ìµœì  ê°€ì¹˜ í•¨ìˆ˜** $V^*$ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ë™ì  í”„ë¡œê·¸ë˜ë° ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "### Bellman Optimality Equation\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "### ì•Œê³ ë¦¬ì¦˜\n",
    "1. ëª¨ë“  ìƒíƒœì˜ ê°€ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "2. ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ:\n",
    "   - ê° ìƒíƒœì— ëŒ€í•´ ëª¨ë“  í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ê³„ì‚°\n",
    "   - ìµœëŒ€ ê°€ì¹˜ë¥¼ ì„ íƒí•˜ì—¬ ì—…ë°ì´íŠ¸\n",
    "3. ìµœì  ì •ì±… ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: GridWorldMDP, theta: float = 0.01, \n",
    "                   max_iters: int = 1000) -> Tuple[Dict[State, float], Dict[State, Action]]:\n",
    "    \"\"\"\n",
    "    ê°€ì¹˜ ë°˜ë³µ: ìµœì  ê°€ì¹˜ í•¨ìˆ˜ì™€ ìµœì  ì •ì±… ê³„ì‚°\n",
    "    \"\"\"\n",
    "    # ê°€ì¹˜ í•¨ìˆ˜ ì´ˆê¸°í™”\n",
    "    V = {state: 0.0 for state in mdp.states}\n",
    "    \n",
    "    # ê°€ì¹˜ ë°˜ë³µ\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            v = V[state]\n",
    "            \n",
    "            # Bellman Optimality Equation\n",
    "            action_values = []\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                action_value = reward + mdp.gamma * V[next_state]\n",
    "                action_values.append(action_value)\n",
    "            \n",
    "            V[state] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"ê°€ì¹˜ ë°˜ë³µ ìˆ˜ë ´ (ë°˜ë³µ: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    # ìµœì  ì •ì±… ì¶”ì¶œ\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        if mdp.is_terminal(state):\n",
    "            continue\n",
    "        \n",
    "        action_values = []\n",
    "        for action in mdp.actions:\n",
    "            next_state = mdp.get_next_state(state, action)\n",
    "            reward = mdp.get_reward(state, action, next_state)\n",
    "            action_value = reward + mdp.gamma * V[next_state]\n",
    "            action_values.append((action_value, action))\n",
    "        \n",
    "        policy[state] = max(action_values)[1]\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# ê°€ì¹˜ ë°˜ë³µ ì‹¤í–‰\n",
    "V_optimal, policy_optimal = value_iteration(mdp)\n",
    "\n",
    "print(\"\\nìµœì  ê°€ì¹˜ í•¨ìˆ˜:\")\n",
    "print(f\"ì‹œì‘ ìƒíƒœ V*(0,0) = {V_optimal[State(0,0)]:.2f}\")\n",
    "print(f\"ëª©í‘œ ê·¼ì²˜ V*(3,3) = {V_optimal[State(3,3)]:.2f}\")\n",
    "print(f\"í•¨ì • ê·¼ì²˜ V*(2,1) = {V_optimal[State(2,1)]:.2f}\")\n",
    "\n",
    "print(\"\\nìµœì  ì •ì±… (í™”ì‚´í‘œë¡œ í‘œì‹œ):\")\n",
    "mdp.visualize(values=V_optimal, policy=policy_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì •ì±… ë°˜ë³µ (Policy Iteration)\n",
    "\n",
    "ì •ì±… ë°˜ë³µì€ ì •ì±… í‰ê°€ì™€ ì •ì±… ê°œì„ ì„ ë²ˆê°ˆì•„ê°€ë©° ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì•Œê³ ë¦¬ì¦˜\n",
    "1. ì„ì˜ì˜ ì •ì±…ìœ¼ë¡œ ì‹œì‘\n",
    "2. ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ:\n",
    "   - **ì •ì±… í‰ê°€**: í˜„ì¬ ì •ì±…ì˜ ê°€ì¹˜ í•¨ìˆ˜ ê³„ì‚°\n",
    "   - **ì •ì±… ê°œì„ **: ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ë‚˜ì€ ì •ì±… ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: GridWorldMDP, max_iters: int = 100) -> Tuple[Dict[State, float], Dict[State, Action]]:\n",
    "    \"\"\"\n",
    "    ì •ì±… ë°˜ë³µ: ì •ì±… í‰ê°€ì™€ ê°œì„ ì„ ë°˜ë³µí•˜ì—¬ ìµœì  ì •ì±… ì°¾ê¸°\n",
    "    \"\"\"\n",
    "    # ì„ì˜ì˜ ì •ì±…ìœ¼ë¡œ ì´ˆê¸°í™” (ëª¨ë‘ ìœ„ë¡œ ê°€ê¸°)\n",
    "    policy = Policy(mdp)\n",
    "    for state in mdp.states:\n",
    "        policy.policy_dict[state] = Action.UP\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # 1. ì •ì±… í‰ê°€\n",
    "        V = policy_evaluation(mdp, policy, theta=0.01)\n",
    "        \n",
    "        # 2. ì •ì±… ê°œì„ \n",
    "        policy_stable = True\n",
    "        \n",
    "        for state in mdp.states:\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            old_action = policy.get_action(state)\n",
    "            \n",
    "            # ê° í–‰ë™ì˜ ê°€ì¹˜ ê³„ì‚°\n",
    "            action_values = []\n",
    "            for action in mdp.actions:\n",
    "                next_state = mdp.get_next_state(state, action)\n",
    "                reward = mdp.get_reward(state, action, next_state)\n",
    "                action_value = reward + mdp.gamma * V[next_state]\n",
    "                action_values.append((action_value, action))\n",
    "            \n",
    "            # ìµœì„ ì˜ í–‰ë™ ì„ íƒ (greedy)\n",
    "            best_action = max(action_values)[1]\n",
    "            policy.policy_dict[state] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"ì •ì±… ë°˜ë³µ ìˆ˜ë ´ (ë°˜ë³µ: {iteration})\")\n",
    "            break\n",
    "    \n",
    "    return V, policy.policy_dict\n",
    "\n",
    "# ì •ì±… ë°˜ë³µ ì‹¤í–‰\n",
    "V_pi, policy_pi = policy_iteration(mdp)\n",
    "\n",
    "print(\"\\nì •ì±… ë°˜ë³µ ê²°ê³¼:\")\n",
    "print(f\"ì‹œì‘ ìƒíƒœ V(0,0) = {V_pi[State(0,0)]:.2f}\")\n",
    "\n",
    "# ê°€ì¹˜ ë°˜ë³µê³¼ ì •ì±… ë°˜ë³µ ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n=== ê°€ì¹˜ ë°˜ë³µ vs ì •ì±… ë°˜ë³µ ë¹„êµ ===\")\n",
    "print(f\"ê°€ì¹˜ í•¨ìˆ˜ ì°¨ì´: {sum(abs(V_optimal[s] - V_pi[s]) for s in mdp.states):.6f}\")\n",
    "print(\"ë™ì¼í•œ ìµœì  ì •ì±… ë„ì¶œ:\", policy_optimal == policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. íƒìƒ‰ vs í™œìš© (Exploration vs Exploitation)\n",
    "\n",
    "ê°•í™”í•™ìŠµì˜ í•µì‹¬ ë”œë ˆë§ˆ: ì•Œë ¤ì§„ ì¢‹ì€ í–‰ë™ì„ ê³„ì†í•  ê²ƒì¸ê°€(í™œìš©), ë” ë‚˜ì€ í–‰ë™ì„ ì°¾ì„ ê²ƒì¸ê°€(íƒìƒ‰)?\n",
    "\n",
    "### Îµ-greedy ì •ì±…\n",
    "- í™•ë¥  Îµë¡œ ëœë¤ í–‰ë™ (íƒìƒ‰)\n",
    "- í™•ë¥  1-Îµë¡œ ìµœì„ ì˜ í–‰ë™ (í™œìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(Policy):\n",
    "    \"\"\"Îµ-greedy ì •ì±…\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp: GridWorldMDP, Q: Dict[Tuple[State, Action], float], \n",
    "                 epsilon: float = 0.1):\n",
    "        super().__init__(mdp)\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, state: State) -> Action:\n",
    "        \"\"\"Îµ-greedy í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # íƒìƒ‰: ëœë¤ í–‰ë™\n",
    "            return random.choice(self.mdp.actions)\n",
    "        else:\n",
    "            # í™œìš©: ìµœì„ ì˜ í–‰ë™\n",
    "            q_values = [(self.Q.get((state, a), 0), a) for a in self.mdp.actions]\n",
    "            return max(q_values)[1]\n",
    "\n",
    "def simulate_episode(mdp: GridWorldMDP, policy: Policy, max_steps: int = 100) -> float:\n",
    "    \"\"\"ì—í”¼ì†Œë“œ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    state = mdp.start_state\n",
    "    total_reward = 0\n",
    "    discount = 1.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if mdp.is_terminal(state):\n",
    "            break\n",
    "        \n",
    "        action = policy.get_action(state)\n",
    "        next_state = mdp.get_next_state(state, action)\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        \n",
    "        total_reward += discount * reward\n",
    "        discount *= mdp.gamma\n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# ë‹¤ì–‘í•œ epsilon ê°’ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
    "epsilons = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
    "results = []\n",
    "\n",
    "# ê°„ë‹¨í•œ Q í…Œì´ë¸” (ìµœì  ì •ì±… ê¸°ë°˜)\n",
    "Q_table = {}\n",
    "for state in mdp.states:\n",
    "    for action in mdp.actions:\n",
    "        next_state = mdp.get_next_state(state, action)\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        Q_table[(state, action)] = reward + mdp.gamma * V_optimal.get(next_state, 0)\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    policy = EpsilonGreedyPolicy(mdp, Q_table, epsilon)\n",
    "    rewards = [simulate_episode(mdp, policy) for _ in range(100)]\n",
    "    avg_reward = np.mean(rewards)\n",
    "    results.append(avg_reward)\n",
    "    print(f\"Îµ={epsilon:.1f}: í‰ê·  ë³´ìƒ = {avg_reward:.2f}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, results, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (íƒìƒ‰ í™•ë¥ )')\n",
    "plt.ylabel('í‰ê·  ëˆ„ì  ë³´ìƒ')\n",
    "plt.title('íƒìƒ‰ vs í™œìš© íŠ¸ë ˆì´ë“œì˜¤í”„')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ìŠµ: ë³µì¡í•œ GridWorld\n",
    "\n",
    "ì´ì œ ë²½ê³¼ ì—¬ëŸ¬ í•¨ì •ì´ ìˆëŠ” ë” ë³µì¡í•œ í™˜ê²½ì—ì„œ í•™ìŠµí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexGridWorld(GridWorldMDP):\n",
    "    \"\"\"ë³µì¡í•œ GridWorld í™˜ê²½\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(7, 7)\n",
    "        \n",
    "        # ë²½ ìœ„ì¹˜\n",
    "        self.walls = [\n",
    "            State(1, 1), State(1, 2), State(1, 3),\n",
    "            State(3, 3), State(3, 4), State(3, 5),\n",
    "            State(5, 1), State(5, 2)\n",
    "        ]\n",
    "        \n",
    "        # ì—¬ëŸ¬ í•¨ì •\n",
    "        self.traps = [State(2, 2), State(4, 4), State(6, 1)]\n",
    "        \n",
    "        # ë³´ìƒ ì§€ì \n",
    "        self.rewards_dict = {\n",
    "            State(3, 1): 5.0,  # ë³´ë„ˆìŠ¤ ì§€ì \n",
    "            State(6, 6): 20.0  # ëª©í‘œ\n",
    "        }\n",
    "        \n",
    "        self.goal_state = State(6, 6)\n",
    "    \n",
    "    def get_next_state(self, state: State, action: Action) -> State:\n",
    "        \"\"\"ë²½ì„ ê³ ë ¤í•œ ìƒíƒœ ì „ì´\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        next_state = super().get_next_state(state, action)\n",
    "        \n",
    "        # ë²½ìœ¼ë¡œëŠ” ì´ë™ ë¶ˆê°€\n",
    "        if next_state in self.walls:\n",
    "            return state\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        \"\"\"í•¨ì •ì´ë‚˜ ëª©í‘œì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ\"\"\"\n",
    "        return state in self.traps or state == self.goal_state\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action, next_state: State) -> float:\n",
    "        \"\"\"ë³µì¡í•œ ë³´ìƒ êµ¬ì¡°\"\"\"\n",
    "        if next_state in self.traps:\n",
    "            return -20.0\n",
    "        elif next_state in self.rewards_dict:\n",
    "            return self.rewards_dict[next_state]\n",
    "        else:\n",
    "            return -0.1\n",
    "    \n",
    "    def visualize(self, values: Optional[Dict[State, float]] = None, \n",
    "                  policy: Optional[Dict[State, Action]] = None):\n",
    "        \"\"\"ë³µì¡í•œ í™˜ê²½ ì‹œê°í™”\"\"\"\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for state, value in values.items():\n",
    "                grid[state.y, state.x] = value\n",
    "        \n",
    "        # ë²½ì€ NaNìœ¼ë¡œ í‘œì‹œ (íšŒìƒ‰ìœ¼ë¡œ ë³´ì„)\n",
    "        for wall in self.walls:\n",
    "            grid[wall.y, wall.x] = np.nan\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        \n",
    "        # ê°€ì¹˜ í•¨ìˆ˜ íˆíŠ¸ë§µ\n",
    "        masked_grid = np.ma.masked_invalid(grid)\n",
    "        im = ax.imshow(masked_grid, cmap='RdYlGn', vmin=-20, vmax=20)\n",
    "        \n",
    "        # ì •ì±… í™”ì‚´í‘œ\n",
    "        if policy:\n",
    "            for state, action in policy.items():\n",
    "                if not self.is_terminal(state) and state not in self.walls:\n",
    "                    dx, dy = 0, 0\n",
    "                    if action == Action.UP: dy = -0.25\n",
    "                    elif action == Action.DOWN: dy = 0.25\n",
    "                    elif action == Action.LEFT: dx = -0.25\n",
    "                    elif action == Action.RIGHT: dx = 0.25\n",
    "                    \n",
    "                    ax.arrow(state.x, state.y, dx, dy, \n",
    "                            head_width=0.08, head_length=0.08, \n",
    "                            fc='black', ec='black', alpha=0.7)\n",
    "        \n",
    "        # íŠ¹ìˆ˜ ì§€ì  í‘œì‹œ\n",
    "        for trap in self.traps:\n",
    "            ax.text(trap.x, trap.y, 'â˜ ', ha='center', va='center', fontsize=20)\n",
    "        \n",
    "        for reward_state, reward in self.rewards_dict.items():\n",
    "            if reward_state == self.goal_state:\n",
    "                ax.text(reward_state.x, reward_state.y, 'â˜…', \n",
    "                       ha='center', va='center', fontsize=25, color='gold')\n",
    "            else:\n",
    "                ax.text(reward_state.x, reward_state.y, 'â—†', \n",
    "                       ha='center', va='center', fontsize=20, color='blue')\n",
    "        \n",
    "        ax.text(self.start_state.x, self.start_state.y, 'S', \n",
    "               ha='center', va='center', fontsize=15, fontweight='bold')\n",
    "        \n",
    "        # ê²©ì\n",
    "        ax.set_xticks(np.arange(self.width))\n",
    "        ax.set_yticks(np.arange(self.height))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Value')\n",
    "        plt.title('Complex GridWorld')\n",
    "        plt.show()\n",
    "\n",
    "# ë³µì¡í•œ í™˜ê²½ì—ì„œ í•™ìŠµ\n",
    "complex_mdp = ComplexGridWorld()\n",
    "print(\"ë³µì¡í•œ GridWorld ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ë²½: {len(complex_mdp.walls)}ê°œ, í•¨ì •: {len(complex_mdp.traps)}ê°œ\")\n",
    "\n",
    "# ê°€ì¹˜ ë°˜ë³µ\n",
    "print(\"\\nê°€ì¹˜ ë°˜ë³µ ì‹¤í–‰ ì¤‘...\")\n",
    "V_complex, policy_complex = value_iteration(complex_mdp, theta=0.001)\n",
    "\n",
    "print(\"\\ní•™ìŠµëœ ìµœì  ì •ì±…:\")\n",
    "complex_mdp.visualize(values=V_complex, policy=policy_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ìš”ì•½ ë° í•µì‹¬ ê°œë…\n",
    "\n",
    "### ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "1. **MDP (Markov Decision Process)**\n",
    "   - ê°•í™”í•™ìŠµ ë¬¸ì œì˜ ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬\n",
    "   - ìƒíƒœ, í–‰ë™, ë³´ìƒ, ì „ì´ í™•ë¥ ë¡œ êµ¬ì„±\n",
    "\n",
    "2. **ê°€ì¹˜ í•¨ìˆ˜**\n",
    "   - ìƒíƒœ ê°€ì¹˜ V(s): íŠ¹ì • ìƒíƒœì˜ ì¥ê¸°ì  ê°€ì¹˜\n",
    "   - í–‰ë™ ê°€ì¹˜ Q(s,a): ìƒíƒœ-í–‰ë™ ìŒì˜ ê°€ì¹˜\n",
    "   - Bellman ë°©ì •ì‹: ì¬ê·€ì  ê´€ê³„\n",
    "\n",
    "3. **ë™ì  í”„ë¡œê·¸ë˜ë°**\n",
    "   - ì •ì±… í‰ê°€: ì£¼ì–´ì§„ ì •ì±…ì˜ ê°€ì¹˜ ê³„ì‚°\n",
    "   - ê°€ì¹˜ ë°˜ë³µ: ìµœì  ê°€ì¹˜ í•¨ìˆ˜ ì§ì ‘ ê³„ì‚°\n",
    "   - ì •ì±… ë°˜ë³µ: í‰ê°€ì™€ ê°œì„  ë°˜ë³µ\n",
    "\n",
    "4. **íƒìƒ‰ vs í™œìš©**\n",
    "   - Îµ-greedy: ê· í˜•ì¡íŒ íƒìƒ‰ ì „ëµ\n",
    "   - ì´ˆê¸°ì—ëŠ” íƒìƒ‰, í›„ë°˜ì—ëŠ” í™œìš©\n",
    "\n",
    "### ë‹¤ìŒ ë…¸íŠ¸ë¶ ì˜ˆê³ \n",
    "**Notebook 2: Q-learningê³¼ SARSA**\n",
    "- Model-free í•™ìŠµ (í™˜ê²½ ëª¨ë¸ ì—†ì´)\n",
    "- Temporal Difference í•™ìŠµ\n",
    "- On-policy vs Off-policy\n",
    "- ì‹¤ì œ ì—ì´ì „íŠ¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\n",
    "print(\"ğŸ¯ í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(\"âœ… MDP êµ¬ì¡° ì´í•´\")\n",
    "print(\"âœ… ê°€ì¹˜ í•¨ìˆ˜ ê³„ì‚°\")\n",
    "print(\"âœ… Bellman ë°©ì •ì‹ ì ìš©\")\n",
    "print(\"âœ… ì •ì±… í‰ê°€ êµ¬í˜„\")\n",
    "print(\"âœ… ê°€ì¹˜ ë°˜ë³µ êµ¬í˜„\")\n",
    "print(\"âœ… ì •ì±… ë°˜ë³µ êµ¬í˜„\")\n",
    "print(\"âœ… íƒìƒ‰-í™œìš© íŠ¸ë ˆì´ë“œì˜¤í”„ ì´í•´\")\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: Q-learningìœ¼ë¡œ ëª¨ë¸ ì—†ì´ í•™ìŠµí•˜ê¸°!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}