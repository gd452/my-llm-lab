{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: ì¶”ë¡  ê¸°ë°˜ RL - Planningê³¼ ReAct\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- Model-based RLê³¼ Planning ì´í•´\n",
    "- MCTS (Monte Carlo Tree Search) êµ¬í˜„\n",
    "- ReAct (Reasoning + Acting) íŒ¨ëŸ¬ë‹¤ì„ í•™ìŠµ\n",
    "- ì¶”ë¡ ì„ í–‰ë™ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²• ì‹¤ìŠµ\n",
    "- \"ì•Œê³ ë¦¬ì¦˜ì´ ì™•\"ì—ì„œ \"ì¶”ë¡ ì´ ì™•\"ìœ¼ë¡œì˜ ì „í™˜ ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: ì•Œê³ ë¦¬ì¦˜ â†’ ì¶”ë¡ \n",
    "\n",
    "### ì „ë°˜ì „: \"ì•Œê³ ë¦¬ì¦˜ì´ ì™•\"\n",
    "- Q-learning, DQN, PPO ë“± ì•Œê³ ë¦¬ì¦˜ ìµœì í™”\n",
    "- ë” ë‚˜ì€ ì—…ë°ì´íŠ¸ ê·œì¹™, ë” íš¨ìœ¨ì ì¸ í•™ìŠµ\n",
    "- ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ê²½ìŸ\n",
    "\n",
    "### í›„ë°˜ì „: \"ì¶”ë¡ ì´ ì™•\"\n",
    "- ì‚¬ì „ì§€ì‹(priors)ê³¼ ê³„íš(planning) í™œìš©\n",
    "- Test-time compute: ì¶”ë¡  ì‹œì ì— ë” ë§ì€ ê³„ì‚°\n",
    "- **THINKë¥¼ í–‰ë™ìœ¼ë¡œ**: ë‚´ë¶€ ì¶”ë¡  ê³¼ì • ìì²´ê°€ í–‰ë™ì´ ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"ì¶”ë¡  ê¸°ë°˜ RL í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"Key Insight: ì¶”ë¡ (Reasoning)ì„ í–‰ë™(Action)ìœ¼ë¡œ í†µí•©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½: ì—´ì‡ -ë¬¸-ëª©í‘œ ë¯¸ë¡œ\n",
    "\n",
    "ê¸°ì¡´ RL ë…¸íŠ¸ë¶ì˜ í™˜ê²½ì„ í™•ì¥í•˜ì—¬, ì¶”ë¡ ì´ í•„ìš”í•œ ë³µì¡í•œ ë¯¸ë¡œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    THINK = 4  # ğŸ¯ í•µì‹¬: THINKë¥¼ í–‰ë™ìœ¼ë¡œ!\n",
    "\n",
    "class ComplexMaze:\n",
    "    \"\"\"ë³µì¡í•œ ë¯¸ë¡œ í™˜ê²½ (ì—´ì‡ -ë¬¸-ëª©í‘œ)\"\"\"\n",
    "    \n",
    "    def __init__(self, maze_type='complex'):\n",
    "        self.maze_type = maze_type\n",
    "        self._create_maze()\n",
    "        self.reset()\n",
    "    \n",
    "    def _create_maze(self):\n",
    "        if self.maze_type == 'simple':\n",
    "            self.grid = [\n",
    "                \"S....\",\n",
    "                \".###.\",\n",
    "                \".....\",\n",
    "                \".###.\",\n",
    "                \"....G\"\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (4, 4)\n",
    "            self.keys = []\n",
    "            self.doors = []\n",
    "            \n",
    "        elif self.maze_type == 'complex':\n",
    "            self.grid = [\n",
    "                \"#########\",\n",
    "                \"#S.....##\",\n",
    "                \"#.##.#.##\",\n",
    "                \"#.#D.#K##\",\n",
    "                \"#.#..#.##\",\n",
    "                \"#.####.##\",\n",
    "                \"#......G#\",\n",
    "                \"#########\"\n",
    "            ]\n",
    "            self.start = (1, 1)\n",
    "            self.goal = (6, 7)\n",
    "            self.keys = [(3, 6)]  # K ìœ„ì¹˜\n",
    "            self.doors = [(3, 3)]  # D ìœ„ì¹˜\n",
    "            \n",
    "        self.height = len(self.grid)\n",
    "        self.width = len(self.grid[0])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start\n",
    "        self.has_keys = set()\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.height * self.width * 10\n",
    "        self.think_count = 0  # THINK í–‰ë™ íšŸìˆ˜\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # ìƒíƒœ = (ìœ„ì¹˜, ì—´ì‡  ë³´ìœ  ìƒíƒœ)\n",
    "        key_tuple = tuple(sorted(self.has_keys))\n",
    "        return (self.pos[0], self.pos[1], key_tuple)\n",
    "    \n",
    "    def step(self, action: Action):\n",
    "        self.steps += 1\n",
    "        reward = -0.01  # ê¸°ë³¸ ìŠ¤í… íŒ¨ë„í‹°\n",
    "        \n",
    "        if action == Action.THINK:\n",
    "            # THINK í–‰ë™: ì™¸ë¶€ ìƒíƒœëŠ” ë³€í•˜ì§€ ì•ŠìŒ\n",
    "            self.think_count += 1\n",
    "            reward = -0.005  # ì‘ì€ íŒ¨ë„í‹°\n",
    "            info = {'think': True, 'think_count': self.think_count}\n",
    "        else:\n",
    "            # ë¬¼ë¦¬ì  ì´ë™\n",
    "            new_pos = self._get_new_pos(self.pos, action)\n",
    "            \n",
    "            if self._is_valid_move(new_pos):\n",
    "                self.pos = new_pos\n",
    "                \n",
    "                # ì—´ì‡  íšë“\n",
    "                if new_pos in self.keys and new_pos not in self.has_keys:\n",
    "                    self.has_keys.add(new_pos)\n",
    "                    reward += 1.0\n",
    "                    print(f\"ğŸ”‘ ì—´ì‡  íšë“! (ìœ„ì¹˜: {new_pos})\")\n",
    "            \n",
    "            info = {'think': False}\n",
    "        \n",
    "        # ëª©í‘œ ë„ë‹¬ í™•ì¸\n",
    "        if self.pos == self.goal:\n",
    "            reward += 10.0\n",
    "            done = True\n",
    "            print(f\"ğŸ‰ ëª©í‘œ ë„ë‹¬! (ì´ ìŠ¤í…: {self.steps}, THINK íšŸìˆ˜: {self.think_count})\")\n",
    "        else:\n",
    "            done = self.steps >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _get_new_pos(self, pos, action):\n",
    "        y, x = pos\n",
    "        if action == Action.UP:\n",
    "            return (y-1, x)\n",
    "        elif action == Action.DOWN:\n",
    "            return (y+1, x)\n",
    "        elif action == Action.LEFT:\n",
    "            return (y, x-1)\n",
    "        elif action == Action.RIGHT:\n",
    "            return (y, x+1)\n",
    "        return pos\n",
    "    \n",
    "    def _is_valid_move(self, pos):\n",
    "        y, x = pos\n",
    "        \n",
    "        # ê²½ê³„ í™•ì¸\n",
    "        if not (0 <= y < self.height and 0 <= x < self.width):\n",
    "            return False\n",
    "        \n",
    "        # ë²½ í™•ì¸\n",
    "        if self.grid[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # ë¬¸ í™•ì¸ (ì—´ì‡  í•„ìš”)\n",
    "        if pos in self.doors:\n",
    "            # í•´ë‹¹ ë¬¸ì— ëŒ€í•œ ì—´ì‡ ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "            required_key = self.keys[self.doors.index(pos)] if self.doors else None\n",
    "            if required_key and required_key not in self.has_keys:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        display = []\n",
    "        for y in range(self.height):\n",
    "            row = []\n",
    "            for x in range(self.width):\n",
    "                if (y, x) == self.pos:\n",
    "                    row.append('A')\n",
    "                elif (y, x) in self.keys and (y, x) not in self.has_keys:\n",
    "                    row.append('K')\n",
    "                elif (y, x) in self.doors:\n",
    "                    if self.keys and self.keys[self.doors.index((y,x))] in self.has_keys:\n",
    "                        row.append('.')\n",
    "                    else:\n",
    "                        row.append('D')\n",
    "                elif (y, x) == self.goal:\n",
    "                    row.append('G')\n",
    "                else:\n",
    "                    row.append(self.grid[y][x])\n",
    "            display.append(' '.join(row))\n",
    "        \n",
    "        print('\\n'.join(display))\n",
    "        if self.has_keys:\n",
    "            print(f\"ğŸ”‘ ë³´ìœ  ì—´ì‡ : {self.has_keys}\")\n",
    "        print(f\"Steps: {self.steps}, Thinks: {self.think_count}\")\n",
    "\n",
    "# í™˜ê²½ í…ŒìŠ¤íŠ¸\n",
    "env = ComplexMaze('complex')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì „ë°˜ì „: ìˆœìˆ˜ Q-Learning\n",
    "\n",
    "ë¨¼ì € ì „í†µì ì¸ Q-learningìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"ì „í†µì ì¸ Q-Learning ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions  # THINK ì œì™¸\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return Action(random.randint(0, self.n_actions-1))\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            return Action(np.argmax(q_values))\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        td_error = target - self.Q[state][action.value]\n",
    "        self.Q[state][action.value] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "\n",
    "# Q-Learning í•™ìŠµ\n",
    "def train_qlearning(n_episodes=200):\n",
    "    env = ComplexMaze('complex')\n",
    "    agent = QLearningAgent(n_actions=4)  # THINK ì—†ìŒ\n",
    "    \n",
    "    successes = 0\n",
    "    \n",
    "    print(\"ì „ë°˜ì „: ìˆœìˆ˜ Q-Learning (ì•Œê³ ë¦¬ì¦˜ì´ ì™•)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward = agent.train_episode(env)\n",
    "        \n",
    "        if env.pos == env.goal:\n",
    "            successes += 1\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            success_rate = successes / (episode + 1) * 100\n",
    "            avg_reward = np.mean(agent.episode_rewards[-50:])\n",
    "            print(f\"Episode {episode+1}: ì„±ê³µë¥ ={success_rate:.1f}%, í‰ê· ë³´ìƒ={avg_reward:.2f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "q_agent = train_qlearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í›„ë°˜ì „: ReAct ì—ì´ì „íŠ¸\n",
    "\n",
    "ì´ì œ **THINKë¥¼ í–‰ë™ìœ¼ë¡œ** ë„ì…í•œ ReAct ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "- THINK í–‰ë™: ë‚´ë¶€ ê³„íš ìˆ˜ë¦½\n",
    "- BFSë¡œ ìµœì  ê²½ë¡œ ê³„ì‚°\n",
    "- ê³„íšì— ë”°ë¼ í–‰ë™ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    ReAct (Reasoning + Acting) ì—ì´ì „íŠ¸\n",
    "    THINKë¥¼ ëª…ì‹œì  í–‰ë™ìœ¼ë¡œ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_memory=True):\n",
    "        self.plan = []  # í˜„ì¬ ê³„íš\n",
    "        self.use_memory = use_memory\n",
    "        self.memory = {}  # í•™ìŠµëœ ê³„íš ì €ì¥\n",
    "        self.think_history = []  # ì¶”ë¡  ê¸°ë¡\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def think(self, env: ComplexMaze) -> List[Action]:\n",
    "        \"\"\"\n",
    "        THINK í–‰ë™: BFSë¡œ ìµœì  ê²½ë¡œ ê³„íš\n",
    "        \"\"\"\n",
    "        # í˜„ì¬ ìƒíƒœ\n",
    "        start_pos = env.pos\n",
    "        has_keys = env.has_keys.copy()\n",
    "        \n",
    "        # ì¶”ë¡  ê¸°ë¡\n",
    "        thought = {\n",
    "            'position': start_pos,\n",
    "            'keys': has_keys,\n",
    "            'goal': env.goal\n",
    "        }\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ì—ì„œ ê³„íš ì°¾ê¸°\n",
    "        if self.use_memory:\n",
    "            memory_key = (start_pos, tuple(sorted(has_keys)))\n",
    "            if memory_key in self.memory:\n",
    "                thought['from_memory'] = True\n",
    "                self.think_history.append(thought)\n",
    "                return self.memory[memory_key].copy()\n",
    "        \n",
    "        # BFSë¡œ ê²½ë¡œ íƒìƒ‰\n",
    "        plan = self._bfs_planning(env, start_pos, has_keys)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        if self.use_memory and plan:\n",
    "            memory_key = (start_pos, tuple(sorted(has_keys)))\n",
    "            self.memory[memory_key] = plan.copy()\n",
    "        \n",
    "        thought['plan_length'] = len(plan) if plan else 0\n",
    "        thought['from_memory'] = False\n",
    "        self.think_history.append(thought)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _bfs_planning(self, env: ComplexMaze, start_pos, has_keys):\n",
    "        \"\"\"\n",
    "        BFSë¡œ ìµœì  ê²½ë¡œ ê³„íš\n",
    "        1. ì—´ì‡ ê°€ í•„ìš”í•˜ë©´ ë¨¼ì € ì—´ì‡ ë¡œ\n",
    "        2. ê·¸ ë‹¤ìŒ ëª©í‘œë¡œ\n",
    "        \"\"\"\n",
    "        plan = []\n",
    "        current_pos = start_pos\n",
    "        current_keys = has_keys.copy()\n",
    "        \n",
    "        # Step 1: í•„ìš”í•œ ì—´ì‡  íšë“\n",
    "        for key_pos in env.keys:\n",
    "            if key_pos not in current_keys:\n",
    "                path = self._bfs_path(env, current_pos, key_pos, current_keys)\n",
    "                if path:\n",
    "                    plan.extend(path)\n",
    "                    current_pos = key_pos\n",
    "                    current_keys.add(key_pos)\n",
    "        \n",
    "        # Step 2: ëª©í‘œë¡œ ì´ë™\n",
    "        path = self._bfs_path(env, current_pos, env.goal, current_keys)\n",
    "        if path:\n",
    "            plan.extend(path)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _bfs_path(self, env, start, goal, has_keys):\n",
    "        \"\"\"BFSë¡œ startì—ì„œ goalê¹Œì§€ ê²½ë¡œ ì°¾ê¸°\"\"\"\n",
    "        from collections import deque\n",
    "        \n",
    "        queue = deque([(start, [])])\n",
    "        visited = set([start])\n",
    "        \n",
    "        while queue:\n",
    "            pos, path = queue.popleft()\n",
    "            \n",
    "            if pos == goal:\n",
    "                return path\n",
    "            \n",
    "            for action in [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]:\n",
    "                new_pos = env._get_new_pos(pos, action)\n",
    "                \n",
    "                if new_pos not in visited:\n",
    "                    # ì´ë™ ê°€ëŠ¥ì„± ì²´í¬\n",
    "                    if self._can_move(env, new_pos, has_keys):\n",
    "                        visited.add(new_pos)\n",
    "                        queue.append((new_pos, path + [action]))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _can_move(self, env, pos, has_keys):\n",
    "        \"\"\"ì´ë™ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\"\"\"\n",
    "        y, x = pos\n",
    "        \n",
    "        if not (0 <= y < env.height and 0 <= x < env.width):\n",
    "            return False\n",
    "        \n",
    "        if env.grid[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # ë¬¸ ì²´í¬\n",
    "        if pos in env.doors:\n",
    "            key_idx = env.doors.index(pos)\n",
    "            if key_idx < len(env.keys):\n",
    "                required_key = env.keys[key_idx]\n",
    "                if required_key not in has_keys:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def act(self, env: ComplexMaze) -> Action:\n",
    "        \"\"\"\n",
    "        í–‰ë™ ì„ íƒ: ê³„íšì´ ì—†ìœ¼ë©´ THINK, ìˆìœ¼ë©´ ê³„íš ì‹¤í–‰\n",
    "        \"\"\"\n",
    "        if not self.plan:\n",
    "            return Action.THINK\n",
    "        \n",
    "        return self.plan.pop(0)\n",
    "    \n",
    "    def episode(self, env: ComplexMaze, verbose=False):\n",
    "        \"\"\"í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            action = self.act(env)\n",
    "            \n",
    "            if action == Action.THINK:\n",
    "                # THINK ì‹¤í–‰: ê³„íš ìˆ˜ë¦½\n",
    "                self.plan = self.think(env)\n",
    "                if verbose:\n",
    "                    print(f\"ğŸ¤” THINK: ê³„íš ìˆ˜ë¦½ (ê¸¸ì´: {len(self.plan)})\")\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            actions_taken.append(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if verbose and action != Action.THINK:\n",
    "                print(f\"í–‰ë™: {action.name}, ë³´ìƒ: {reward:.3f}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # ì—´ì‡  íšë“ ì‹œ ì¬ê³„íš\n",
    "            if len(env.has_keys) > len(state[2]):\n",
    "                self.plan = []  # ê³„íš ì´ˆê¸°í™”í•˜ì—¬ ë‹¤ì‹œ THINK\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        \n",
    "        return total_reward, actions_taken\n",
    "\n",
    "# ReAct ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "def test_react(n_episodes=10, verbose=True):\n",
    "    env = ComplexMaze('complex')\n",
    "    agent = ReActAgent(use_memory=True)\n",
    "    \n",
    "    print(\"\\ní›„ë°˜ì „: ReAct ì—ì´ì „íŠ¸ (ì¶”ë¡ ì´ ì™•)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"THINKë¥¼ ëª…ì‹œì  í–‰ë™ìœ¼ë¡œ ì‚¬ìš©\\n\")\n",
    "    \n",
    "    successes = 0\n",
    "    total_steps = []\n",
    "    think_counts = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        reward, actions = agent.episode(env, verbose=(episode == 0 and verbose))\n",
    "        \n",
    "        if env.pos == env.goal:\n",
    "            successes += 1\n",
    "        \n",
    "        total_steps.append(len(actions))\n",
    "        think_counts.append(sum(1 for a in actions if a == Action.THINK))\n",
    "        \n",
    "        if episode == 0 and verbose:\n",
    "            print(f\"\\nì²« ì—í”¼ì†Œë“œ ê²°ê³¼:\")\n",
    "            print(f\"  ì´ ìŠ¤í…: {len(actions)}\")\n",
    "            print(f\"  THINK íšŸìˆ˜: {think_counts[-1]}\")\n",
    "            print(f\"  ì´ ë³´ìƒ: {reward:.2f}\")\n",
    "            print(f\"  ì„±ê³µ: {env.pos == env.goal}\")\n",
    "    \n",
    "    print(f\"\\nì „ì²´ ê²°ê³¼ ({n_episodes} ì—í”¼ì†Œë“œ):\")\n",
    "    print(f\"  ì„±ê³µë¥ : {successes/n_episodes*100:.1f}%\")\n",
    "    print(f\"  í‰ê·  ìŠ¤í…: {np.mean(total_steps):.1f}\")\n",
    "    print(f\"  í‰ê·  THINK: {np.mean(think_counts):.1f}\")\n",
    "    print(f\"  í‰ê·  ë³´ìƒ: {np.mean(agent.episode_rewards):.2f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "react_agent = test_react(n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì„±ëŠ¥ ë¹„êµ: Q-Learning vs ReAct\n",
    "\n",
    "ë‘ ì ‘ê·¼ë²•ì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(n_episodes=100):\n",
    "    \"\"\"Q-Learning vs ReAct ë¹„êµ\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ”¥ ì „ë°˜ì „ vs í›„ë°˜ì „ ë¹„êµ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Q-Learning (ì „ë°˜ì „)\n",
    "    env_q = ComplexMaze('complex')\n",
    "    q_agent = QLearningAgent(n_actions=4, epsilon=0.1)\n",
    "    q_successes = 0\n",
    "    q_steps = []\n",
    "    \n",
    "    print(\"Q-Learning í•™ìŠµ ì¤‘...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        state = env_q.reset()\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < env_q.max_steps:\n",
    "            action = q_agent.get_action(state)\n",
    "            next_state, reward, done, _ = env_q.step(action)\n",
    "            q_agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if env_q.pos == env_q.goal:\n",
    "                    q_successes += 1\n",
    "                break\n",
    "        \n",
    "        q_steps.append(steps)\n",
    "        q_agent.episode_rewards.append(reward)\n",
    "    \n",
    "    # ReAct (í›„ë°˜ì „)\n",
    "    env_react = ComplexMaze('complex')\n",
    "    react_agent = ReActAgent(use_memory=True)\n",
    "    react_successes = 0\n",
    "    react_steps = []\n",
    "    react_thinks = []\n",
    "    \n",
    "    print(\"\\nReAct ì‹¤í–‰ ì¤‘...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, actions = react_agent.episode(env_react, verbose=False)\n",
    "        \n",
    "        if env_react.pos == env_react.goal:\n",
    "            react_successes += 1\n",
    "        \n",
    "        react_steps.append(len(actions))\n",
    "        react_thinks.append(sum(1 for a in actions if a == Action.THINK))\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {\n",
    "        'Metric': ['ì„±ê³µë¥  (%)', 'í‰ê·  ìŠ¤í…', 'ì²« ì„±ê³µ ì—í”¼ì†Œë“œ', 'í‰ê·  ë³´ìƒ'],\n",
    "        'Q-Learning': [\n",
    "            f\"{q_successes/n_episodes*100:.1f}\",\n",
    "            f\"{np.mean(q_steps):.1f}\",\n",
    "            str(next((i for i, r in enumerate(q_agent.episode_rewards) if r > 5), 'N/A')),\n",
    "            f\"{np.mean(q_agent.episode_rewards):.2f}\"\n",
    "        ],\n",
    "        'ReAct': [\n",
    "            f\"{react_successes/n_episodes*100:.1f}\",\n",
    "            f\"{np.mean(react_steps):.1f}\",\n",
    "            '1',  # ReActëŠ” ë³´í†µ ì²« ì—í”¼ì†Œë“œë¶€í„° ì„±ê³µ\n",
    "            f\"{np.mean(react_agent.episode_rewards):.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nReAct í‰ê·  THINK íšŸìˆ˜: {np.mean(react_thinks):.1f}\")\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # ì„±ê³µë¥  ì¶”ì´\n",
    "    ax = axes[0, 0]\n",
    "    q_success_rate = np.cumsum([1 if r > 5 else 0 for r in q_agent.episode_rewards]) / np.arange(1, n_episodes+1)\n",
    "    react_success_rate = np.cumsum([1 if r > 5 else 0 for r in react_agent.episode_rewards]) / np.arange(1, n_episodes+1)\n",
    "    \n",
    "    ax.plot(q_success_rate, label='Q-Learning', linewidth=2)\n",
    "    ax.plot(react_success_rate, label='ReAct', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('ì„±ê³µë¥  ì¶”ì´')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ìŠ¤í… ìˆ˜ ë¹„êµ\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(q_steps, label='Q-Learning', alpha=0.6)\n",
    "    ax.plot(react_steps, label='ReAct', alpha=0.6)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('ì—í”¼ì†Œë“œë‹¹ ìŠ¤í… ìˆ˜')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ë³´ìƒ ì¶”ì´\n",
    "    ax = axes[1, 0]\n",
    "    window = 10\n",
    "    q_rewards_smooth = np.convolve(q_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    react_rewards_smooth = np.convolve(react_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(q_rewards_smooth, label='Q-Learning', linewidth=2)\n",
    "    ax.plot(react_rewards_smooth, label='ReAct', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Reward')\n",
    "    ax.set_title('ë³´ìƒ ì¶”ì´ (ì´ë™í‰ê· )')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # THINK íšŸìˆ˜ (ReActë§Œ)\n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(range(len(react_thinks[:20])), react_thinks[:20], color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('THINK Count')\n",
    "    ax.set_title('ReAct: THINK í–‰ë™ íšŸìˆ˜ (ì²˜ìŒ 20 ì—í”¼ì†Œë“œ)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\n",
    "    print(\"1. ReActëŠ” ì²« ì—í”¼ì†Œë“œë¶€í„° ì„±ê³µ (ì¶”ë¡  í™œìš©)\")\n",
    "    print(\"2. Q-Learningì€ ë§ì€ ì—í”¼ì†Œë“œ í•„ìš” (ì‹œí–‰ì°©ì˜¤)\")\n",
    "    print(\"3. THINK í–‰ë™ìœ¼ë¡œ ê³„íš ìˆ˜ë¦½ â†’ íš¨ìœ¨ì  ì‹¤í–‰\")\n",
    "    print(\"4. ë©”ëª¨ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ë°˜ë³µ ì‹œ ë”ìš± ë¹ ë¦„\")\n",
    "\n",
    "compare_approaches(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MCTS (Monte Carlo Tree Search)\n",
    "\n",
    "ë” ë³µì¡í•œ ê³„íšì„ ìœ„í•œ MCTSë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    \"\"\"MCTS ë…¸ë“œ\"\"\"\n",
    "    \n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        self.untried_actions = [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.untried_actions) == 0\n",
    "    \n",
    "    def best_child(self, c_param=1.4):\n",
    "        \"\"\"UCB1 ê¸°ë°˜ ìµœì„ ì˜ ìì‹ ì„ íƒ\"\"\"\n",
    "        choices_weights = [\n",
    "            (child.value / child.visits) + c_param * math.sqrt(2 * math.log(self.visits) / child.visits)\n",
    "            for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def expand(self, action, next_state):\n",
    "        \"\"\"ìƒˆ ìì‹ ë…¸ë“œ ì¶”ê°€\"\"\"\n",
    "        child = MCTSNode(next_state, parent=self, action=action)\n",
    "        self.untried_actions.remove(action)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def update(self, value):\n",
    "        \"\"\"ë…¸ë“œ ê°’ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.visits += 1\n",
    "        self.value += value\n",
    "\n",
    "class MCTSAgent:\n",
    "    \"\"\"MCTS ê¸°ë°˜ ê³„íš ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, n_simulations=100):\n",
    "        self.n_simulations = n_simulations\n",
    "        self.think_time = 0\n",
    "    \n",
    "    def think(self, env: ComplexMaze, state):\n",
    "        \"\"\"MCTSë¡œ ìµœì„ ì˜ í–‰ë™ ê³„íš\"\"\"\n",
    "        root = MCTSNode(state)\n",
    "        \n",
    "        for _ in range(self.n_simulations):\n",
    "            node = root\n",
    "            temp_env = self._copy_env(env)\n",
    "            \n",
    "            # Selection\n",
    "            while node.is_fully_expanded() and len(node.children) > 0:\n",
    "                node = node.best_child()\n",
    "                temp_env.step(node.action)\n",
    "            \n",
    "            # Expansion\n",
    "            if not node.is_fully_expanded():\n",
    "                action = random.choice(node.untried_actions)\n",
    "                next_state, _, _, _ = temp_env.step(action)\n",
    "                node = node.expand(action, next_state)\n",
    "            \n",
    "            # Simulation\n",
    "            value = self._simulate(temp_env)\n",
    "            \n",
    "            # Backpropagation\n",
    "            while node is not None:\n",
    "                node.update(value)\n",
    "                node = node.parent\n",
    "        \n",
    "        # ìµœì„ ì˜ í–‰ë™ ì„ íƒ\n",
    "        best_action = root.best_child(c_param=0).action\n",
    "        self.think_time += 1\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _copy_env(self, env):\n",
    "        \"\"\"í™˜ê²½ ë³µì‚¬ (ê°„ë‹¨í•œ ë²„ì „)\"\"\"\n",
    "        import copy\n",
    "        return copy.deepcopy(env)\n",
    "    \n",
    "    def _simulate(self, env):\n",
    "        \"\"\"ëœë¤ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        max_steps = 50\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT])\n",
    "            _, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "# MCTS í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸŒ³ MCTS (Monte Carlo Tree Search)\")\n",
    "print(\"=\"*50)\n",
    "print(\"ë” ë³µì¡í•œ ê³„íšì„ ìœ„í•œ íŠ¸ë¦¬ íƒìƒ‰\\n\")\n",
    "\n",
    "env = ComplexMaze('simple')  # ê°„ë‹¨í•œ ë¯¸ë¡œë¡œ í…ŒìŠ¤íŠ¸\n",
    "mcts_agent = MCTSAgent(n_simulations=50)\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"MCTS ì‹¤í–‰:\")\n",
    "while steps < 30:\n",
    "    # THINK: MCTSë¡œ ê³„íš\n",
    "    action = mcts_agent.think(env, state)\n",
    "    \n",
    "    # ACT: í–‰ë™ ì‹¤í–‰\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if steps <= 10:\n",
    "        print(f\"Step {steps}: {action.name} â†’ ë³´ìƒ: {reward:.3f}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nì™„ë£Œ! ì´ ìŠ¤í…: {steps}, ì´ ë³´ìƒ: {total_reward:.2f}\")\n",
    "        print(f\"THINK íšŸìˆ˜: {mcts_agent.think_time}\")\n",
    "        break\n",
    "    \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chain-of-Thought in RL\n",
    "\n",
    "LLMì˜ Chain-of-Thoughtë¥¼ RLì— ì ìš©í•œ ì˜ˆì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThoughtAgent:\n",
    "    \"\"\"\n",
    "    Chain-of-Thought ì¶”ë¡ ì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸\n",
    "    ê° ë‹¨ê³„ë§ˆë‹¤ ëª…ì‹œì ì¸ ì¶”ë¡  ê³¼ì •ì„ ê±°ì¹¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.thoughts = []  # ì¶”ë¡  ì²´ì¸\n",
    "        self.plan = []\n",
    "    \n",
    "    def think_step_by_step(self, env: ComplexMaze):\n",
    "        \"\"\"\n",
    "        ë‹¨ê³„ë³„ ì¶”ë¡  (Chain-of-Thought)\n",
    "        \"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        # Step 1: í˜„ì¬ ìƒí™© íŒŒì•…\n",
    "        thought1 = f\"í˜„ì¬ ìœ„ì¹˜: {env.pos}, ëª©í‘œ: {env.goal}\"\n",
    "        thoughts.append(thought1)\n",
    "        \n",
    "        # Step 2: í•„ìš”í•œ ê²ƒ íŒŒì•…\n",
    "        needs_key = False\n",
    "        for door in env.doors:\n",
    "            key_idx = env.doors.index(door)\n",
    "            if key_idx < len(env.keys):\n",
    "                key = env.keys[key_idx]\n",
    "                if key not in env.has_keys:\n",
    "                    needs_key = True\n",
    "                    thought2 = f\"ë¬¸({door})ì„ í†µê³¼í•˜ë ¤ë©´ ì—´ì‡ ({key})ê°€ í•„ìš”í•¨\"\n",
    "                    thoughts.append(thought2)\n",
    "                    break\n",
    "        \n",
    "        if not needs_key:\n",
    "            thought2 = \"ëª¨ë“  í•„ìš”í•œ ì—´ì‡ ë¥¼ ë³´ìœ  ì¤‘\"\n",
    "            thoughts.append(thought2)\n",
    "        \n",
    "        # Step 3: ì „ëµ ìˆ˜ë¦½\n",
    "        if needs_key:\n",
    "            thought3 = \"ì „ëµ: ì—´ì‡  íšë“ â†’ ë¬¸ í†µê³¼ â†’ ëª©í‘œ ë„ë‹¬\"\n",
    "        else:\n",
    "            thought3 = \"ì „ëµ: ì§ì ‘ ëª©í‘œë¡œ ì´ë™\"\n",
    "        thoughts.append(thought3)\n",
    "        \n",
    "        # Step 4: êµ¬ì²´ì  ê³„íš\n",
    "        if needs_key:\n",
    "            thought4 = \"ê³„íš: \"\n",
    "            # ì—´ì‡ ê¹Œì§€ ê±°ë¦¬\n",
    "            key_dist = abs(env.pos[0] - env.keys[0][0]) + abs(env.pos[1] - env.keys[0][1])\n",
    "            thought4 += f\"ì—´ì‡ ê¹Œì§€ ì•½ {key_dist}ìŠ¤í…, \"\n",
    "            # ëª©í‘œê¹Œì§€ ê±°ë¦¬\n",
    "            goal_dist = abs(env.keys[0][0] - env.goal[0]) + abs(env.keys[0][1] - env.goal[1])\n",
    "            thought4 += f\"ê·¸ í›„ ëª©í‘œê¹Œì§€ ì•½ {goal_dist}ìŠ¤í…\"\n",
    "        else:\n",
    "            goal_dist = abs(env.pos[0] - env.goal[0]) + abs(env.pos[1] - env.goal[1])\n",
    "            thought4 = f\"ê³„íš: ëª©í‘œê¹Œì§€ ì•½ {goal_dist}ìŠ¤í…\"\n",
    "        thoughts.append(thought4)\n",
    "        \n",
    "        self.thoughts.extend(thoughts)\n",
    "        return thoughts\n",
    "    \n",
    "    def reason_and_act(self, env: ComplexMaze, verbose=True):\n",
    "        \"\"\"ì¶”ë¡ í•˜ê³  í–‰ë™\"\"\"\n",
    "        # Chain-of-Thought ì¶”ë¡ \n",
    "        thoughts = self.think_step_by_step(env)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nğŸ§  Chain-of-Thought:\")\n",
    "            for i, thought in enumerate(thoughts, 1):\n",
    "                print(f\"  {i}. {thought}\")\n",
    "        \n",
    "        # ì¶”ë¡  ê¸°ë°˜ ê³„íš ìˆ˜ë¦½ (ReActì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë” ëª…ì‹œì )\n",
    "        react_agent = ReActAgent(use_memory=False)\n",
    "        self.plan = react_agent.think(env)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nâ†’ ìƒì„±ëœ ê³„íš: {len(self.plan)}ê°œ í–‰ë™\")\n",
    "        \n",
    "        return self.plan\n",
    "\n",
    "# Chain-of-Thought ë°ëª¨\n",
    "print(\"\\nğŸ§  Chain-of-Thought in RL\")\n",
    "print(\"=\"*50)\n",
    "print(\"ëª…ì‹œì  ì¶”ë¡  ì²´ì¸ì„ í†µí•œ ë¬¸ì œ í•´ê²°\\n\")\n",
    "\n",
    "env = ComplexMaze('complex')\n",
    "cot_agent = ChainOfThoughtAgent()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"ì´ˆê¸° ìƒíƒœ:\")\n",
    "env.render()\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì¶”ë¡ \n",
    "plan = cot_agent.reason_and_act(env, verbose=True)\n",
    "\n",
    "# ê³„íš ì‹¤í–‰\n",
    "print(\"\\nê³„íš ì‹¤í–‰:\")\n",
    "total_reward = 0\n",
    "for i, action in enumerate(plan[:10]):  # ì²˜ìŒ 10ê°œ í–‰ë™ë§Œ í‘œì‹œ\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"  {i+1}. {action.name} â†’ ë³´ìƒ: {reward:.3f}\")\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"\\nê²°ê³¼: ì´ ë³´ìƒ = {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test-time Computeì™€ ì¶”ë¡  ìŠ¤ì¼€ì¼ë§\n",
    "\n",
    "ì¶”ë¡  ì‹œì ì— ë” ë§ì€ ê³„ì‚°ì„ íˆ¬ì…í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_compute_analysis():\n",
    "    \"\"\"\n",
    "    Test-time computeì˜ íš¨ê³¼ ë¶„ì„\n",
    "    ë” ë§ì€ ì¶”ë¡  = ë” ë‚˜ì€ ì„±ëŠ¥?\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nâš¡ Test-time Compute ë¶„ì„\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"ì¶”ë¡  ì‹œê°„ì„ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ”ê°€?\\n\")\n",
    "    \n",
    "    env = ComplexMaze('complex')\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ì¶”ë¡  ê¹Šì´\n",
    "    think_depths = [0, 1, 5, 10, 20]\n",
    "    results = []\n",
    "    \n",
    "    for depth in think_depths:\n",
    "        env.reset()\n",
    "        \n",
    "        if depth == 0:\n",
    "            # ì¶”ë¡  ì—†ì´ ëœë¤\n",
    "            steps = 0\n",
    "            while steps < 100:\n",
    "                action = random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT])\n",
    "                _, _, done, _ = env.step(action)\n",
    "                steps += 1\n",
    "                if done:\n",
    "                    break\n",
    "            success = env.pos == env.goal\n",
    "            \n",
    "        else:\n",
    "            # MCTS with different simulation depths\n",
    "            mcts = MCTSAgent(n_simulations=depth * 10)\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 100:\n",
    "                state = env._get_state()\n",
    "                action = mcts.think(env, state)\n",
    "                _, _, done, _ = env.step(action)\n",
    "                steps += 1\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            success = env.pos == env.goal\n",
    "        \n",
    "        results.append({\n",
    "            'depth': depth,\n",
    "            'success': success,\n",
    "            'steps': steps,\n",
    "            'compute': depth * 10  # ì¶”ë¡  ê³„ì‚°ëŸ‰\n",
    "        })\n",
    "        \n",
    "        print(f\"ì¶”ë¡  ê¹Šì´ {depth:2d}: ì„±ê³µ={success}, ìŠ¤í…={steps:3d}\")\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ì„±ê³µë¥  vs ì¶”ë¡  ê¹Šì´\n",
    "    ax = axes[0]\n",
    "    success_rate = [r['success'] for r in results]\n",
    "    ax.bar(think_depths, success_rate, color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Think Depth')\n",
    "    ax.set_ylabel('Success')\n",
    "    ax.set_title('ì¶”ë¡  ê¹Šì´ì™€ ì„±ê³µë¥ ')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ìŠ¤í… ìˆ˜ vs ì¶”ë¡  ê¹Šì´\n",
    "    ax = axes[1]\n",
    "    steps = [r['steps'] for r in results]\n",
    "    ax.plot(think_depths, steps, 'o-', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Think Depth')\n",
    "    ax.set_ylabel('Steps to Goal')\n",
    "    ax.set_title('ì¶”ë¡  ê¹Šì´ì™€ íš¨ìœ¨ì„±')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:\")\n",
    "    print(\"1. ì¶”ë¡  ì—†ìŒ (depth=0): ëœë¤ íƒìƒ‰, ì‹¤íŒ¨\")\n",
    "    print(\"2. ì–•ì€ ì¶”ë¡  (depth=1-5): ê¸°ë³¸ì ì¸ ê³„íš\")\n",
    "    print(\"3. ê¹Šì€ ì¶”ë¡  (depth=10+): ìµœì  ê²½ë¡œ ë°œê²¬\")\n",
    "    print(\"4. Test-time compute â†‘ â†’ ì„±ëŠ¥ â†‘\")\n",
    "\n",
    "test_time_compute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë©”íƒ€ í•™ìŠµ: ì¶”ë¡  ìì²´ë¥¼ í•™ìŠµ\n",
    "\n",
    "ì¶”ë¡  ê³¼ì • ìì²´ë¥¼ í•™ìŠµí•˜ëŠ” ë©”íƒ€ RL ì ‘ê·¼ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaReasoningAgent:\n",
    "    \"\"\"\n",
    "    ë©”íƒ€ ì¶”ë¡  ì—ì´ì „íŠ¸\n",
    "    ì–´ë–»ê²Œ ì¶”ë¡ í• ì§€ë¥¼ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reasoning_strategies = [\n",
    "            'direct',      # ì§ì ‘ ëª©í‘œë¡œ\n",
    "            'explore',     # íƒìƒ‰ ìš°ì„ \n",
    "            'systematic',  # ì²´ê³„ì  íƒìƒ‰\n",
    "            'planning'     # ê³„íš ìš°ì„ \n",
    "        ]\n",
    "        self.strategy_scores = defaultdict(float)\n",
    "        self.current_strategy = None\n",
    "    \n",
    "    def select_strategy(self, env_features):\n",
    "        \"\"\"\n",
    "        í™˜ê²½ íŠ¹ì„±ì— ë”°ë¼ ì¶”ë¡  ì „ëµ ì„ íƒ\n",
    "        \"\"\"\n",
    "        # í™˜ê²½ ë³µì¡ë„ ì¶”ì •\n",
    "        has_obstacles = '#' in str(env_features.get('grid', ''))\n",
    "        has_keys = len(env_features.get('keys', [])) > 0\n",
    "        maze_size = env_features.get('size', 0)\n",
    "        \n",
    "        if not has_keys and maze_size < 25:\n",
    "            strategy = 'direct'\n",
    "        elif has_keys:\n",
    "            strategy = 'planning'\n",
    "        elif has_obstacles:\n",
    "            strategy = 'systematic'\n",
    "        else:\n",
    "            strategy = 'explore'\n",
    "        \n",
    "        # í•™ìŠµëœ ì ìˆ˜ ë°˜ì˜\n",
    "        if self.strategy_scores:\n",
    "            best_strategy = max(self.strategy_scores.items(), key=lambda x: x[1])\n",
    "            if best_strategy[1] > 0.5:  # ì¶©ë¶„íˆ ì¢‹ì€ ì „ëµì´ ìˆìœ¼ë©´\n",
    "                strategy = best_strategy[0]\n",
    "        \n",
    "        self.current_strategy = strategy\n",
    "        return strategy\n",
    "    \n",
    "    def execute_strategy(self, strategy, env):\n",
    "        \"\"\"\n",
    "        ì„ íƒëœ ì „ëµ ì‹¤í–‰\n",
    "        \"\"\"\n",
    "        if strategy == 'direct':\n",
    "            # ì§ì ‘ ê²½ë¡œ\n",
    "            return self._direct_path(env)\n",
    "        elif strategy == 'explore':\n",
    "            # íƒìƒ‰ ìš°ì„ \n",
    "            return self._explore_first(env)\n",
    "        elif strategy == 'systematic':\n",
    "            # ì²´ê³„ì  íƒìƒ‰\n",
    "            return self._systematic_search(env)\n",
    "        elif strategy == 'planning':\n",
    "            # ê³„íš ê¸°ë°˜\n",
    "            react = ReActAgent()\n",
    "            return react.think(env)\n",
    "    \n",
    "    def _direct_path(self, env):\n",
    "        \"\"\"ëª©í‘œê¹Œì§€ ì§ì ‘ ê²½ë¡œ\"\"\"\n",
    "        path = []\n",
    "        current = env.pos\n",
    "        goal = env.goal\n",
    "        \n",
    "        while current != goal:\n",
    "            if current[0] < goal[0]:\n",
    "                path.append(Action.DOWN)\n",
    "                current = (current[0] + 1, current[1])\n",
    "            elif current[0] > goal[0]:\n",
    "                path.append(Action.UP)\n",
    "                current = (current[0] - 1, current[1])\n",
    "            elif current[1] < goal[1]:\n",
    "                path.append(Action.RIGHT)\n",
    "                current = (current[0], current[1] + 1)\n",
    "            else:\n",
    "                path.append(Action.LEFT)\n",
    "                current = (current[0], current[1] - 1)\n",
    "            \n",
    "            if len(path) > 100:  # ë¬´í•œ ë£¨í”„ ë°©ì§€\n",
    "                break\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def _explore_first(self, env):\n",
    "        \"\"\"íƒìƒ‰ ìš°ì„  ì „ëµ\"\"\"\n",
    "        # ê°„ë‹¨í•œ DFS íƒìƒ‰\n",
    "        return [random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]) \n",
    "                for _ in range(20)]\n",
    "    \n",
    "    def _systematic_search(self, env):\n",
    "        \"\"\"ì²´ê³„ì  íƒìƒ‰\"\"\"\n",
    "        # ë‚˜ì„ í˜• íƒìƒ‰ íŒ¨í„´\n",
    "        pattern = [Action.RIGHT, Action.DOWN, Action.LEFT, Action.LEFT, \n",
    "                  Action.UP, Action.UP, Action.RIGHT, Action.RIGHT]\n",
    "        return pattern * 5\n",
    "    \n",
    "    def update_scores(self, strategy, reward):\n",
    "        \"\"\"ì „ëµ ì ìˆ˜ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.strategy_scores[strategy] = 0.9 * self.strategy_scores[strategy] + 0.1 * reward\n",
    "\n",
    "# ë©”íƒ€ ì¶”ë¡  ë°ëª¨\n",
    "print(\"\\nğŸ“ ë©”íƒ€ ì¶”ë¡ : ì¶”ë¡  ë°©ë²•ì„ í•™ìŠµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "meta_agent = MetaReasoningAgent()\n",
    "\n",
    "# ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸\n",
    "env_types = ['simple', 'complex']\n",
    "\n",
    "for env_type in env_types:\n",
    "    print(f\"\\ní™˜ê²½: {env_type}\")\n",
    "    env = ComplexMaze(env_type)\n",
    "    \n",
    "    # í™˜ê²½ íŠ¹ì„± ì¶”ì¶œ\n",
    "    env_features = {\n",
    "        'grid': env.grid,\n",
    "        'keys': env.keys,\n",
    "        'size': env.height * env.width\n",
    "    }\n",
    "    \n",
    "    # ì „ëµ ì„ íƒ\n",
    "    strategy = meta_agent.select_strategy(env_features)\n",
    "    print(f\"ì„ íƒëœ ì „ëµ: {strategy}\")\n",
    "    \n",
    "    # ì „ëµ ì‹¤í–‰\n",
    "    state = env.reset()\n",
    "    plan = meta_agent.execute_strategy(strategy, env)\n",
    "    \n",
    "    # ì‹¤í–‰ ë° í‰ê°€\n",
    "    total_reward = 0\n",
    "    for action in plan[:30]:\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # ì ìˆ˜ ì—…ë°ì´íŠ¸\n",
    "    meta_agent.update_scores(strategy, total_reward)\n",
    "    \n",
    "    print(f\"ê²°ê³¼: ë³´ìƒ={total_reward:.2f}, ì„±ê³µ={env.pos == env.goal}\")\n",
    "\n",
    "print(\"\\ní•™ìŠµëœ ì „ëµ ì ìˆ˜:\")\n",
    "for strategy, score in meta_agent.strategy_scores.items():\n",
    "    print(f\"  {strategy}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìš”ì•½: íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì˜ ì˜ë¯¸\n",
    "\n",
    "### ì „ë°˜ì „ â†’ í›„ë°˜ì „ ì „í™˜ì˜ í•µì‹¬\n",
    "\n",
    "1. **ì•Œê³ ë¦¬ì¦˜ ìµœì í™” â†’ ì¶”ë¡  í™œìš©**\n",
    "   - ë” ë‚˜ì€ ì—…ë°ì´íŠ¸ ê·œì¹™ â†’ ë” ë‚˜ì€ ê³„íš\n",
    "   - ë§ì€ ê²½í—˜ í•„ìš” â†’ ì ì€ ê²½í—˜ìœ¼ë¡œ í•´ê²°\n",
    "\n",
    "2. **THINK as Action**\n",
    "   - ì¶”ë¡ ì„ ëª…ì‹œì  í–‰ë™ìœ¼ë¡œ\n",
    "   - ë‚´ë¶€ ê³„ì‚°ë„ í™˜ê²½ì˜ ì¼ë¶€\n",
    "\n",
    "3. **Test-time Compute**\n",
    "   - í•™ìŠµ ì‹œê°„ â†’ ì¶”ë¡  ì‹œê°„\n",
    "   - ë” ë§ì€ ì¶”ë¡  = ë” ë‚˜ì€ ì„±ëŠ¥\n",
    "\n",
    "4. **ì‚¬ì „ì§€ì‹ê³¼ ê³„íš**\n",
    "   - Tabula rasa â†’ Prior knowledge\n",
    "   - ì‹œí–‰ì°©ì˜¤ â†’ ê³„íš ê¸°ë°˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ì •ë¦¬\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ í•µì‹¬ í†µì°°: ì¶”ë¡  ê¸°ë°˜ RLì˜ ë¯¸ë˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "insights = \"\"\"\n",
    "1. ì „í†µì  RL (ì „ë°˜ì „)\n",
    "   â€¢ Q-learning, DQN, PPO\n",
    "   â€¢ ë§ì€ ê²½í—˜ í•„ìš”\n",
    "   â€¢ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì— ì§‘ì¤‘\n",
    "   â€¢ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ê²½ìŸ\n",
    "\n",
    "2. ì¶”ë¡  ê¸°ë°˜ RL (í›„ë°˜ì „)\n",
    "   â€¢ ReAct, Chain-of-Thought\n",
    "   â€¢ ì ì€ ê²½í—˜ìœ¼ë¡œ í•´ê²°\n",
    "   â€¢ ì¶”ë¡ ê³¼ ê³„íšì— ì§‘ì¤‘\n",
    "   â€¢ ì‹¤ì œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥\n",
    "\n",
    "3. THINK as Actionì˜ ì˜ë¯¸\n",
    "   â€¢ ì¶”ë¡  ìì²´ê°€ í–‰ë™\n",
    "   â€¢ ë‚´ë¶€ ìƒíƒœ vs ì™¸ë¶€ ìƒíƒœ\n",
    "   â€¢ ë©”íƒ€ ì¸ì§€ì˜ ì¤‘ìš”ì„±\n",
    "\n",
    "4. LLMê³¼ì˜ ìœµí•©\n",
    "   â€¢ ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ì§€ì‹\n",
    "   â€¢ ì¶”ë¡  ëŠ¥ë ¥ í™œìš©\n",
    "   â€¢ Few-shot í•™ìŠµ\n",
    "\n",
    "5. ë¯¸ë˜ ë°©í–¥\n",
    "   â€¢ ë” ë³µì¡í•œ ì¶”ë¡  ì²´ì¸\n",
    "   â€¢ ìê¸° ë°˜ì„± (self-reflection)\n",
    "   â€¢ ë©”íƒ€ í•™ìŠµê³¼ ì ì‘\n",
    "   â€¢ ì¸ê°„ ìˆ˜ì¤€ì˜ ê³„íš ëŠ¥ë ¥\n",
    "\"\"\"\n",
    "\n",
    "print(insights)\n",
    "\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë…¸íŠ¸ë¶ ì˜ˆê³ :\")\n",
    "print(\"Notebook 5: LLM + RL - RLHFì™€ ë¯¸ë˜\")\n",
    "print(\"â€¢ RLHFë¡œ LLM ì •ë ¬\")\n",
    "print(\"â€¢ Constitutional AI\")\n",
    "print(\"â€¢ LLM as Policy\")\n",
    "print(\"â€¢ RLì˜ ë¯¸ë˜ ì „ë§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\n",
    "print(\"\\nğŸ¯ í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(\"âœ… ì „ë°˜ì „ vs í›„ë°˜ì „ íŒ¨ëŸ¬ë‹¤ì„ ì´í•´\")\n",
    "print(\"âœ… THINKë¥¼ í–‰ë™ìœ¼ë¡œ êµ¬í˜„\")\n",
    "print(\"âœ… ReAct ì—ì´ì „íŠ¸ êµ¬í˜„\")\n",
    "print(\"âœ… MCTS ì´í•´ ë° êµ¬í˜„\")\n",
    "print(\"âœ… Chain-of-Thought in RL\")\n",
    "print(\"âœ… Test-time compute ë¶„ì„\")\n",
    "print(\"âœ… ë©”íƒ€ ì¶”ë¡  ê°œë… ì´í•´\")\n",
    "print(\"\\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ì¶”ë¡  ê¸°ë°˜ RLì˜ í•µì‹¬ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}