{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Ï∂îÎ°† Í∏∞Î∞ò RL - PlanningÍ≥º ReAct\n",
    "\n",
    "## üéØ ÌïôÏäµ Î™©Ìëú\n",
    "- Model-based RLÍ≥º Planning Ïù¥Ìï¥\n",
    "- MCTS (Monte Carlo Tree Search) Íµ¨ÌòÑ\n",
    "- ReAct (Reasoning + Acting) Ìå®Îü¨Îã§ÏûÑ ÌïôÏäµ\n",
    "- Ï∂îÎ°†ÏùÑ ÌñâÎèôÏúºÎ°ú ÌÜµÌï©ÌïòÎäî Î∞©Î≤ï Ïã§Ïäµ\n",
    "- \"ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Ïôï\"ÏóêÏÑú \"Ï∂îÎ°†Ïù¥ Ïôï\"ÏúºÎ°úÏùò Ï†ÑÌôò Ïù¥Ìï¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ìå®Îü¨Îã§ÏûÑ Ï†ÑÌôò: ÏïåÍ≥†Î¶¨Ï¶ò ‚Üí Ï∂îÎ°†\n",
    "\n",
    "### Ï†ÑÎ∞òÏ†Ñ: \"ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Ïôï\"\n",
    "- Q-learning, DQN, PPO Îì± ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî\n",
    "- Îçî ÎÇòÏùÄ ÏóÖÎç∞Ïù¥Ìä∏ Í∑úÏπô, Îçî Ìö®Ïú®Ï†ÅÏù∏ ÌïôÏäµ\n",
    "- Î≤§ÏπòÎßàÌÅ¨ Ï†êÏàò Í≤ΩÏüÅ\n",
    "\n",
    "### ÌõÑÎ∞òÏ†Ñ: \"Ï∂îÎ°†Ïù¥ Ïôï\"\n",
    "- ÏÇ¨Ï†ÑÏßÄÏãù(priors)Í≥º Í≥ÑÌöç(planning) ÌôúÏö©\n",
    "- Test-time compute: Ï∂îÎ°† ÏãúÏ†êÏóê Îçî ÎßéÏùÄ Í≥ÑÏÇ∞\n",
    "- **THINKÎ•º ÌñâÎèôÏúºÎ°ú**: ÎÇ¥Î∂Ä Ï∂îÎ°† Í≥ºÏ†ï ÏûêÏ≤¥Í∞Ä ÌñâÎèôÏù¥ Îê®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Ï∂îÎ°† Í∏∞Î∞ò RL ÌôòÍ≤Ω Ï§ÄÎπÑ ÏôÑÎ£å!\")\n",
    "print(\"Key Insight: Ï∂îÎ°†(Reasoning)ÏùÑ ÌñâÎèô(Action)ÏúºÎ°ú ÌÜµÌï©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ÌôòÍ≤Ω: Ïó¥Ïá†-Î¨∏-Î™©Ìëú ÎØ∏Î°ú\n",
    "\n",
    "Í∏∞Ï°¥ RL ÎÖ∏Ìä∏Î∂ÅÏùò ÌôòÍ≤ΩÏùÑ ÌôïÏû•ÌïòÏó¨, Ï∂îÎ°†Ïù¥ ÌïÑÏöîÌïú Î≥µÏû°Ìïú ÎØ∏Î°úÎ•º Íµ¨Ï∂ïÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    THINK = 4  # üéØ ÌïµÏã¨: THINKÎ•º ÌñâÎèôÏúºÎ°ú!\n",
    "\n",
    "class ComplexMaze:\n",
    "    \"\"\"Î≥µÏû°Ìïú ÎØ∏Î°ú ÌôòÍ≤Ω (Ïó¥Ïá†-Î¨∏-Î™©Ìëú)\"\"\"\n",
    "    \n",
    "    def __init__(self, maze_type='complex'):\n",
    "        self.maze_type = maze_type\n",
    "        self._create_maze()\n",
    "        self.reset()\n",
    "    \n",
    "    def _create_maze(self):\n",
    "        if self.maze_type == 'simple':\n",
    "            self.grid = [\n",
    "                \"S....\",\n",
    "                \".###.\",\n",
    "                \".....\",\n",
    "                \".###.\",\n",
    "                \"....G\"\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (4, 4)\n",
    "            self.keys = []\n",
    "            self.doors = []\n",
    "            \n",
    "        elif self.maze_type == 'complex':\n",
    "            self.grid = [\n",
    "                \"#########\",\n",
    "                \"#S.....##\",\n",
    "                \"#.##.#.##\",\n",
    "                \"#.#D.#K##\",\n",
    "                \"#.#..#.##\",\n",
    "                \"#.####.##\",\n",
    "                \"#......G#\",\n",
    "                \"#########\"\n",
    "            ]\n",
    "            self.start = (1, 1)\n",
    "            self.goal = (6, 7)\n",
    "            self.keys = [(3, 6)]  # K ÏúÑÏπò\n",
    "            self.doors = [(3, 3)]  # D ÏúÑÏπò\n",
    "            \n",
    "        self.height = len(self.grid)\n",
    "        self.width = len(self.grid[0])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pos = self.start\n",
    "        self.has_keys = set()\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.height * self.width * 10\n",
    "        self.think_count = 0  # THINK ÌñâÎèô ÌöüÏàò\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # ÏÉÅÌÉú = (ÏúÑÏπò, Ïó¥Ïá† Î≥¥Ïú† ÏÉÅÌÉú)\n",
    "        key_tuple = tuple(sorted(self.has_keys))\n",
    "        return (self.pos[0], self.pos[1], key_tuple)\n",
    "    \n",
    "    def step(self, action: Action):\n",
    "        self.steps += 1\n",
    "        reward = -0.01  # Í∏∞Î≥∏ Ïä§ÌÖù Ìå®ÎÑêÌã∞\n",
    "        \n",
    "        if action == Action.THINK:\n",
    "            # THINK ÌñâÎèô: Ïô∏Î∂Ä ÏÉÅÌÉúÎäî Î≥ÄÌïòÏßÄ ÏïäÏùå\n",
    "            self.think_count += 1\n",
    "            reward = -0.005  # ÏûëÏùÄ Ìå®ÎÑêÌã∞\n",
    "            info = {'think': True, 'think_count': self.think_count}\n",
    "        else:\n",
    "            # Î¨ºÎ¶¨Ï†Å Ïù¥Îèô\n",
    "            new_pos = self._get_new_pos(self.pos, action)\n",
    "            \n",
    "            if self._is_valid_move(new_pos):\n",
    "                self.pos = new_pos\n",
    "                \n",
    "                # Ïó¥Ïá† ÌöçÎìù\n",
    "                if new_pos in self.keys and new_pos not in self.has_keys:\n",
    "                    self.has_keys.add(new_pos)\n",
    "                    reward += 1.0\n",
    "                    print(f\"üîë Ïó¥Ïá† ÌöçÎìù! (ÏúÑÏπò: {new_pos})\")\n",
    "            \n",
    "            info = {'think': False}\n",
    "        \n",
    "        # Î™©Ìëú ÎèÑÎã¨ ÌôïÏù∏\n",
    "        if self.pos == self.goal:\n",
    "            reward += 10.0\n",
    "            done = True\n",
    "            print(f\"üéâ Î™©Ìëú ÎèÑÎã¨! (Ï¥ù Ïä§ÌÖù: {self.steps}, THINK ÌöüÏàò: {self.think_count})\")\n",
    "        else:\n",
    "            done = self.steps >= self.max_steps\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _get_new_pos(self, pos, action):\n",
    "        y, x = pos\n",
    "        if action == Action.UP:\n",
    "            return (y-1, x)\n",
    "        elif action == Action.DOWN:\n",
    "            return (y+1, x)\n",
    "        elif action == Action.LEFT:\n",
    "            return (y, x-1)\n",
    "        elif action == Action.RIGHT:\n",
    "            return (y, x+1)\n",
    "        return pos\n",
    "    \n",
    "    def _is_valid_move(self, pos):\n",
    "        y, x = pos\n",
    "        \n",
    "        # Í≤ΩÍ≥Ñ ÌôïÏù∏\n",
    "        if not (0 <= y < self.height and 0 <= x < self.width):\n",
    "            return False\n",
    "        \n",
    "        # Î≤Ω ÌôïÏù∏\n",
    "        if self.grid[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # Î¨∏ ÌôïÏù∏ (Ïó¥Ïá† ÌïÑÏöî)\n",
    "        if pos in self.doors:\n",
    "            # Ìï¥Îãπ Î¨∏Ïóê ÎåÄÌïú Ïó¥Ïá†Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "            required_key = self.keys[self.doors.index(pos)] if self.doors else None\n",
    "            if required_key and required_key not in self.has_keys:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        display = []\n",
    "        for y in range(self.height):\n",
    "            row = []\n",
    "            for x in range(self.width):\n",
    "                if (y, x) == self.pos:\n",
    "                    row.append('A')\n",
    "                elif (y, x) in self.keys and (y, x) not in self.has_keys:\n",
    "                    row.append('K')\n",
    "                elif (y, x) in self.doors:\n",
    "                    if self.keys and self.keys[self.doors.index((y,x))] in self.has_keys:\n",
    "                        row.append('.')\n",
    "                    else:\n",
    "                        row.append('D')\n",
    "                elif (y, x) == self.goal:\n",
    "                    row.append('G')\n",
    "                else:\n",
    "                    row.append(self.grid[y][x])\n",
    "            display.append(' '.join(row))\n",
    "        \n",
    "        print('\\n'.join(display))\n",
    "        if self.has_keys:\n",
    "            print(f\"üîë Î≥¥Ïú† Ïó¥Ïá†: {self.has_keys}\")\n",
    "        print(f\"Steps: {self.steps}, Thinks: {self.think_count}\")\n",
    "\n",
    "# ÌôòÍ≤Ω ÌÖåÏä§Ìä∏\n",
    "env = ComplexMaze('complex')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ï†ÑÎ∞òÏ†Ñ: ÏàúÏàò Q-Learning\n",
    "\n",
    "Î®ºÏ†Ä Ï†ÑÌÜµÏ†ÅÏù∏ Q-learningÏúºÎ°ú Î¨∏Ï†úÎ•º Ìï¥Í≤∞Ìï¥Î¥ÖÏãúÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Ï†ÑÌÜµÏ†ÅÏù∏ Q-Learning ÏóêÏù¥Ï†ÑÌä∏\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions  # THINK Ï†úÏô∏\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return Action(random.randint(0, self.n_actions-1))\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            return Action(np.argmax(q_values))\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        td_error = target - self.Q[state][action.value]\n",
    "        self.Q[state][action.value] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "\n",
    "# Q-Learning ÌïôÏäµ\n",
    "def train_qlearning(n_episodes=200):\n",
    "    env = ComplexMaze('complex')\n",
    "    agent = QLearningAgent(n_actions=4)  # THINK ÏóÜÏùå\n",
    "    \n",
    "    successes = 0\n",
    "    \n",
    "    print(\"Ï†ÑÎ∞òÏ†Ñ: ÏàúÏàò Q-Learning (ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Ïôï)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward = agent.train_episode(env)\n",
    "        \n",
    "        if env.pos == env.goal:\n",
    "            successes += 1\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            success_rate = successes / (episode + 1) * 100\n",
    "            avg_reward = np.mean(agent.episode_rewards[-50:])\n",
    "            print(f\"Episode {episode+1}: ÏÑ±Í≥µÎ•†={success_rate:.1f}%, ÌèâÍ∑†Î≥¥ÏÉÅ={avg_reward:.2f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "q_agent = train_qlearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ÌõÑÎ∞òÏ†Ñ: ReAct ÏóêÏù¥Ï†ÑÌä∏\n",
    "\n",
    "Ïù¥Ï†ú **THINKÎ•º ÌñâÎèôÏúºÎ°ú** ÎèÑÏûÖÌïú ReAct ÏóêÏù¥Ï†ÑÌä∏Î•º Íµ¨ÌòÑÌï©ÎãàÎã§.\n",
    "\n",
    "### ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥\n",
    "- THINK ÌñâÎèô: ÎÇ¥Î∂Ä Í≥ÑÌöç ÏàòÎ¶Ω\n",
    "- BFSÎ°ú ÏµúÏ†Å Í≤ΩÎ°ú Í≥ÑÏÇ∞\n",
    "- Í≥ÑÌöçÏóê Îî∞Îùº ÌñâÎèô Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    ReAct (Reasoning + Acting) ÏóêÏù¥Ï†ÑÌä∏\n",
    "    THINKÎ•º Î™ÖÏãúÏ†Å ÌñâÎèôÏúºÎ°ú ÏÇ¨Ïö©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_memory=True):\n",
    "        self.plan = []  # ÌòÑÏû¨ Í≥ÑÌöç\n",
    "        self.use_memory = use_memory\n",
    "        self.memory = {}  # ÌïôÏäµÎêú Í≥ÑÌöç Ï†ÄÏû•\n",
    "        self.think_history = []  # Ï∂îÎ°† Í∏∞Î°ù\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def think(self, env: ComplexMaze) -> List[Action]:\n",
    "        \"\"\"\n",
    "        THINK ÌñâÎèô: BFSÎ°ú ÏµúÏ†Å Í≤ΩÎ°ú Í≥ÑÌöç\n",
    "        \"\"\"\n",
    "        # ÌòÑÏû¨ ÏÉÅÌÉú\n",
    "        start_pos = env.pos\n",
    "        has_keys = env.has_keys.copy()\n",
    "        \n",
    "        # Ï∂îÎ°† Í∏∞Î°ù\n",
    "        thought = {\n",
    "            'position': start_pos,\n",
    "            'keys': has_keys,\n",
    "            'goal': env.goal\n",
    "        }\n",
    "        \n",
    "        # Î©îÎ™®Î¶¨ÏóêÏÑú Í≥ÑÌöç Ï∞æÍ∏∞\n",
    "        if self.use_memory:\n",
    "            memory_key = (start_pos, tuple(sorted(has_keys)))\n",
    "            if memory_key in self.memory:\n",
    "                thought['from_memory'] = True\n",
    "                self.think_history.append(thought)\n",
    "                return self.memory[memory_key].copy()\n",
    "        \n",
    "        # BFSÎ°ú Í≤ΩÎ°ú ÌÉêÏÉâ\n",
    "        plan = self._bfs_planning(env, start_pos, has_keys)\n",
    "        \n",
    "        # Î©îÎ™®Î¶¨Ïóê Ï†ÄÏû•\n",
    "        if self.use_memory and plan:\n",
    "            memory_key = (start_pos, tuple(sorted(has_keys)))\n",
    "            self.memory[memory_key] = plan.copy()\n",
    "        \n",
    "        thought['plan_length'] = len(plan) if plan else 0\n",
    "        thought['from_memory'] = False\n",
    "        self.think_history.append(thought)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _bfs_planning(self, env: ComplexMaze, start_pos, has_keys):\n",
    "        \"\"\"\n",
    "        BFSÎ°ú ÏµúÏ†Å Í≤ΩÎ°ú Í≥ÑÌöç\n",
    "        1. Ïó¥Ïá†Í∞Ä ÌïÑÏöîÌïòÎ©¥ Î®ºÏ†Ä Ïó¥Ïá†Î°ú\n",
    "        2. Í∑∏ Îã§Ïùå Î™©ÌëúÎ°ú\n",
    "        \"\"\"\n",
    "        plan = []\n",
    "        current_pos = start_pos\n",
    "        current_keys = has_keys.copy()\n",
    "        \n",
    "        # Step 1: ÌïÑÏöîÌïú Ïó¥Ïá† ÌöçÎìù\n",
    "        for key_pos in env.keys:\n",
    "            if key_pos not in current_keys:\n",
    "                path = self._bfs_path(env, current_pos, key_pos, current_keys)\n",
    "                if path:\n",
    "                    plan.extend(path)\n",
    "                    current_pos = key_pos\n",
    "                    current_keys.add(key_pos)\n",
    "        \n",
    "        # Step 2: Î™©ÌëúÎ°ú Ïù¥Îèô\n",
    "        path = self._bfs_path(env, current_pos, env.goal, current_keys)\n",
    "        if path:\n",
    "            plan.extend(path)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _bfs_path(self, env, start, goal, has_keys):\n",
    "        \"\"\"BFSÎ°ú startÏóêÏÑú goalÍπåÏßÄ Í≤ΩÎ°ú Ï∞æÍ∏∞\"\"\"\n",
    "        from collections import deque\n",
    "        \n",
    "        queue = deque([(start, [])])\n",
    "        visited = set([start])\n",
    "        \n",
    "        while queue:\n",
    "            pos, path = queue.popleft()\n",
    "            \n",
    "            if pos == goal:\n",
    "                return path\n",
    "            \n",
    "            for action in [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]:\n",
    "                new_pos = env._get_new_pos(pos, action)\n",
    "                \n",
    "                if new_pos not in visited:\n",
    "                    # Ïù¥Îèô Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨\n",
    "                    if self._can_move(env, new_pos, has_keys):\n",
    "                        visited.add(new_pos)\n",
    "                        queue.append((new_pos, path + [action]))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _can_move(self, env, pos, has_keys):\n",
    "        \"\"\"Ïù¥Îèô Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\"\"\"\n",
    "        y, x = pos\n",
    "        \n",
    "        if not (0 <= y < env.height and 0 <= x < env.width):\n",
    "            return False\n",
    "        \n",
    "        if env.grid[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # Î¨∏ Ï≤¥ÌÅ¨\n",
    "        if pos in env.doors:\n",
    "            key_idx = env.doors.index(pos)\n",
    "            if key_idx < len(env.keys):\n",
    "                required_key = env.keys[key_idx]\n",
    "                if required_key not in has_keys:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def act(self, env: ComplexMaze) -> Action:\n",
    "        \"\"\"\n",
    "        ÌñâÎèô ÏÑ†ÌÉù: Í≥ÑÌöçÏù¥ ÏóÜÏúºÎ©¥ THINK, ÏûàÏúºÎ©¥ Í≥ÑÌöç Ïã§Ìñâ\n",
    "        \"\"\"\n",
    "        if not self.plan:\n",
    "            return Action.THINK\n",
    "        \n",
    "        return self.plan.pop(0)\n",
    "    \n",
    "    def episode(self, env: ComplexMaze, verbose=False):\n",
    "        \"\"\"Ìïú ÏóêÌîºÏÜåÎìú Ïã§Ìñâ\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        actions_taken = []\n",
    "        \n",
    "        while True:\n",
    "            action = self.act(env)\n",
    "            \n",
    "            if action == Action.THINK:\n",
    "                # THINK Ïã§Ìñâ: Í≥ÑÌöç ÏàòÎ¶Ω\n",
    "                self.plan = self.think(env)\n",
    "                if verbose:\n",
    "                    print(f\"ü§î THINK: Í≥ÑÌöç ÏàòÎ¶Ω (Í∏∏Ïù¥: {len(self.plan)})\")\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            actions_taken.append(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if verbose and action != Action.THINK:\n",
    "                print(f\"ÌñâÎèô: {action.name}, Î≥¥ÏÉÅ: {reward:.3f}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # Ïó¥Ïá† ÌöçÎìù Ïãú Ïû¨Í≥ÑÌöç\n",
    "            if len(env.has_keys) > len(state[2]):\n",
    "                self.plan = []  # Í≥ÑÌöç Ï¥àÍ∏∞ÌôîÌïòÏó¨ Îã§Ïãú THINK\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        \n",
    "        return total_reward, actions_taken\n",
    "\n",
    "# ReAct ÏóêÏù¥Ï†ÑÌä∏ ÌÖåÏä§Ìä∏\n",
    "def test_react(n_episodes=10, verbose=True):\n",
    "    env = ComplexMaze('complex')\n",
    "    agent = ReActAgent(use_memory=True)\n",
    "    \n",
    "    print(\"\\nÌõÑÎ∞òÏ†Ñ: ReAct ÏóêÏù¥Ï†ÑÌä∏ (Ï∂îÎ°†Ïù¥ Ïôï)\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"THINKÎ•º Î™ÖÏãúÏ†Å ÌñâÎèôÏúºÎ°ú ÏÇ¨Ïö©\\n\")\n",
    "    \n",
    "    successes = 0\n",
    "    total_steps = []\n",
    "    think_counts = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        reward, actions = agent.episode(env, verbose=(episode == 0 and verbose))\n",
    "        \n",
    "        if env.pos == env.goal:\n",
    "            successes += 1\n",
    "        \n",
    "        total_steps.append(len(actions))\n",
    "        think_counts.append(sum(1 for a in actions if a == Action.THINK))\n",
    "        \n",
    "        if episode == 0 and verbose:\n",
    "            print(f\"\\nÏ≤´ ÏóêÌîºÏÜåÎìú Í≤∞Í≥º:\")\n",
    "            print(f\"  Ï¥ù Ïä§ÌÖù: {len(actions)}\")\n",
    "            print(f\"  THINK ÌöüÏàò: {think_counts[-1]}\")\n",
    "            print(f\"  Ï¥ù Î≥¥ÏÉÅ: {reward:.2f}\")\n",
    "            print(f\"  ÏÑ±Í≥µ: {env.pos == env.goal}\")\n",
    "    \n",
    "    print(f\"\\nÏ†ÑÏ≤¥ Í≤∞Í≥º ({n_episodes} ÏóêÌîºÏÜåÎìú):\")\n",
    "    print(f\"  ÏÑ±Í≥µÎ•†: {successes/n_episodes*100:.1f}%\")\n",
    "    print(f\"  ÌèâÍ∑† Ïä§ÌÖù: {np.mean(total_steps):.1f}\")\n",
    "    print(f\"  ÌèâÍ∑† THINK: {np.mean(think_counts):.1f}\")\n",
    "    print(f\"  ÌèâÍ∑† Î≥¥ÏÉÅ: {np.mean(agent.episode_rewards):.2f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "react_agent = test_react(n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ÏÑ±Îä• ÎπÑÍµê: Q-Learning vs ReAct\n",
    "\n",
    "Îëê Ï†ëÍ∑ºÎ≤ïÏùò ÏÑ±Îä•ÏùÑ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú ÎπÑÍµêÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(n_episodes=100):\n",
    "    \"\"\"Q-Learning vs ReAct ÎπÑÍµê\"\"\"\n",
    "    \n",
    "    print(\"\\nüî• Ï†ÑÎ∞òÏ†Ñ vs ÌõÑÎ∞òÏ†Ñ ÎπÑÍµê\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Q-Learning (Ï†ÑÎ∞òÏ†Ñ)\n",
    "    env_q = ComplexMaze('complex')\n",
    "    q_agent = QLearningAgent(n_actions=4, epsilon=0.1)\n",
    "    q_successes = 0\n",
    "    q_steps = []\n",
    "    \n",
    "    print(\"Q-Learning ÌïôÏäµ Ï§ë...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        state = env_q.reset()\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < env_q.max_steps:\n",
    "            action = q_agent.get_action(state)\n",
    "            next_state, reward, done, _ = env_q.step(action)\n",
    "            q_agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if env_q.pos == env_q.goal:\n",
    "                    q_successes += 1\n",
    "                break\n",
    "        \n",
    "        q_steps.append(steps)\n",
    "        q_agent.episode_rewards.append(reward)\n",
    "    \n",
    "    # ReAct (ÌõÑÎ∞òÏ†Ñ)\n",
    "    env_react = ComplexMaze('complex')\n",
    "    react_agent = ReActAgent(use_memory=True)\n",
    "    react_successes = 0\n",
    "    react_steps = []\n",
    "    react_thinks = []\n",
    "    \n",
    "    print(\"\\nReAct Ïã§Ìñâ Ï§ë...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, actions = react_agent.episode(env_react, verbose=False)\n",
    "        \n",
    "        if env_react.pos == env_react.goal:\n",
    "            react_successes += 1\n",
    "        \n",
    "        react_steps.append(len(actions))\n",
    "        react_thinks.append(sum(1 for a in actions if a == Action.THINK))\n",
    "    \n",
    "    # Í≤∞Í≥º ÎπÑÍµê\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä ÏµúÏ¢Ö ÎπÑÍµê Í≤∞Í≥º\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {\n",
    "        'Metric': ['ÏÑ±Í≥µÎ•† (%)', 'ÌèâÍ∑† Ïä§ÌÖù', 'Ï≤´ ÏÑ±Í≥µ ÏóêÌîºÏÜåÎìú', 'ÌèâÍ∑† Î≥¥ÏÉÅ'],\n",
    "        'Q-Learning': [\n",
    "            f\"{q_successes/n_episodes*100:.1f}\",\n",
    "            f\"{np.mean(q_steps):.1f}\",\n",
    "            str(next((i for i, r in enumerate(q_agent.episode_rewards) if r > 5), 'N/A')),\n",
    "            f\"{np.mean(q_agent.episode_rewards):.2f}\"\n",
    "        ],\n",
    "        'ReAct': [\n",
    "            f\"{react_successes/n_episodes*100:.1f}\",\n",
    "            f\"{np.mean(react_steps):.1f}\",\n",
    "            '1',  # ReActÎäî Î≥¥ÌÜµ Ï≤´ ÏóêÌîºÏÜåÎìúÎ∂ÄÌÑ∞ ÏÑ±Í≥µ\n",
    "            f\"{np.mean(react_agent.episode_rewards):.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nReAct ÌèâÍ∑† THINK ÌöüÏàò: {np.mean(react_thinks):.1f}\")\n",
    "    \n",
    "    # ÏãúÍ∞ÅÌôî\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # ÏÑ±Í≥µÎ•† Ï∂îÏù¥\n",
    "    ax = axes[0, 0]\n",
    "    q_success_rate = np.cumsum([1 if r > 5 else 0 for r in q_agent.episode_rewards]) / np.arange(1, n_episodes+1)\n",
    "    react_success_rate = np.cumsum([1 if r > 5 else 0 for r in react_agent.episode_rewards]) / np.arange(1, n_episodes+1)\n",
    "    \n",
    "    ax.plot(q_success_rate, label='Q-Learning', linewidth=2)\n",
    "    ax.plot(react_success_rate, label='ReAct', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('ÏÑ±Í≥µÎ•† Ï∂îÏù¥')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ïä§ÌÖù Ïàò ÎπÑÍµê\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(q_steps, label='Q-Learning', alpha=0.6)\n",
    "    ax.plot(react_steps, label='ReAct', alpha=0.6)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('ÏóêÌîºÏÜåÎìúÎãπ Ïä§ÌÖù Ïàò')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Î≥¥ÏÉÅ Ï∂îÏù¥\n",
    "    ax = axes[1, 0]\n",
    "    window = 10\n",
    "    q_rewards_smooth = np.convolve(q_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    react_rewards_smooth = np.convolve(react_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(q_rewards_smooth, label='Q-Learning', linewidth=2)\n",
    "    ax.plot(react_rewards_smooth, label='ReAct', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Reward')\n",
    "    ax.set_title('Î≥¥ÏÉÅ Ï∂îÏù¥ (Ïù¥ÎèôÌèâÍ∑†)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # THINK ÌöüÏàò (ReActÎßå)\n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(range(len(react_thinks[:20])), react_thinks[:20], color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('THINK Count')\n",
    "    ax.set_title('ReAct: THINK ÌñâÎèô ÌöüÏàò (Ï≤òÏùå 20 ÏóêÌîºÏÜåÎìú)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏:\")\n",
    "    print(\"1. ReActÎäî Ï≤´ ÏóêÌîºÏÜåÎìúÎ∂ÄÌÑ∞ ÏÑ±Í≥µ (Ï∂îÎ°† ÌôúÏö©)\")\n",
    "    print(\"2. Q-LearningÏùÄ ÎßéÏùÄ ÏóêÌîºÏÜåÎìú ÌïÑÏöî (ÏãúÌñâÏ∞©Ïò§)\")\n",
    "    print(\"3. THINK ÌñâÎèôÏúºÎ°ú Í≥ÑÌöç ÏàòÎ¶Ω ‚Üí Ìö®Ïú®Ï†Å Ïã§Ìñâ\")\n",
    "    print(\"4. Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÏúºÎ°ú Î∞òÎ≥µ Ïãú ÎçîÏö± Îπ†Î¶Ñ\")\n",
    "\n",
    "compare_approaches(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MCTS (Monte Carlo Tree Search)\n",
    "\n",
    "Îçî Î≥µÏû°Ìïú Í≥ÑÌöçÏùÑ ÏúÑÌïú MCTSÎ•º Íµ¨ÌòÑÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    \"\"\"MCTS ÎÖ∏Îìú\"\"\"\n",
    "    \n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        self.untried_actions = [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.untried_actions) == 0\n",
    "    \n",
    "    def best_child(self, c_param=1.4):\n",
    "        \"\"\"UCB1 Í∏∞Î∞ò ÏµúÏÑ†Ïùò ÏûêÏãù ÏÑ†ÌÉù\"\"\"\n",
    "        choices_weights = [\n",
    "            (child.value / child.visits) + c_param * math.sqrt(2 * math.log(self.visits) / child.visits)\n",
    "            for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    def expand(self, action, next_state):\n",
    "        \"\"\"ÏÉà ÏûêÏãù ÎÖ∏Îìú Ï∂îÍ∞Ä\"\"\"\n",
    "        child = MCTSNode(next_state, parent=self, action=action)\n",
    "        self.untried_actions.remove(action)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def update(self, value):\n",
    "        \"\"\"ÎÖ∏Îìú Í∞í ÏóÖÎç∞Ïù¥Ìä∏\"\"\"\n",
    "        self.visits += 1\n",
    "        self.value += value\n",
    "\n",
    "class MCTSAgent:\n",
    "    \"\"\"MCTS Í∏∞Î∞ò Í≥ÑÌöç ÏóêÏù¥Ï†ÑÌä∏\"\"\"\n",
    "    \n",
    "    def __init__(self, n_simulations=100):\n",
    "        self.n_simulations = n_simulations\n",
    "        self.think_time = 0\n",
    "    \n",
    "    def think(self, env: ComplexMaze, state):\n",
    "        \"\"\"MCTSÎ°ú ÏµúÏÑ†Ïùò ÌñâÎèô Í≥ÑÌöç\"\"\"\n",
    "        root = MCTSNode(state)\n",
    "        \n",
    "        for _ in range(self.n_simulations):\n",
    "            node = root\n",
    "            temp_env = self._copy_env(env)\n",
    "            \n",
    "            # Selection\n",
    "            while node.is_fully_expanded() and len(node.children) > 0:\n",
    "                node = node.best_child()\n",
    "                temp_env.step(node.action)\n",
    "            \n",
    "            # Expansion\n",
    "            if not node.is_fully_expanded():\n",
    "                action = random.choice(node.untried_actions)\n",
    "                next_state, _, _, _ = temp_env.step(action)\n",
    "                node = node.expand(action, next_state)\n",
    "            \n",
    "            # Simulation\n",
    "            value = self._simulate(temp_env)\n",
    "            \n",
    "            # Backpropagation\n",
    "            while node is not None:\n",
    "                node.update(value)\n",
    "                node = node.parent\n",
    "        \n",
    "        # ÏµúÏÑ†Ïùò ÌñâÎèô ÏÑ†ÌÉù\n",
    "        best_action = root.best_child(c_param=0).action\n",
    "        self.think_time += 1\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _copy_env(self, env):\n",
    "        \"\"\"ÌôòÍ≤Ω Î≥µÏÇ¨ (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)\"\"\"\n",
    "        import copy\n",
    "        return copy.deepcopy(env)\n",
    "    \n",
    "    def _simulate(self, env):\n",
    "        \"\"\"ÎûúÎç§ ÏãúÎÆ¨Î†àÏù¥ÏÖò\"\"\"\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        max_steps = 50\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT])\n",
    "            _, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "# MCTS ÌÖåÏä§Ìä∏\n",
    "print(\"\\nüå≥ MCTS (Monte Carlo Tree Search)\")\n",
    "print(\"=\"*50)\n",
    "print(\"Îçî Î≥µÏû°Ìïú Í≥ÑÌöçÏùÑ ÏúÑÌïú Ìä∏Î¶¨ ÌÉêÏÉâ\\n\")\n",
    "\n",
    "env = ComplexMaze('simple')  # Í∞ÑÎã®Ìïú ÎØ∏Î°úÎ°ú ÌÖåÏä§Ìä∏\n",
    "mcts_agent = MCTSAgent(n_simulations=50)\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"MCTS Ïã§Ìñâ:\")\n",
    "while steps < 30:\n",
    "    # THINK: MCTSÎ°ú Í≥ÑÌöç\n",
    "    action = mcts_agent.think(env, state)\n",
    "    \n",
    "    # ACT: ÌñâÎèô Ïã§Ìñâ\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if steps <= 10:\n",
    "        print(f\"Step {steps}: {action.name} ‚Üí Î≥¥ÏÉÅ: {reward:.3f}\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nÏôÑÎ£å! Ï¥ù Ïä§ÌÖù: {steps}, Ï¥ù Î≥¥ÏÉÅ: {total_reward:.2f}\")\n",
    "        print(f\"THINK ÌöüÏàò: {mcts_agent.think_time}\")\n",
    "        break\n",
    "    \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chain-of-Thought in RL\n",
    "\n",
    "LLMÏùò Chain-of-ThoughtÎ•º RLÏóê Ï†ÅÏö©Ìïú ÏòàÏ†úÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThoughtAgent:\n",
    "    \"\"\"\n",
    "    Chain-of-Thought Ï∂îÎ°†ÏùÑ ÏÇ¨Ïö©ÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏\n",
    "    Í∞Å Îã®Í≥ÑÎßàÎã§ Î™ÖÏãúÏ†ÅÏù∏ Ï∂îÎ°† Í≥ºÏ†ïÏùÑ Í±∞Ïπ®\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.thoughts = []  # Ï∂îÎ°† Ï≤¥Ïù∏\n",
    "        self.plan = []\n",
    "    \n",
    "    def think_step_by_step(self, env: ComplexMaze):\n",
    "        \"\"\"\n",
    "        Îã®Í≥ÑÎ≥Ñ Ï∂îÎ°† (Chain-of-Thought)\n",
    "        \"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        # Step 1: ÌòÑÏû¨ ÏÉÅÌô© ÌååÏïÖ\n",
    "        thought1 = f\"ÌòÑÏû¨ ÏúÑÏπò: {env.pos}, Î™©Ìëú: {env.goal}\"\n",
    "        thoughts.append(thought1)\n",
    "        \n",
    "        # Step 2: ÌïÑÏöîÌïú Í≤É ÌååÏïÖ\n",
    "        needs_key = False\n",
    "        for door in env.doors:\n",
    "            key_idx = env.doors.index(door)\n",
    "            if key_idx < len(env.keys):\n",
    "                key = env.keys[key_idx]\n",
    "                if key not in env.has_keys:\n",
    "                    needs_key = True\n",
    "                    thought2 = f\"Î¨∏({door})ÏùÑ ÌÜµÍ≥ºÌïòÎ†§Î©¥ Ïó¥Ïá†({key})Í∞Ä ÌïÑÏöîÌï®\"\n",
    "                    thoughts.append(thought2)\n",
    "                    break\n",
    "        \n",
    "        if not needs_key:\n",
    "            thought2 = \"Î™®Îì† ÌïÑÏöîÌïú Ïó¥Ïá†Î•º Î≥¥Ïú† Ï§ë\"\n",
    "            thoughts.append(thought2)\n",
    "        \n",
    "        # Step 3: Ï†ÑÎûµ ÏàòÎ¶Ω\n",
    "        if needs_key:\n",
    "            thought3 = \"Ï†ÑÎûµ: Ïó¥Ïá† ÌöçÎìù ‚Üí Î¨∏ ÌÜµÍ≥º ‚Üí Î™©Ìëú ÎèÑÎã¨\"\n",
    "        else:\n",
    "            thought3 = \"Ï†ÑÎûµ: ÏßÅÏ†ë Î™©ÌëúÎ°ú Ïù¥Îèô\"\n",
    "        thoughts.append(thought3)\n",
    "        \n",
    "        # Step 4: Íµ¨Ï≤¥Ï†Å Í≥ÑÌöç\n",
    "        if needs_key:\n",
    "            thought4 = \"Í≥ÑÌöç: \"\n",
    "            # Ïó¥Ïá†ÍπåÏßÄ Í±∞Î¶¨\n",
    "            key_dist = abs(env.pos[0] - env.keys[0][0]) + abs(env.pos[1] - env.keys[0][1])\n",
    "            thought4 += f\"Ïó¥Ïá†ÍπåÏßÄ ÏïΩ {key_dist}Ïä§ÌÖù, \"\n",
    "            # Î™©ÌëúÍπåÏßÄ Í±∞Î¶¨\n",
    "            goal_dist = abs(env.keys[0][0] - env.goal[0]) + abs(env.keys[0][1] - env.goal[1])\n",
    "            thought4 += f\"Í∑∏ ÌõÑ Î™©ÌëúÍπåÏßÄ ÏïΩ {goal_dist}Ïä§ÌÖù\"\n",
    "        else:\n",
    "            goal_dist = abs(env.pos[0] - env.goal[0]) + abs(env.pos[1] - env.goal[1])\n",
    "            thought4 = f\"Í≥ÑÌöç: Î™©ÌëúÍπåÏßÄ ÏïΩ {goal_dist}Ïä§ÌÖù\"\n",
    "        thoughts.append(thought4)\n",
    "        \n",
    "        self.thoughts.extend(thoughts)\n",
    "        return thoughts\n",
    "    \n",
    "    def reason_and_act(self, env: ComplexMaze, verbose=True):\n",
    "        \"\"\"Ï∂îÎ°†ÌïòÍ≥† ÌñâÎèô\"\"\"\n",
    "        # Chain-of-Thought Ï∂îÎ°†\n",
    "        thoughts = self.think_step_by_step(env)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nüß† Chain-of-Thought:\")\n",
    "            for i, thought in enumerate(thoughts, 1):\n",
    "                print(f\"  {i}. {thought}\")\n",
    "        \n",
    "        # Ï∂îÎ°† Í∏∞Î∞ò Í≥ÑÌöç ÏàòÎ¶Ω (ReActÏôÄ Ïú†ÏÇ¨ÌïòÏßÄÎßå Îçî Î™ÖÏãúÏ†Å)\n",
    "        react_agent = ReActAgent(use_memory=False)\n",
    "        self.plan = react_agent.think(env)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚Üí ÏÉùÏÑ±Îêú Í≥ÑÌöç: {len(self.plan)}Í∞ú ÌñâÎèô\")\n",
    "        \n",
    "        return self.plan\n",
    "\n",
    "# Chain-of-Thought Îç∞Î™®\n",
    "print(\"\\nüß† Chain-of-Thought in RL\")\n",
    "print(\"=\"*50)\n",
    "print(\"Î™ÖÏãúÏ†Å Ï∂îÎ°† Ï≤¥Ïù∏ÏùÑ ÌÜµÌïú Î¨∏Ï†ú Ìï¥Í≤∞\\n\")\n",
    "\n",
    "env = ComplexMaze('complex')\n",
    "cot_agent = ChainOfThoughtAgent()\n",
    "\n",
    "state = env.reset()\n",
    "print(\"Ï¥àÍ∏∞ ÏÉÅÌÉú:\")\n",
    "env.render()\n",
    "\n",
    "# Ï≤´ Î≤àÏß∏ Ï∂îÎ°†\n",
    "plan = cot_agent.reason_and_act(env, verbose=True)\n",
    "\n",
    "# Í≥ÑÌöç Ïã§Ìñâ\n",
    "print(\"\\nÍ≥ÑÌöç Ïã§Ìñâ:\")\n",
    "total_reward = 0\n",
    "for i, action in enumerate(plan[:10]):  # Ï≤òÏùå 10Í∞ú ÌñâÎèôÎßå ÌëúÏãú\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"  {i+1}. {action.name} ‚Üí Î≥¥ÏÉÅ: {reward:.3f}\")\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"\\nÍ≤∞Í≥º: Ï¥ù Î≥¥ÏÉÅ = {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test-time ComputeÏôÄ Ï∂îÎ°† Ïä§ÏºÄÏùºÎßÅ\n",
    "\n",
    "Ï∂îÎ°† ÏãúÏ†êÏóê Îçî ÎßéÏùÄ Í≥ÑÏÇ∞ÏùÑ Ìà¨ÏûÖÌïòÏó¨ ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Î∞©Î≤ïÏùÑ ÌÉêÍµ¨Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_compute_analysis():\n",
    "    \"\"\"\n",
    "    Test-time computeÏùò Ìö®Í≥º Î∂ÑÏÑù\n",
    "    Îçî ÎßéÏùÄ Ï∂îÎ°† = Îçî ÎÇòÏùÄ ÏÑ±Îä•?\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n‚ö° Test-time Compute Î∂ÑÏÑù\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Ï∂îÎ°† ÏãúÍ∞ÑÏùÑ ÎäòÎ¶¨Î©¥ ÏÑ±Îä•Ïù¥ Ìñ•ÏÉÅÎêòÎäîÍ∞Ä?\\n\")\n",
    "    \n",
    "    env = ComplexMaze('complex')\n",
    "    \n",
    "    # Îã§ÏñëÌïú Ï∂îÎ°† ÍπäÏù¥\n",
    "    think_depths = [0, 1, 5, 10, 20]\n",
    "    results = []\n",
    "    \n",
    "    for depth in think_depths:\n",
    "        env.reset()\n",
    "        \n",
    "        if depth == 0:\n",
    "            # Ï∂îÎ°† ÏóÜÏù¥ ÎûúÎç§\n",
    "            steps = 0\n",
    "            while steps < 100:\n",
    "                action = random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT])\n",
    "                _, _, done, _ = env.step(action)\n",
    "                steps += 1\n",
    "                if done:\n",
    "                    break\n",
    "            success = env.pos == env.goal\n",
    "            \n",
    "        else:\n",
    "            # MCTS with different simulation depths\n",
    "            mcts = MCTSAgent(n_simulations=depth * 10)\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 100:\n",
    "                state = env._get_state()\n",
    "                action = mcts.think(env, state)\n",
    "                _, _, done, _ = env.step(action)\n",
    "                steps += 1\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            success = env.pos == env.goal\n",
    "        \n",
    "        results.append({\n",
    "            'depth': depth,\n",
    "            'success': success,\n",
    "            'steps': steps,\n",
    "            'compute': depth * 10  # Ï∂îÎ°† Í≥ÑÏÇ∞Îüâ\n",
    "        })\n",
    "        \n",
    "        print(f\"Ï∂îÎ°† ÍπäÏù¥ {depth:2d}: ÏÑ±Í≥µ={success}, Ïä§ÌÖù={steps:3d}\")\n",
    "    \n",
    "    # ÏãúÍ∞ÅÌôî\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ÏÑ±Í≥µÎ•† vs Ï∂îÎ°† ÍπäÏù¥\n",
    "    ax = axes[0]\n",
    "    success_rate = [r['success'] for r in results]\n",
    "    ax.bar(think_depths, success_rate, color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Think Depth')\n",
    "    ax.set_ylabel('Success')\n",
    "    ax.set_title('Ï∂îÎ°† ÍπäÏù¥ÏôÄ ÏÑ±Í≥µÎ•†')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ïä§ÌÖù Ïàò vs Ï∂îÎ°† ÍπäÏù¥\n",
    "    ax = axes[1]\n",
    "    steps = [r['steps'] for r in results]\n",
    "    ax.plot(think_depths, steps, 'o-', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Think Depth')\n",
    "    ax.set_ylabel('Steps to Goal')\n",
    "    ax.set_title('Ï∂îÎ°† ÍπäÏù¥ÏôÄ Ìö®Ïú®ÏÑ±')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Ïù∏ÏÇ¨Ïù¥Ìä∏:\")\n",
    "    print(\"1. Ï∂îÎ°† ÏóÜÏùå (depth=0): ÎûúÎç§ ÌÉêÏÉâ, Ïã§Ìå®\")\n",
    "    print(\"2. ÏñïÏùÄ Ï∂îÎ°† (depth=1-5): Í∏∞Î≥∏Ï†ÅÏù∏ Í≥ÑÌöç\")\n",
    "    print(\"3. ÍπäÏùÄ Ï∂îÎ°† (depth=10+): ÏµúÏ†Å Í≤ΩÎ°ú Î∞úÍ≤¨\")\n",
    "    print(\"4. Test-time compute ‚Üë ‚Üí ÏÑ±Îä• ‚Üë\")\n",
    "\n",
    "test_time_compute_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Î©îÌÉÄ ÌïôÏäµ: Ï∂îÎ°† ÏûêÏ≤¥Î•º ÌïôÏäµ\n",
    "\n",
    "Ï∂îÎ°† Í≥ºÏ†ï ÏûêÏ≤¥Î•º ÌïôÏäµÌïòÎäî Î©îÌÉÄ RL Ï†ëÍ∑ºÎ≤ïÏùÑ ÏÜåÍ∞úÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaReasoningAgent:\n",
    "    \"\"\"\n",
    "    Î©îÌÉÄ Ï∂îÎ°† ÏóêÏù¥Ï†ÑÌä∏\n",
    "    Ïñ¥ÎñªÍ≤å Ï∂îÎ°†Ìï†ÏßÄÎ•º ÌïôÏäµ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reasoning_strategies = [\n",
    "            'direct',      # ÏßÅÏ†ë Î™©ÌëúÎ°ú\n",
    "            'explore',     # ÌÉêÏÉâ Ïö∞ÏÑ†\n",
    "            'systematic',  # Ï≤¥Í≥ÑÏ†Å ÌÉêÏÉâ\n",
    "            'planning'     # Í≥ÑÌöç Ïö∞ÏÑ†\n",
    "        ]\n",
    "        self.strategy_scores = defaultdict(float)\n",
    "        self.current_strategy = None\n",
    "    \n",
    "    def select_strategy(self, env_features):\n",
    "        \"\"\"\n",
    "        ÌôòÍ≤Ω ÌäπÏÑ±Ïóê Îî∞Îùº Ï∂îÎ°† Ï†ÑÎûµ ÏÑ†ÌÉù\n",
    "        \"\"\"\n",
    "        # ÌôòÍ≤Ω Î≥µÏû°ÎèÑ Ï∂îÏ†ï\n",
    "        has_obstacles = '#' in str(env_features.get('grid', ''))\n",
    "        has_keys = len(env_features.get('keys', [])) > 0\n",
    "        maze_size = env_features.get('size', 0)\n",
    "        \n",
    "        if not has_keys and maze_size < 25:\n",
    "            strategy = 'direct'\n",
    "        elif has_keys:\n",
    "            strategy = 'planning'\n",
    "        elif has_obstacles:\n",
    "            strategy = 'systematic'\n",
    "        else:\n",
    "            strategy = 'explore'\n",
    "        \n",
    "        # ÌïôÏäµÎêú Ï†êÏàò Î∞òÏòÅ\n",
    "        if self.strategy_scores:\n",
    "            best_strategy = max(self.strategy_scores.items(), key=lambda x: x[1])\n",
    "            if best_strategy[1] > 0.5:  # Ï∂©Î∂ÑÌûà Ï¢ãÏùÄ Ï†ÑÎûµÏù¥ ÏûàÏúºÎ©¥\n",
    "                strategy = best_strategy[0]\n",
    "        \n",
    "        self.current_strategy = strategy\n",
    "        return strategy\n",
    "    \n",
    "    def execute_strategy(self, strategy, env):\n",
    "        \"\"\"\n",
    "        ÏÑ†ÌÉùÎêú Ï†ÑÎûµ Ïã§Ìñâ\n",
    "        \"\"\"\n",
    "        if strategy == 'direct':\n",
    "            # ÏßÅÏ†ë Í≤ΩÎ°ú\n",
    "            return self._direct_path(env)\n",
    "        elif strategy == 'explore':\n",
    "            # ÌÉêÏÉâ Ïö∞ÏÑ†\n",
    "            return self._explore_first(env)\n",
    "        elif strategy == 'systematic':\n",
    "            # Ï≤¥Í≥ÑÏ†Å ÌÉêÏÉâ\n",
    "            return self._systematic_search(env)\n",
    "        elif strategy == 'planning':\n",
    "            # Í≥ÑÌöç Í∏∞Î∞ò\n",
    "            react = ReActAgent()\n",
    "            return react.think(env)\n",
    "    \n",
    "    def _direct_path(self, env):\n",
    "        \"\"\"Î™©ÌëúÍπåÏßÄ ÏßÅÏ†ë Í≤ΩÎ°ú\"\"\"\n",
    "        path = []\n",
    "        current = env.pos\n",
    "        goal = env.goal\n",
    "        \n",
    "        while current != goal:\n",
    "            if current[0] < goal[0]:\n",
    "                path.append(Action.DOWN)\n",
    "                current = (current[0] + 1, current[1])\n",
    "            elif current[0] > goal[0]:\n",
    "                path.append(Action.UP)\n",
    "                current = (current[0] - 1, current[1])\n",
    "            elif current[1] < goal[1]:\n",
    "                path.append(Action.RIGHT)\n",
    "                current = (current[0], current[1] + 1)\n",
    "            else:\n",
    "                path.append(Action.LEFT)\n",
    "                current = (current[0], current[1] - 1)\n",
    "            \n",
    "            if len(path) > 100:  # Î¨¥Ìïú Î£®ÌîÑ Î∞©ÏßÄ\n",
    "                break\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def _explore_first(self, env):\n",
    "        \"\"\"ÌÉêÏÉâ Ïö∞ÏÑ† Ï†ÑÎûµ\"\"\"\n",
    "        # Í∞ÑÎã®Ìïú DFS ÌÉêÏÉâ\n",
    "        return [random.choice([Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]) \n",
    "                for _ in range(20)]\n",
    "    \n",
    "    def _systematic_search(self, env):\n",
    "        \"\"\"Ï≤¥Í≥ÑÏ†Å ÌÉêÏÉâ\"\"\"\n",
    "        # ÎÇòÏÑ†Ìòï ÌÉêÏÉâ Ìå®ÌÑ¥\n",
    "        pattern = [Action.RIGHT, Action.DOWN, Action.LEFT, Action.LEFT, \n",
    "                  Action.UP, Action.UP, Action.RIGHT, Action.RIGHT]\n",
    "        return pattern * 5\n",
    "    \n",
    "    def update_scores(self, strategy, reward):\n",
    "        \"\"\"Ï†ÑÎûµ Ï†êÏàò ÏóÖÎç∞Ïù¥Ìä∏\"\"\"\n",
    "        self.strategy_scores[strategy] = 0.9 * self.strategy_scores[strategy] + 0.1 * reward\n",
    "\n",
    "# Î©îÌÉÄ Ï∂îÎ°† Îç∞Î™®\n",
    "print(\"\\nüéì Î©îÌÉÄ Ï∂îÎ°†: Ï∂îÎ°† Î∞©Î≤ïÏùÑ ÌïôÏäµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "meta_agent = MetaReasoningAgent()\n",
    "\n",
    "# Îã§ÏñëÌïú ÌôòÍ≤ΩÏóêÏÑú ÌÖåÏä§Ìä∏\n",
    "env_types = ['simple', 'complex']\n",
    "\n",
    "for env_type in env_types:\n",
    "    print(f\"\\nÌôòÍ≤Ω: {env_type}\")\n",
    "    env = ComplexMaze(env_type)\n",
    "    \n",
    "    # ÌôòÍ≤Ω ÌäπÏÑ± Ï∂îÏ∂ú\n",
    "    env_features = {\n",
    "        'grid': env.grid,\n",
    "        'keys': env.keys,\n",
    "        'size': env.height * env.width\n",
    "    }\n",
    "    \n",
    "    # Ï†ÑÎûµ ÏÑ†ÌÉù\n",
    "    strategy = meta_agent.select_strategy(env_features)\n",
    "    print(f\"ÏÑ†ÌÉùÎêú Ï†ÑÎûµ: {strategy}\")\n",
    "    \n",
    "    # Ï†ÑÎûµ Ïã§Ìñâ\n",
    "    state = env.reset()\n",
    "    plan = meta_agent.execute_strategy(strategy, env)\n",
    "    \n",
    "    # Ïã§Ìñâ Î∞è ÌèâÍ∞Ä\n",
    "    total_reward = 0\n",
    "    for action in plan[:30]:\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Ï†êÏàò ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    meta_agent.update_scores(strategy, total_reward)\n",
    "    \n",
    "    print(f\"Í≤∞Í≥º: Î≥¥ÏÉÅ={total_reward:.2f}, ÏÑ±Í≥µ={env.pos == env.goal}\")\n",
    "\n",
    "print(\"\\nÌïôÏäµÎêú Ï†ÑÎûµ Ï†êÏàò:\")\n",
    "for strategy, score in meta_agent.strategy_scores.items():\n",
    "    print(f\"  {strategy}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ÏöîÏïΩ: Ìå®Îü¨Îã§ÏûÑ Ï†ÑÌôòÏùò ÏùòÎØ∏\n",
    "\n",
    "### Ï†ÑÎ∞òÏ†Ñ ‚Üí ÌõÑÎ∞òÏ†Ñ Ï†ÑÌôòÏùò ÌïµÏã¨\n",
    "\n",
    "1. **ÏïåÍ≥†Î¶¨Ï¶ò ÏµúÏ†ÅÌôî ‚Üí Ï∂îÎ°† ÌôúÏö©**\n",
    "   - Îçî ÎÇòÏùÄ ÏóÖÎç∞Ïù¥Ìä∏ Í∑úÏπô ‚Üí Îçî ÎÇòÏùÄ Í≥ÑÌöç\n",
    "   - ÎßéÏùÄ Í≤ΩÌóò ÌïÑÏöî ‚Üí Ï†ÅÏùÄ Í≤ΩÌóòÏúºÎ°ú Ìï¥Í≤∞\n",
    "\n",
    "2. **THINK as Action**\n",
    "   - Ï∂îÎ°†ÏùÑ Î™ÖÏãúÏ†Å ÌñâÎèôÏúºÎ°ú\n",
    "   - ÎÇ¥Î∂Ä Í≥ÑÏÇ∞ÎèÑ ÌôòÍ≤ΩÏùò ÏùºÎ∂Ä\n",
    "\n",
    "3. **Test-time Compute**\n",
    "   - ÌïôÏäµ ÏãúÍ∞Ñ ‚Üí Ï∂îÎ°† ÏãúÍ∞Ñ\n",
    "   - Îçî ÎßéÏùÄ Ï∂îÎ°† = Îçî ÎÇòÏùÄ ÏÑ±Îä•\n",
    "\n",
    "4. **ÏÇ¨Ï†ÑÏßÄÏãùÍ≥º Í≥ÑÌöç**\n",
    "   - Tabula rasa ‚Üí Prior knowledge\n",
    "   - ÏãúÌñâÏ∞©Ïò§ ‚Üí Í≥ÑÌöç Í∏∞Î∞ò Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏµúÏ¢Ö Ï†ïÎ¶¨\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ÌïµÏã¨ ÌÜµÏ∞∞: Ï∂îÎ°† Í∏∞Î∞ò RLÏùò ÎØ∏Îûò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "insights = \"\"\"\n",
    "1. Ï†ÑÌÜµÏ†Å RL (Ï†ÑÎ∞òÏ†Ñ)\n",
    "   ‚Ä¢ Q-learning, DQN, PPO\n",
    "   ‚Ä¢ ÎßéÏùÄ Í≤ΩÌóò ÌïÑÏöî\n",
    "   ‚Ä¢ ÏïåÍ≥†Î¶¨Ï¶ò Í∞úÏÑ†Ïóê ÏßëÏ§ë\n",
    "   ‚Ä¢ Î≤§ÏπòÎßàÌÅ¨ Ï†êÏàò Í≤ΩÏüÅ\n",
    "\n",
    "2. Ï∂îÎ°† Í∏∞Î∞ò RL (ÌõÑÎ∞òÏ†Ñ)\n",
    "   ‚Ä¢ ReAct, Chain-of-Thought\n",
    "   ‚Ä¢ Ï†ÅÏùÄ Í≤ΩÌóòÏúºÎ°ú Ìï¥Í≤∞\n",
    "   ‚Ä¢ Ï∂îÎ°†Í≥º Í≥ÑÌöçÏóê ÏßëÏ§ë\n",
    "   ‚Ä¢ Ïã§Ï†ú Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•\n",
    "\n",
    "3. THINK as ActionÏùò ÏùòÎØ∏\n",
    "   ‚Ä¢ Ï∂îÎ°† ÏûêÏ≤¥Í∞Ä ÌñâÎèô\n",
    "   ‚Ä¢ ÎÇ¥Î∂Ä ÏÉÅÌÉú vs Ïô∏Î∂Ä ÏÉÅÌÉú\n",
    "   ‚Ä¢ Î©îÌÉÄ Ïù∏ÏßÄÏùò Ï§ëÏöîÏÑ±\n",
    "\n",
    "4. LLMÍ≥ºÏùò ÏúµÌï©\n",
    "   ‚Ä¢ Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏÇ¨Ï†ÑÏßÄÏãù\n",
    "   ‚Ä¢ Ï∂îÎ°† Îä•Î†• ÌôúÏö©\n",
    "   ‚Ä¢ Few-shot ÌïôÏäµ\n",
    "\n",
    "5. ÎØ∏Îûò Î∞©Ìñ•\n",
    "   ‚Ä¢ Îçî Î≥µÏû°Ìïú Ï∂îÎ°† Ï≤¥Ïù∏\n",
    "   ‚Ä¢ ÏûêÍ∏∞ Î∞òÏÑ± (self-reflection)\n",
    "   ‚Ä¢ Î©îÌÉÄ ÌïôÏäµÍ≥º Ï†ÅÏùë\n",
    "   ‚Ä¢ Ïù∏Í∞Ñ ÏàòÏ§ÄÏùò Í≥ÑÌöç Îä•Î†•\n",
    "\"\"\"\n",
    "\n",
    "print(insights)\n",
    "\n",
    "print(\"\\nüöÄ Îã§Ïùå ÎÖ∏Ìä∏Î∂Å ÏòàÍ≥†:\")\n",
    "print(\"Notebook 5: LLM + RL - RLHFÏôÄ ÎØ∏Îûò\")\n",
    "print(\"‚Ä¢ RLHFÎ°ú LLM Ï†ïÎ†¨\")\n",
    "print(\"‚Ä¢ Constitutional AI\")\n",
    "print(\"‚Ä¢ LLM as Policy\")\n",
    "print(\"‚Ä¢ RLÏùò ÎØ∏Îûò Ï†ÑÎßù\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïôÏäµ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏\n",
    "print(\"\\nüéØ ÌïôÏäµ ÏôÑÎ£å Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏:\")\n",
    "print(\"‚úÖ Ï†ÑÎ∞òÏ†Ñ vs ÌõÑÎ∞òÏ†Ñ Ìå®Îü¨Îã§ÏûÑ Ïù¥Ìï¥\")\n",
    "print(\"‚úÖ THINKÎ•º ÌñâÎèôÏúºÎ°ú Íµ¨ÌòÑ\")\n",
    "print(\"‚úÖ ReAct ÏóêÏù¥Ï†ÑÌä∏ Íµ¨ÌòÑ\")\n",
    "print(\"‚úÖ MCTS Ïù¥Ìï¥ Î∞è Íµ¨ÌòÑ\")\n",
    "print(\"‚úÖ Chain-of-Thought in RL\")\n",
    "print(\"‚úÖ Test-time compute Î∂ÑÏÑù\")\n",
    "print(\"‚úÖ Î©îÌÉÄ Ï∂îÎ°† Í∞úÎÖê Ïù¥Ìï¥\")\n",
    "print(\"\\nüéâ Ï∂ïÌïòÌï©ÎãàÎã§! Ï∂îÎ°† Í∏∞Î∞ò RLÏùò ÌïµÏã¨ÏùÑ ÎßàÏä§ÌÑ∞ÌñàÏäµÎãàÎã§!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}