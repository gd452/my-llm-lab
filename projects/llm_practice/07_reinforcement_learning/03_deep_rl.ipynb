{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Deep RL - DQNê³¼ PPO\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- Deep Q-Network (DQN) ì´í•´ ë° êµ¬í˜„\n",
    "- Experience Replayì™€ Target Network í™œìš©\n",
    "- Policy Gradient Methods í•™ìŠµ\n",
    "- PPO (Proximal Policy Optimization) êµ¬í˜„\n",
    "- OpenAI Gym í™˜ê²½ì—ì„œ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep RLì˜ í•„ìš”ì„±\n",
    "\n",
    "### Tabular RLì˜ í•œê³„\n",
    "- **ìƒíƒœ ê³µê°„ì´ í¬ë©´**: Q-tableì´ ë„ˆë¬´ ì»¤ì§\n",
    "- **ì—°ì† ìƒíƒœ**: í…Œì´ë¸”ë¡œ í‘œí˜„ ë¶ˆê°€\n",
    "- **ì¼ë°˜í™” ë¶ˆê°€**: ë¹„ìŠ·í•œ ìƒíƒœ ê°„ ì •ë³´ ê³µìœ  X\n",
    "\n",
    "### Deep RLì˜ í•´ê²°ì±…\n",
    "- **í•¨ìˆ˜ ê·¼ì‚¬**: Neural Networkë¡œ Q í•¨ìˆ˜ ê·¼ì‚¬\n",
    "- **ì¼ë°˜í™”**: ë¹„ìŠ·í•œ ìƒíƒœì— ëŒ€í•œ ì¼ë°˜í™”\n",
    "- **í™•ì¥ì„±**: ë³µì¡í•œ í™˜ê²½ ì²˜ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Deep RL í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network (DQN)\n",
    "\n",
    "DQNì€ Q-Learning + Neural Networkì…ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ í˜ì‹ \n",
    "1. **Experience Replay**: ê²½í—˜ì„ ì €ì¥í•˜ê³  ëœë¤ ìƒ˜í”Œë§\n",
    "2. **Target Network**: ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ ë³„ë„ ë„¤íŠ¸ì›Œí¬\n",
    "3. **Gradient Clipping**: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple('Experience', \n",
    "                                    ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"ëœë¤ ìƒ˜í”Œë§\"\"\"\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)\n",
    "        dones = torch.FloatTensor([e.done for e in experiences]).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                 buffer_size=10000, batch_size=64):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Q-Networkì™€ Target Network\n",
    "        self.q_network = DQN(state_dim, 128, action_dim).to(device)\n",
    "        self.target_network = DQN(state_dim, 128, action_dim).to(device)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # í•™ìŠµ ê¸°ë¡\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Target Network ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Îµ-greedy í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Experience Replayë¡œ í•™ìŠµ\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # í˜„ì¬ Q ê°’\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q ê°’ (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# CartPole í™˜ê²½ì—ì„œ DQN í…ŒìŠ¤íŠ¸\n",
    "def train_dqn_cartpole(n_episodes=200):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training DQN\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # Target network ì—…ë°ì´íŠ¸\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Episode {episode}, Average Score: {np.mean(scores_window):.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # í•´ê²° ì¡°ê±´\n",
    "        if np.mean(scores_window) >= 195.0:\n",
    "            print(f\"\\ní™˜ê²½ í•´ê²°! Episode {episode}, Average Score: {np.mean(scores_window):.2f}\")\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "dqn_agent, dqn_scores = train_dqn_cartpole()\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dqn_scores)\n",
    "plt.plot(np.convolve(dqn_scores, np.ones(20)/20, mode='valid'), 'r', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('DQN Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(dqn_agent.losses) > 0:\n",
    "    plt.plot(dqn_agent.losses[-1000:])\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('DQN Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient Methods\n",
    "\n",
    "Policy GradientëŠ” ì •ì±…ì„ ì§ì ‘ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "- **ì§ì ‘ ì •ì±… í•™ìŠµ**: Ï€(a|s) ìì²´ë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ í‘œí˜„\n",
    "- **í™•ë¥ ì  ì •ì±…**: í–‰ë™ í™•ë¥  ë¶„í¬ ì¶œë ¥\n",
    "- **ê²½ì‚¬ ìƒìŠ¹**: ê¸°ëŒ€ ë³´ìƒì„ ìµœëŒ€í™”\n",
    "\n",
    "### REINFORCE ì•Œê³ ë¦¬ì¦˜\n",
    "$$\\nabla J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"ì •ì±… ë„¤íŠ¸ì›Œí¬\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE ì•Œê³ ë¦¬ì¦˜\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"í™•ë¥ ì  í–‰ë™ ì„ íƒ\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state_tensor)\n",
    "        \n",
    "        # í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        R = 0\n",
    "        returns = []\n",
    "        \n",
    "        # ë¦¬í„´ ê³„ì‚° (backward)\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # ì •ê·œí™” (variance reduction)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # ë²„í¼ ì´ˆê¸°í™”\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "# REINFORCE í•™ìŠµ\n",
    "def train_reinforce(n_episodes=500):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = REINFORCE(state_dim, action_dim)\n",
    "    scores = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training REINFORCE\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update()\n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_score = np.mean(scores[-50:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "reinforce_agent, reinforce_scores = train_reinforce(300)\n",
    "\n",
    "# ë¹„êµ ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(reinforce_scores, np.ones(20)/20, mode='valid'), label='REINFORCE')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('REINFORCE Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor-Critic\n",
    "\n",
    "Actor-Criticì€ Policy Gradientì™€ Value Functionì„ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### êµ¬ì„± ìš”ì†Œ\n",
    "- **Actor**: ì •ì±… Ï€(a|s) í•™ìŠµ\n",
    "- **Critic**: ê°€ì¹˜ í•¨ìˆ˜ V(s) í•™ìŠµ\n",
    "- **Advantage**: A(s,a) = Q(s,a) - V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic ë„¤íŠ¸ì›Œí¬\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # ê³µìœ  ë ˆì´ì–´\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Actor í—¤ë“œ\n",
    "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Critic í—¤ë“œ\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # ì •ì±… (í–‰ë™ í™•ë¥ )\n",
    "        policy = F.softmax(self.actor(x), dim=-1)\n",
    "        \n",
    "        # ê°€ì¹˜\n",
    "        value = self.critic(x)\n",
    "        \n",
    "        return policy, value\n",
    "\n",
    "class A2C:\n",
    "    \"\"\"Advantage Actor-Critic\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.model = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        policy, value = self.model(state_tensor)\n",
    "        \n",
    "        m = torch.distributions.Categorical(policy)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        self.values.append(value)\n",
    "        self.entropy.append(m.entropy())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, next_state, done):\n",
    "        R = 0\n",
    "        if not done:\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            _, next_value = self.model(next_state_tensor)\n",
    "            R = next_value.item()\n",
    "        \n",
    "        values = self.values + [torch.tensor([[R]]).to(device)]\n",
    "        \n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # Advantage ê³„ì‚°\n",
    "        advantages = returns - torch.cat(values[:-1]).squeeze()\n",
    "        \n",
    "        # Loss ê³„ì‚°\n",
    "        actor_loss = -(torch.stack(self.log_probs) * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        entropy_loss = -torch.stack(self.entropy).mean()\n",
    "        \n",
    "        loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # ë²„í¼ ì´ˆê¸°í™”\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "# A2C í•™ìŠµ\n",
    "def train_a2c(n_episodes=300):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = A2C(state_dim, action_dim)\n",
    "    scores = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training A2C\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                agent.update(next_state, done)\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_score = np.mean(scores[-50:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "a2c_agent, a2c_scores = train_a2c()\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(a2c_scores, np.ones(20)/20, mode='valid'), label='A2C')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('A2C Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPOëŠ” í˜„ì¬ ê°€ì¥ ì¸ê¸° ìˆëŠ” RL ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "- **Trust Region**: ì •ì±… ì—…ë°ì´íŠ¸ë¥¼ ì œí•œ\n",
    "- **Clipped Objective**: ë„ˆë¬´ í° ì—…ë°ì´íŠ¸ ë°©ì§€\n",
    "- **Multiple Epochs**: ê°™ì€ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ë²ˆ í•™ìŠµ\n",
    "\n",
    "### PPO-Clip Objective\n",
    "$$L^{CLIP}(\\theta) = E_t[\\min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\"Proximal Policy Optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, \n",
    "                 eps_clip=0.2, k_epochs=4, gae_lambda=0.95):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.memory = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'next_states': [],\n",
    "            'dones': [],\n",
    "            'log_probs': []\n",
    "        }\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            policy, _ = self.policy_old(state_tensor)\n",
    "            m = torch.distributions.Categorical(policy)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item()\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory['states'].append(state)\n",
    "        self.memory['actions'].append(action)\n",
    "        self.memory['rewards'].append(reward)\n",
    "        self.memory['next_states'].append(next_state)\n",
    "        self.memory['dones'].append(done)\n",
    "        self.memory['log_probs'].append(log_prob)\n",
    "    \n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"Generalized Advantage Estimation\"\"\"\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                nextnonterminal = 1.0 - dones[t]\n",
    "                nextvalues = next_values[t]\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t]\n",
    "                nextvalues = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"PPO ì—…ë°ì´íŠ¸\"\"\"\n",
    "        # í…ì„œ ë³€í™˜\n",
    "        states = torch.FloatTensor(self.memory['states']).to(device)\n",
    "        actions = torch.LongTensor(self.memory['actions']).to(device)\n",
    "        rewards = torch.FloatTensor(self.memory['rewards']).to(device)\n",
    "        next_states = torch.FloatTensor(self.memory['next_states']).to(device)\n",
    "        dones = torch.FloatTensor(self.memory['dones']).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.memory['log_probs']).to(device)\n",
    "        \n",
    "        # ê°€ì¹˜ ê³„ì‚°\n",
    "        with torch.no_grad():\n",
    "            _, values = self.policy_old(states)\n",
    "            _, next_values = self.policy_old(next_states)\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "        \n",
    "        # GAE ê³„ì‚°\n",
    "        advantages = self.compute_gae(rewards, values, next_values, dones)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # K epochs í•™ìŠµ\n",
    "        for _ in range(self.k_epochs):\n",
    "            # í˜„ì¬ ì •ì±… í‰ê°€\n",
    "            policy, values = self.policy(states)\n",
    "            m = torch.distributions.Categorical(policy)\n",
    "            log_probs = m.log_prob(actions)\n",
    "            entropy = m.entropy()\n",
    "            \n",
    "            # Ratio ê³„ì‚°\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            # Clipped objective\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            # Loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss\n",
    "            \n",
    "            # ì—­ì „íŒŒ\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Old policy ì—…ë°ì´íŠ¸\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
    "        self.memory = {key: [] for key in self.memory.keys()}\n",
    "\n",
    "# PPO í•™ìŠµ\n",
    "def train_ppo(n_episodes=300, update_interval=2000):\n",
    "    env = gym.make('LunarLander-v2', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    scores = []\n",
    "    steps = 0\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training PPO\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # ì—…ë°ì´íŠ¸\n",
    "            if steps % update_interval == 0:\n",
    "                agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            avg_score = np.mean(scores[-20:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "print(\"\\nPPOë¥¼ LunarLanderì—ì„œ í•™ìŠµ ì¤‘...\")\n",
    "ppo_agent, ppo_scores = train_ppo(200)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(ppo_scores, np.ones(20)/20, mode='valid'))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('PPO on LunarLander-v2')\n",
    "plt.axhline(y=200, color='r', linestyle='--', label='Solved')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì•Œê³ ë¦¬ì¦˜ ë¹„êµ\n",
    "\n",
    "ê° ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì§•ê³¼ ì„±ëŠ¥ì„ ë¹„êµí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms():\n",
    "    \"\"\"DQN vs Policy Gradient ì•Œê³ ë¦¬ì¦˜ ë¹„êµ\"\"\"\n",
    "    \n",
    "    print(\"ì•Œê³ ë¦¬ì¦˜ ë¹„êµ í‘œ\\n\" + \"=\"*60)\n",
    "    \n",
    "    comparison = {\n",
    "        'Algorithm': ['DQN', 'REINFORCE', 'A2C', 'PPO'],\n",
    "        'Type': ['Value-based', 'Policy-based', 'Actor-Critic', 'Actor-Critic'],\n",
    "        'Off-policy': ['Yes', 'No', 'No', 'No'],\n",
    "        'Sample Efficiency': ['High', 'Low', 'Medium', 'High'],\n",
    "        'Stability': ['Medium', 'Low', 'Medium', 'High'],\n",
    "        'Continuous Action': ['No', 'Yes', 'Yes', 'Yes'],\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\nì£¼ìš” ì¸ì‚¬ì´íŠ¸:\")\n",
    "    print(\"1. DQN: ì´ì‚° í–‰ë™ ê³µê°„ì— íš¨ê³¼ì , Experience Replayë¡œ ìƒ˜í”Œ íš¨ìœ¨ì„± ë†’ìŒ\")\n",
    "    print(\"2. REINFORCE: ê°„ë‹¨í•˜ì§€ë§Œ ë†’ì€ ë¶„ì‚°, ìƒ˜í”Œ íš¨ìœ¨ì„± ë‚®ìŒ\")\n",
    "    print(\"3. A2C: Actor-Criticìœ¼ë¡œ ë¶„ì‚° ê°ì†Œ, ë” ì•ˆì •ì \")\n",
    "    print(\"4. PPO: í˜„ì¬ ê°€ì¥ ì¸ê¸°, ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•\")\n",
    "    \n",
    "    # í•™ìŠµ ê³¡ì„  ë¹„êµ (CartPole)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # ê°€ìƒì˜ í•™ìŠµ ê³¡ì„  (ì‹¤ì œ ê²°ê³¼ ê¸°ë°˜)\n",
    "    episodes = np.arange(200)\n",
    "    \n",
    "    # DQN: ë¹ ë¥¸ ìˆ˜ë ´\n",
    "    dqn_curve = 200 / (1 + np.exp(-0.05 * (episodes - 50)))\n",
    "    \n",
    "    # REINFORCE: ëŠë¦¬ê³  ë¶ˆì•ˆì •\n",
    "    reinforce_curve = 200 / (1 + np.exp(-0.03 * (episodes - 100))) + np.random.normal(0, 20, 200)\n",
    "    \n",
    "    # A2C: ì¤‘ê°„ ì†ë„\n",
    "    a2c_curve = 200 / (1 + np.exp(-0.04 * (episodes - 70)))\n",
    "    \n",
    "    # PPO: ì•ˆì •ì \n",
    "    ppo_curve = 200 / (1 + np.exp(-0.045 * (episodes - 60)))\n",
    "    \n",
    "    plt.plot(episodes, np.convolve(dqn_curve, np.ones(10)/10, mode='same'), label='DQN', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(reinforce_curve, np.ones(10)/10, mode='same'), label='REINFORCE', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(a2c_curve, np.ones(10)/10, mode='same'), label='A2C', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(ppo_curve, np.ones(10)/10, mode='same'), label='PPO', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=195, color='r', linestyle='--', alpha=0.5, label='Solved')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Algorithm Comparison on CartPole-v1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "compare_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ì „ íŒê³¼ íŠ¸ë¦­\n",
    "\n",
    "Deep RL êµ¬í˜„ ì‹œ ìœ ìš©í•œ íŒë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_rl_tips():\n",
    "    \"\"\"Deep RL ì‹¤ì „ íŒ\"\"\"\n",
    "    \n",
    "    tips = \"\"\"\n",
    "    ğŸ¯ Deep RL ì‹¤ì „ íŒ\n",
    "    ==================\n",
    "    \n",
    "    1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "    ----------------------\n",
    "    â€¢ Learning Rate: ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµ ì•ˆë¨\n",
    "    â€¢ Batch Size: í´ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼\n",
    "    â€¢ Network Size: ë¬¸ì œ ë³µì¡ë„ì— ë§ê²Œ ì¡°ì ˆ\n",
    "    â€¢ Epsilon Decay: íƒìƒ‰-í™œìš© ê· í˜• ì¤‘ìš”\n",
    "    \n",
    "    2. ë””ë²„ê¹… ì „ëµ\n",
    "    --------------\n",
    "    â€¢ ê°„ë‹¨í•œ í™˜ê²½ë¶€í„° ì‹œì‘ (CartPole â†’ LunarLander â†’ Atari)\n",
    "    â€¢ ë³´ìƒ ìŠ¤ì¼€ì¼ë§ í™•ì¸\n",
    "    â€¢ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ëª¨ë‹ˆí„°ë§\n",
    "    â€¢ í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "    \n",
    "    3. ì•ˆì •ì„± ê°œì„ \n",
    "    --------------\n",
    "    â€¢ Gradient Clipping ì‚¬ìš©\n",
    "    â€¢ ë³´ìƒ ì •ê·œí™” (reward normalization)\n",
    "    â€¢ Advantage ì •ê·œí™”\n",
    "    â€¢ Learning rate scheduling\n",
    "    \n",
    "    4. ì„±ëŠ¥ ê°œì„ \n",
    "    ------------\n",
    "    â€¢ Parallel environments ì‚¬ìš©\n",
    "    â€¢ GPU í™œìš©\n",
    "    â€¢ Vectorized operations\n",
    "    â€¢ JIT compilation (PyTorch 2.0+)\n",
    "    \n",
    "    5. ì¼ë°˜ì ì¸ ì‹¤ìˆ˜\n",
    "    ----------------\n",
    "    âŒ ë„ˆë¬´ ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ì‹œì‘\n",
    "    âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œì— ì—¬ëŸ¬ ê°œ ë³€ê²½\n",
    "    âŒ ì¶©ë¶„í•œ íƒìƒ‰ ì—†ì´ í™œìš©ë§Œ\n",
    "    âŒ ë³´ìƒ í•¨ìˆ˜ ì˜ëª» ì„¤ê³„\n",
    "    \n",
    "    6. ê¶Œì¥ ì‚¬í•­\n",
    "    ------------\n",
    "    âœ… ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¡œ ì‹œì‘\n",
    "    âœ… í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½\n",
    "    âœ… ë¡œê¹…ê³¼ ì‹œê°í™” ì² ì €íˆ\n",
    "    âœ… ì¬í˜„ ê°€ëŠ¥ì„± ìœ„í•´ ì‹œë“œ ê³ ì •\n",
    "    \"\"\"\n",
    "    \n",
    "    print(tips)\n",
    "    \n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜í–¥ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Learning Rate ì˜í–¥\n",
    "    ax = axes[0, 0]\n",
    "    lrs = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    colors = ['blue', 'green', 'orange', 'red']\n",
    "    for lr, color in zip(lrs, colors):\n",
    "        if lr == 1e-3:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 1)\n",
    "        elif lr == 1e-4:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 0.5)\n",
    "        elif lr == 1e-2:\n",
    "            curve = np.cumsum(np.random.randn(100) * 3 + 0.8)\n",
    "        else:\n",
    "            curve = np.cumsum(np.random.randn(100) * 10 - 2)\n",
    "        ax.plot(curve, label=f'LR={lr}', color=color)\n",
    "    ax.set_title('Learning Rate ì˜í–¥')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Batch Size ì˜í–¥\n",
    "    ax = axes[0, 1]\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    for bs, color in zip(batch_sizes, colors):\n",
    "        noise = 50 / bs\n",
    "        curve = np.cumsum(np.random.randn(100) * noise + 1)\n",
    "        ax.plot(curve, label=f'Batch={bs}', color=color)\n",
    "    ax.set_title('Batch Size ì˜í–¥')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Network Size ì˜í–¥\n",
    "    ax = axes[1, 0]\n",
    "    sizes = ['Small (32)', 'Medium (128)', 'Large (512)', 'Huge (2048)']\n",
    "    for size, color in zip(sizes, colors):\n",
    "        if 'Small' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 0.7)\n",
    "        elif 'Medium' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 1)\n",
    "        elif 'Large' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2.5 + 0.9)\n",
    "        else:\n",
    "            curve = np.cumsum(np.random.randn(100) * 3 + 0.6)  # Overfitting\n",
    "        ax.plot(curve, label=size, color=color)\n",
    "    ax.set_title('Network Size ì˜í–¥')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Epsilon Decay ì˜í–¥\n",
    "    ax = axes[1, 1]\n",
    "    epsilons = np.linspace(0, 100, 100)\n",
    "    ax.plot(np.exp(-epsilons/20), label='Fast decay', color='red')\n",
    "    ax.plot(np.exp(-epsilons/50), label='Medium decay', color='green')\n",
    "    ax.plot(np.exp(-epsilons/100), label='Slow decay', color='blue')\n",
    "    ax.plot(np.ones(100) * 0.1, label='Constant', color='orange', linestyle='--')\n",
    "    ax.set_title('Epsilon Decay ì „ëµ')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Epsilon')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "deep_rl_tips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ìš”ì•½ ë° í•µì‹¬ ê°œë…\n",
    "\n",
    "### ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "1. **Deep Q-Network (DQN)**\n",
    "   - Neural Networkë¡œ Q í•¨ìˆ˜ ê·¼ì‚¬\n",
    "   - Experience Replay: ë°ì´í„° íš¨ìœ¨ì„±\n",
    "   - Target Network: í•™ìŠµ ì•ˆì •ì„±\n",
    "   - Double DQN: ê³¼ëŒ€í‰ê°€ í•´ê²°\n",
    "\n",
    "2. **Policy Gradient Methods**\n",
    "   - REINFORCE: ê¸°ë³¸ policy gradient\n",
    "   - ì§ì ‘ ì •ì±… ìµœì í™”\n",
    "   - í™•ë¥ ì  ì •ì±… í•™ìŠµ\n",
    "\n",
    "3. **Actor-Critic**\n",
    "   - Policy + Value ê²°í•©\n",
    "   - Advantage í•¨ìˆ˜ í™œìš©\n",
    "   - ë¶„ì‚° ê°ì†Œ\n",
    "\n",
    "4. **PPO**\n",
    "   - Trust Region ë°©ë²•\n",
    "   - Clipped objective\n",
    "   - í˜„ëŒ€ RLì˜ ì£¼ë ¥ ì•Œê³ ë¦¬ì¦˜\n",
    "\n",
    "### ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ìƒí™© | ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ | ì´ìœ  |\n",
    "|------|--------------|------|\n",
    "| ì´ì‚° í–‰ë™, ê°„ë‹¨í•œ í™˜ê²½ | DQN | ìƒ˜í”Œ íš¨ìœ¨ì , ì•ˆì •ì  |\n",
    "| ì—°ì† í–‰ë™ ê³µê°„ | PPO, SAC | ì—°ì† í–‰ë™ ì§ì ‘ ì²˜ë¦¬ |\n",
    "| ë³µì¡í•œ í™˜ê²½ | PPO | ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ ê· í˜• |\n",
    "| ë¡œë´‡ ì œì–´ | TD3, SAC | ì—°ì† ì œì–´ íŠ¹í™” |\n",
    "| ë©€í‹°ì—ì´ì „íŠ¸ | MAPPO | í˜‘ë ¥ í•™ìŠµ |\n",
    "\n",
    "### ë‹¤ìŒ ë…¸íŠ¸ë¶ ì˜ˆê³ \n",
    "**Notebook 4: ì¶”ë¡  ê¸°ë°˜ RL - Planningê³¼ ReAct**\n",
    "- Model-based planning\n",
    "- MCTSì™€ AlphaZero\n",
    "- ReAct: Reasoning + Acting\n",
    "- LLMê³¼ RLì˜ ë§Œë‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\n",
    "print(\"ğŸ¯ í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(\"âœ… DQN êµ¬í˜„ ë° ì´í•´\")\n",
    "print(\"âœ… Experience Replayì™€ Target Network\")\n",
    "print(\"âœ… Policy Gradient (REINFORCE) êµ¬í˜„\")\n",
    "print(\"âœ… Actor-Critic ì´í•´\")\n",
    "print(\"âœ… PPO êµ¬í˜„\")\n",
    "print(\"âœ… ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ë° ì„ íƒ ê¸°ì¤€\")\n",
    "print(\"âœ… Deep RL ì‹¤ì „ íŒ\")\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: ì¶”ë¡ (Reasoning)ì„ í™œìš©í•œ ì§€ëŠ¥ì  RL!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}