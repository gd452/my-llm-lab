{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Deep RL - DQN과 PPO\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- Deep Q-Network (DQN) 이해 및 구현\n",
    "- Experience Replay와 Target Network 활용\n",
    "- Policy Gradient Methods 학습\n",
    "- PPO (Proximal Policy Optimization) 구현\n",
    "- OpenAI Gym 환경에서 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep RL의 필요성\n",
    "\n",
    "### Tabular RL의 한계\n",
    "- **상태 공간이 크면**: Q-table이 너무 커짐\n",
    "- **연속 상태**: 테이블로 표현 불가\n",
    "- **일반화 불가**: 비슷한 상태 간 정보 공유 X\n",
    "\n",
    "### Deep RL의 해결책\n",
    "- **함수 근사**: Neural Network로 Q 함수 근사\n",
    "- **일반화**: 비슷한 상태에 대한 일반화\n",
    "- **확장성**: 복잡한 환경 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 사용 가능 여부\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# 시드 설정\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Deep RL 환경 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network (DQN)\n",
    "\n",
    "DQN은 Q-Learning + Neural Network입니다.\n",
    "\n",
    "### 핵심 혁신\n",
    "1. **Experience Replay**: 경험을 저장하고 랜덤 샘플링\n",
    "2. **Target Network**: 안정적인 학습을 위한 별도 네트워크\n",
    "3. **Gradient Clipping**: 그래디언트 폭발 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple('Experience', \n",
    "                                    ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"랜덤 샘플링\"\"\"\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device)\n",
    "        dones = torch.FloatTensor([e.done for e in experiences]).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                 buffer_size=10000, batch_size=64):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Q-Network와 Target Network\n",
    "        self.q_network = DQN(state_dim, 128, action_dim).to(device)\n",
    "        self.target_network = DQN(state_dim, 128, action_dim).to(device)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # 학습 기록\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Target Network 업데이트\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"ε-greedy 행동 선택\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Experience Replay로 학습\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # 현재 Q 값\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q 값 (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # 역전파\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# CartPole 환경에서 DQN 테스트\n",
    "def train_dqn_cartpole(n_episodes=200):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training DQN\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # Target network 업데이트\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Episode {episode}, Average Score: {np.mean(scores_window):.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # 해결 조건\n",
    "        if np.mean(scores_window) >= 195.0:\n",
    "            print(f\"\\n환경 해결! Episode {episode}, Average Score: {np.mean(scores_window):.2f}\")\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# 학습 실행\n",
    "dqn_agent, dqn_scores = train_dqn_cartpole()\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dqn_scores)\n",
    "plt.plot(np.convolve(dqn_scores, np.ones(20)/20, mode='valid'), 'r', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('DQN Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(dqn_agent.losses) > 0:\n",
    "    plt.plot(dqn_agent.losses[-1000:])\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('DQN Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient Methods\n",
    "\n",
    "Policy Gradient는 정책을 직접 최적화합니다.\n",
    "\n",
    "### 핵심 아이디어\n",
    "- **직접 정책 학습**: π(a|s) 자체를 신경망으로 표현\n",
    "- **확률적 정책**: 행동 확률 분포 출력\n",
    "- **경사 상승**: 기대 보상을 최대화\n",
    "\n",
    "### REINFORCE 알고리즘\n",
    "$$\\nabla J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"정책 네트워크\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE 알고리즘\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"확률적 행동 선택\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state_tensor)\n",
    "        \n",
    "        # 확률 분포에서 샘플링\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"에피소드 종료 후 업데이트\"\"\"\n",
    "        R = 0\n",
    "        returns = []\n",
    "        \n",
    "        # 리턴 계산 (backward)\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # 정규화 (variance reduction)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # 역전파\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 버퍼 초기화\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "# REINFORCE 학습\n",
    "def train_reinforce(n_episodes=500):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = REINFORCE(state_dim, action_dim)\n",
    "    scores = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training REINFORCE\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update()\n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_score = np.mean(scores[-50:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# 학습 실행\n",
    "reinforce_agent, reinforce_scores = train_reinforce(300)\n",
    "\n",
    "# 비교 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(reinforce_scores, np.ones(20)/20, mode='valid'), label='REINFORCE')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('REINFORCE Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor-Critic\n",
    "\n",
    "Actor-Critic은 Policy Gradient와 Value Function을 결합합니다.\n",
    "\n",
    "### 구성 요소\n",
    "- **Actor**: 정책 π(a|s) 학습\n",
    "- **Critic**: 가치 함수 V(s) 학습\n",
    "- **Advantage**: A(s,a) = Q(s,a) - V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic 네트워크\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # 공유 레이어\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Actor 헤드\n",
    "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Critic 헤드\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # 정책 (행동 확률)\n",
    "        policy = F.softmax(self.actor(x), dim=-1)\n",
    "        \n",
    "        # 가치\n",
    "        value = self.critic(x)\n",
    "        \n",
    "        return policy, value\n",
    "\n",
    "class A2C:\n",
    "    \"\"\"Advantage Actor-Critic\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.model = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        policy, value = self.model(state_tensor)\n",
    "        \n",
    "        m = torch.distributions.Categorical(policy)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.log_probs.append(m.log_prob(action))\n",
    "        self.values.append(value)\n",
    "        self.entropy.append(m.entropy())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, next_state, done):\n",
    "        R = 0\n",
    "        if not done:\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            _, next_value = self.model(next_state_tensor)\n",
    "            R = next_value.item()\n",
    "        \n",
    "        values = self.values + [torch.tensor([[R]]).to(device)]\n",
    "        \n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # Advantage 계산\n",
    "        advantages = returns - torch.cat(values[:-1]).squeeze()\n",
    "        \n",
    "        # Loss 계산\n",
    "        actor_loss = -(torch.stack(self.log_probs) * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        entropy_loss = -torch.stack(self.entropy).mean()\n",
    "        \n",
    "        loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss\n",
    "        \n",
    "        # 역전파\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 버퍼 초기화\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "# A2C 학습\n",
    "def train_a2c(n_episodes=300):\n",
    "    env = gym.make('CartPole-v1', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = A2C(state_dim, action_dim)\n",
    "    scores = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training A2C\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                agent.update(next_state, done)\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            avg_score = np.mean(scores[-50:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# 학습 실행\n",
    "a2c_agent, a2c_scores = train_a2c()\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(a2c_scores, np.ones(20)/20, mode='valid'), label='A2C')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('A2C Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO는 현재 가장 인기 있는 RL 알고리즘 중 하나입니다.\n",
    "\n",
    "### 핵심 아이디어\n",
    "- **Trust Region**: 정책 업데이트를 제한\n",
    "- **Clipped Objective**: 너무 큰 업데이트 방지\n",
    "- **Multiple Epochs**: 같은 데이터로 여러 번 학습\n",
    "\n",
    "### PPO-Clip Objective\n",
    "$$L^{CLIP}(\\theta) = E_t[\\min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\"Proximal Policy Optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, \n",
    "                 eps_clip=0.2, k_epochs=4, gae_lambda=0.95):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_dim, 128, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.memory = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'next_states': [],\n",
    "            'dones': [],\n",
    "            'log_probs': []\n",
    "        }\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            policy, _ = self.policy_old(state_tensor)\n",
    "            m = torch.distributions.Categorical(policy)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item()\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"경험 저장\"\"\"\n",
    "        self.memory['states'].append(state)\n",
    "        self.memory['actions'].append(action)\n",
    "        self.memory['rewards'].append(reward)\n",
    "        self.memory['next_states'].append(next_state)\n",
    "        self.memory['dones'].append(done)\n",
    "        self.memory['log_probs'].append(log_prob)\n",
    "    \n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"Generalized Advantage Estimation\"\"\"\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                nextnonterminal = 1.0 - dones[t]\n",
    "                nextvalues = next_values[t]\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t]\n",
    "                nextvalues = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"PPO 업데이트\"\"\"\n",
    "        # 텐서 변환\n",
    "        states = torch.FloatTensor(self.memory['states']).to(device)\n",
    "        actions = torch.LongTensor(self.memory['actions']).to(device)\n",
    "        rewards = torch.FloatTensor(self.memory['rewards']).to(device)\n",
    "        next_states = torch.FloatTensor(self.memory['next_states']).to(device)\n",
    "        dones = torch.FloatTensor(self.memory['dones']).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.memory['log_probs']).to(device)\n",
    "        \n",
    "        # 가치 계산\n",
    "        with torch.no_grad():\n",
    "            _, values = self.policy_old(states)\n",
    "            _, next_values = self.policy_old(next_states)\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "        \n",
    "        # GAE 계산\n",
    "        advantages = self.compute_gae(rewards, values, next_values, dones)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        # 정규화\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # K epochs 학습\n",
    "        for _ in range(self.k_epochs):\n",
    "            # 현재 정책 평가\n",
    "            policy, values = self.policy(states)\n",
    "            m = torch.distributions.Categorical(policy)\n",
    "            log_probs = m.log_prob(actions)\n",
    "            entropy = m.entropy()\n",
    "            \n",
    "            # Ratio 계산\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            # Clipped objective\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            # Loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss\n",
    "            \n",
    "            # 역전파\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Old policy 업데이트\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # 메모리 초기화\n",
    "        self.memory = {key: [] for key in self.memory.keys()}\n",
    "\n",
    "# PPO 학습\n",
    "def train_ppo(n_episodes=300, update_interval=2000):\n",
    "    env = gym.make('LunarLander-v2', render_mode=None)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    scores = []\n",
    "    steps = 0\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Training PPO\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # 업데이트\n",
    "            if steps % update_interval == 0:\n",
    "                agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            avg_score = np.mean(scores[-20:])\n",
    "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, scores\n",
    "\n",
    "# 학습 실행\n",
    "print(\"\\nPPO를 LunarLander에서 학습 중...\")\n",
    "ppo_agent, ppo_scores = train_ppo(200)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.convolve(ppo_scores, np.ones(20)/20, mode='valid'))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('PPO on LunarLander-v2')\n",
    "plt.axhline(y=200, color='r', linestyle='--', label='Solved')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 알고리즘 비교\n",
    "\n",
    "각 알고리즘의 특징과 성능을 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms():\n",
    "    \"\"\"DQN vs Policy Gradient 알고리즘 비교\"\"\"\n",
    "    \n",
    "    print(\"알고리즘 비교 표\\n\" + \"=\"*60)\n",
    "    \n",
    "    comparison = {\n",
    "        'Algorithm': ['DQN', 'REINFORCE', 'A2C', 'PPO'],\n",
    "        'Type': ['Value-based', 'Policy-based', 'Actor-Critic', 'Actor-Critic'],\n",
    "        'Off-policy': ['Yes', 'No', 'No', 'No'],\n",
    "        'Sample Efficiency': ['High', 'Low', 'Medium', 'High'],\n",
    "        'Stability': ['Medium', 'Low', 'Medium', 'High'],\n",
    "        'Continuous Action': ['No', 'Yes', 'Yes', 'Yes'],\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\\n주요 인사이트:\")\n",
    "    print(\"1. DQN: 이산 행동 공간에 효과적, Experience Replay로 샘플 효율성 높음\")\n",
    "    print(\"2. REINFORCE: 간단하지만 높은 분산, 샘플 효율성 낮음\")\n",
    "    print(\"3. A2C: Actor-Critic으로 분산 감소, 더 안정적\")\n",
    "    print(\"4. PPO: 현재 가장 인기, 안정성과 성능의 균형\")\n",
    "    \n",
    "    # 학습 곡선 비교 (CartPole)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 가상의 학습 곡선 (실제 결과 기반)\n",
    "    episodes = np.arange(200)\n",
    "    \n",
    "    # DQN: 빠른 수렴\n",
    "    dqn_curve = 200 / (1 + np.exp(-0.05 * (episodes - 50)))\n",
    "    \n",
    "    # REINFORCE: 느리고 불안정\n",
    "    reinforce_curve = 200 / (1 + np.exp(-0.03 * (episodes - 100))) + np.random.normal(0, 20, 200)\n",
    "    \n",
    "    # A2C: 중간 속도\n",
    "    a2c_curve = 200 / (1 + np.exp(-0.04 * (episodes - 70)))\n",
    "    \n",
    "    # PPO: 안정적\n",
    "    ppo_curve = 200 / (1 + np.exp(-0.045 * (episodes - 60)))\n",
    "    \n",
    "    plt.plot(episodes, np.convolve(dqn_curve, np.ones(10)/10, mode='same'), label='DQN', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(reinforce_curve, np.ones(10)/10, mode='same'), label='REINFORCE', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(a2c_curve, np.ones(10)/10, mode='same'), label='A2C', linewidth=2)\n",
    "    plt.plot(episodes, np.convolve(ppo_curve, np.ones(10)/10, mode='same'), label='PPO', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=195, color='r', linestyle='--', alpha=0.5, label='Solved')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Algorithm Comparison on CartPole-v1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "compare_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실전 팁과 트릭\n",
    "\n",
    "Deep RL 구현 시 유용한 팁들을 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_rl_tips():\n",
    "    \"\"\"Deep RL 실전 팁\"\"\"\n",
    "    \n",
    "    tips = \"\"\"\n",
    "    🎯 Deep RL 실전 팁\n",
    "    ==================\n",
    "    \n",
    "    1. 하이퍼파라미터 튜닝\n",
    "    ----------------------\n",
    "    • Learning Rate: 너무 크면 발산, 너무 작으면 학습 안됨\n",
    "    • Batch Size: 클수록 안정적이지만 느림\n",
    "    • Network Size: 문제 복잡도에 맞게 조절\n",
    "    • Epsilon Decay: 탐색-활용 균형 중요\n",
    "    \n",
    "    2. 디버깅 전략\n",
    "    --------------\n",
    "    • 간단한 환경부터 시작 (CartPole → LunarLander → Atari)\n",
    "    • 보상 스케일링 확인\n",
    "    • 그래디언트 크기 모니터링\n",
    "    • 학습 곡선 시각화\n",
    "    \n",
    "    3. 안정성 개선\n",
    "    --------------\n",
    "    • Gradient Clipping 사용\n",
    "    • 보상 정규화 (reward normalization)\n",
    "    • Advantage 정규화\n",
    "    • Learning rate scheduling\n",
    "    \n",
    "    4. 성능 개선\n",
    "    ------------\n",
    "    • Parallel environments 사용\n",
    "    • GPU 활용\n",
    "    • Vectorized operations\n",
    "    • JIT compilation (PyTorch 2.0+)\n",
    "    \n",
    "    5. 일반적인 실수\n",
    "    ----------------\n",
    "    ❌ 너무 복잡한 네트워크로 시작\n",
    "    ❌ 하이퍼파라미터 동시에 여러 개 변경\n",
    "    ❌ 충분한 탐색 없이 활용만\n",
    "    ❌ 보상 함수 잘못 설계\n",
    "    \n",
    "    6. 권장 사항\n",
    "    ------------\n",
    "    ✅ 작은 네트워크로 시작\n",
    "    ✅ 한 번에 하나씩 변경\n",
    "    ✅ 로깅과 시각화 철저히\n",
    "    ✅ 재현 가능성 위해 시드 고정\n",
    "    \"\"\"\n",
    "    \n",
    "    print(tips)\n",
    "    \n",
    "    # 하이퍼파라미터 영향 시각화\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Learning Rate 영향\n",
    "    ax = axes[0, 0]\n",
    "    lrs = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    colors = ['blue', 'green', 'orange', 'red']\n",
    "    for lr, color in zip(lrs, colors):\n",
    "        if lr == 1e-3:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 1)\n",
    "        elif lr == 1e-4:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 0.5)\n",
    "        elif lr == 1e-2:\n",
    "            curve = np.cumsum(np.random.randn(100) * 3 + 0.8)\n",
    "        else:\n",
    "            curve = np.cumsum(np.random.randn(100) * 10 - 2)\n",
    "        ax.plot(curve, label=f'LR={lr}', color=color)\n",
    "    ax.set_title('Learning Rate 영향')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Batch Size 영향\n",
    "    ax = axes[0, 1]\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    for bs, color in zip(batch_sizes, colors):\n",
    "        noise = 50 / bs\n",
    "        curve = np.cumsum(np.random.randn(100) * noise + 1)\n",
    "        ax.plot(curve, label=f'Batch={bs}', color=color)\n",
    "    ax.set_title('Batch Size 영향')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Network Size 영향\n",
    "    ax = axes[1, 0]\n",
    "    sizes = ['Small (32)', 'Medium (128)', 'Large (512)', 'Huge (2048)']\n",
    "    for size, color in zip(sizes, colors):\n",
    "        if 'Small' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 0.7)\n",
    "        elif 'Medium' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2 + 1)\n",
    "        elif 'Large' in size:\n",
    "            curve = np.cumsum(np.random.randn(100) * 2.5 + 0.9)\n",
    "        else:\n",
    "            curve = np.cumsum(np.random.randn(100) * 3 + 0.6)  # Overfitting\n",
    "        ax.plot(curve, label=size, color=color)\n",
    "    ax.set_title('Network Size 영향')\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Epsilon Decay 영향\n",
    "    ax = axes[1, 1]\n",
    "    epsilons = np.linspace(0, 100, 100)\n",
    "    ax.plot(np.exp(-epsilons/20), label='Fast decay', color='red')\n",
    "    ax.plot(np.exp(-epsilons/50), label='Medium decay', color='green')\n",
    "    ax.plot(np.exp(-epsilons/100), label='Slow decay', color='blue')\n",
    "    ax.plot(np.ones(100) * 0.1, label='Constant', color='orange', linestyle='--')\n",
    "    ax.set_title('Epsilon Decay 전략')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Epsilon')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "deep_rl_tips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 요약 및 핵심 개념\n",
    "\n",
    "### 이번 노트북에서 배운 내용\n",
    "\n",
    "1. **Deep Q-Network (DQN)**\n",
    "   - Neural Network로 Q 함수 근사\n",
    "   - Experience Replay: 데이터 효율성\n",
    "   - Target Network: 학습 안정성\n",
    "   - Double DQN: 과대평가 해결\n",
    "\n",
    "2. **Policy Gradient Methods**\n",
    "   - REINFORCE: 기본 policy gradient\n",
    "   - 직접 정책 최적화\n",
    "   - 확률적 정책 학습\n",
    "\n",
    "3. **Actor-Critic**\n",
    "   - Policy + Value 결합\n",
    "   - Advantage 함수 활용\n",
    "   - 분산 감소\n",
    "\n",
    "4. **PPO**\n",
    "   - Trust Region 방법\n",
    "   - Clipped objective\n",
    "   - 현대 RL의 주력 알고리즘\n",
    "\n",
    "### 알고리즘 선택 가이드\n",
    "\n",
    "| 상황 | 추천 알고리즘 | 이유 |\n",
    "|------|--------------|------|\n",
    "| 이산 행동, 간단한 환경 | DQN | 샘플 효율적, 안정적 |\n",
    "| 연속 행동 공간 | PPO, SAC | 연속 행동 직접 처리 |\n",
    "| 복잡한 환경 | PPO | 안정성과 성능 균형 |\n",
    "| 로봇 제어 | TD3, SAC | 연속 제어 특화 |\n",
    "| 멀티에이전트 | MAPPO | 협력 학습 |\n",
    "\n",
    "### 다음 노트북 예고\n",
    "**Notebook 4: 추론 기반 RL - Planning과 ReAct**\n",
    "- Model-based planning\n",
    "- MCTS와 AlphaZero\n",
    "- ReAct: Reasoning + Acting\n",
    "- LLM과 RL의 만남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 체크포인트\n",
    "print(\"🎯 학습 완료 체크리스트:\")\n",
    "print(\"✅ DQN 구현 및 이해\")\n",
    "print(\"✅ Experience Replay와 Target Network\")\n",
    "print(\"✅ Policy Gradient (REINFORCE) 구현\")\n",
    "print(\"✅ Actor-Critic 이해\")\n",
    "print(\"✅ PPO 구현\")\n",
    "print(\"✅ 알고리즘 비교 및 선택 기준\")\n",
    "print(\"✅ Deep RL 실전 팁\")\n",
    "print(\"\\n🚀 다음 단계: 추론(Reasoning)을 활용한 지능적 RL!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}