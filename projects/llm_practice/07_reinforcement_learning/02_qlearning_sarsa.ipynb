{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: 전통 RL - Q-learning과 SARSA\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- Model-free 강화학습 이해\n",
    "- Temporal Difference (TD) 학습 구현\n",
    "- Q-learning과 SARSA 알고리즘 마스터\n",
    "- On-policy vs Off-policy 차이 이해\n",
    "- 실제 에이전트 훈련 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model-free 강화학습\n",
    "\n",
    "### Model-based vs Model-free\n",
    "\n",
    "**Model-based (이전 노트북)**\n",
    "- 환경의 전이 확률 P(s'|s,a)를 알고 있음\n",
    "- 동적 프로그래밍으로 최적 정책 계산\n",
    "- 예: Value Iteration, Policy Iteration\n",
    "\n",
    "**Model-free (이번 노트북)**\n",
    "- 환경 모델을 모름\n",
    "- 경험(experience)으로부터 직접 학습\n",
    "- 예: Q-learning, SARSA, Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Q-learning과 SARSA 학습 환경 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 구축: 미로 탐험\n",
    "\n",
    "열쇠를 찾아 문을 열고 목표에 도달하는 미로 환경을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "class MazeEnv:\n",
    "    \"\"\"미로 환경\"\"\"\n",
    "    \n",
    "    def __init__(self, maze_type='simple'):\n",
    "        self.maze_type = maze_type\n",
    "        self.reset()\n",
    "        \n",
    "    def _create_maze(self):\n",
    "        \"\"\"미로 생성\"\"\"\n",
    "        if self.maze_type == 'simple':\n",
    "            # 5x5 간단한 미로\n",
    "            self.maze = [\n",
    "                ['S', '.', '.', '.', '.'],\n",
    "                ['.', '#', '#', '#', '.'],\n",
    "                ['.', '.', '.', '.', '.'],\n",
    "                ['.', '#', '#', '#', '.'],\n",
    "                ['.', '.', '.', '.', 'G']\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (4, 4)\n",
    "            self.key_pos = None\n",
    "            self.door_pos = None\n",
    "            \n",
    "        elif self.maze_type == 'key_door':\n",
    "            # 7x7 열쇠-문 미로\n",
    "            self.maze = [\n",
    "                ['S', '.', '#', '.', '.', '.', '.'],\n",
    "                ['.', '.', '#', '.', '#', '#', '.'],\n",
    "                ['.', '.', '#', '.', '#', 'K', '.'],\n",
    "                ['.', '.', 'D', '.', '#', '#', '.'],\n",
    "                ['.', '.', '#', '.', '.', '.', '.'],\n",
    "                ['.', '.', '#', '#', '#', '.', '.'],\n",
    "                ['.', '.', '.', '.', '.', '.', 'G']\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (6, 6)\n",
    "            self.key_pos = (2, 5)\n",
    "            self.door_pos = (3, 2)\n",
    "            \n",
    "        self.height = len(self.maze)\n",
    "        self.width = len(self.maze[0])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"환경 초기화\"\"\"\n",
    "        self._create_maze()\n",
    "        self.agent_pos = self.start\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.height * self.width * 4\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"현재 상태 반환\"\"\"\n",
    "        # 상태 = (위치, 열쇠 소지 여부)\n",
    "        return (self.agent_pos[0], self.agent_pos[1], int(self.has_key))\n",
    "    \n",
    "    def step(self, action: Action):\n",
    "        \"\"\"행동 수행\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # 행동에 따른 이동\n",
    "        y, x = self.agent_pos\n",
    "        if action == Action.UP:\n",
    "            new_pos = (y-1, x)\n",
    "        elif action == Action.DOWN:\n",
    "            new_pos = (y+1, x)\n",
    "        elif action == Action.LEFT:\n",
    "            new_pos = (y, x-1)\n",
    "        elif action == Action.RIGHT:\n",
    "            new_pos = (y, x+1)\n",
    "        \n",
    "        # 이동 가능 확인\n",
    "        if self._is_valid_move(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        \n",
    "        # 열쇠 획득\n",
    "        if self.key_pos and self.agent_pos == self.key_pos and not self.has_key:\n",
    "            self.has_key = True\n",
    "        \n",
    "        # 보상 계산\n",
    "        reward = self._get_reward()\n",
    "        \n",
    "        # 종료 조건\n",
    "        done = (self.agent_pos == self.goal) or (self.steps >= self.max_steps)\n",
    "        \n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def _is_valid_move(self, pos):\n",
    "        \"\"\"이동 가능 여부 확인\"\"\"\n",
    "        y, x = pos\n",
    "        \n",
    "        # 경계 확인\n",
    "        if y < 0 or y >= self.height or x < 0 or x >= self.width:\n",
    "            return False\n",
    "        \n",
    "        # 벽 확인\n",
    "        if self.maze[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # 문 확인 (열쇠 없으면 통과 불가)\n",
    "        if self.door_pos and pos == self.door_pos and not self.has_key:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\"보상 반환\"\"\"\n",
    "        if self.agent_pos == self.goal:\n",
    "            return 100.0  # 목표 도달\n",
    "        elif self.key_pos and self.agent_pos == self.key_pos and self.has_key:\n",
    "            return 10.0  # 열쇠 획득\n",
    "        else:\n",
    "            return -1.0  # 각 스텝마다 패널티\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"환경 시각화\"\"\"\n",
    "        display = []\n",
    "        for y in range(self.height):\n",
    "            row = []\n",
    "            for x in range(self.width):\n",
    "                if (y, x) == self.agent_pos:\n",
    "                    row.append('A')\n",
    "                elif self.maze[y][x] == 'K' and not self.has_key:\n",
    "                    row.append('K')\n",
    "                elif self.maze[y][x] == 'D':\n",
    "                    row.append('D' if not self.has_key else '.')\n",
    "                else:\n",
    "                    row.append(self.maze[y][x])\n",
    "            display.append(' '.join(row))\n",
    "        \n",
    "        print('\\n'.join(display))\n",
    "        if self.has_key:\n",
    "            print(\"🔑 열쇠 보유 중\")\n",
    "\n",
    "# 환경 테스트\n",
    "env = MazeEnv('simple')\n",
    "print(\"간단한 미로:\")\n",
    "env.render()\n",
    "\n",
    "env = MazeEnv('key_door')\n",
    "print(\"\\n열쇠-문 미로:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Difference (TD) 학습\n",
    "\n",
    "TD 학습은 **현재 추정값**과 **다음 추정값**의 차이를 이용해 학습합니다.\n",
    "\n",
    "### TD Error\n",
    "$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### TD Update\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot \\delta_t$$\n",
    "\n",
    "여기서 α는 학습률(learning rate)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "    \"\"\"기본 TD 학습 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.99):\n",
    "        self.alpha = alpha  # 학습률\n",
    "        self.gamma = gamma  # 할인 인자\n",
    "        self.V = defaultdict(float)  # 상태 가치 함수\n",
    "        self.returns = defaultdict(list)  # 에피소드별 리턴\n",
    "    \n",
    "    def update(self, state, reward, next_state):\n",
    "        \"\"\"TD(0) 업데이트\"\"\"\n",
    "        td_error = reward + self.gamma * self.V[next_state] - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "        return td_error\n",
    "\n",
    "# TD 학습 데모\n",
    "def demonstrate_td_learning():\n",
    "    \"\"\"TD 학습 과정 시각화\"\"\"\n",
    "    agent = TDAgent(alpha=0.1, gamma=0.9)\n",
    "    \n",
    "    # 간단한 체인 환경 (1D)\n",
    "    states = ['A', 'B', 'C', 'D', 'E']\n",
    "    rewards = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 10}\n",
    "    \n",
    "    # 여러 에피소드 시뮬레이션\n",
    "    history = {s: [] for s in states}\n",
    "    \n",
    "    for episode in range(100):\n",
    "        # 랜덤 시작\n",
    "        path = ['A', 'B', 'C', 'D', 'E']\n",
    "        \n",
    "        for i in range(len(path)-1):\n",
    "            state = path[i]\n",
    "            next_state = path[i+1]\n",
    "            reward = rewards[next_state]\n",
    "            \n",
    "            agent.update(state, reward, next_state)\n",
    "        \n",
    "        # 기록\n",
    "        for s in states:\n",
    "            history[s].append(agent.V[s])\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for s in states:\n",
    "        plt.plot(history[s], label=f'V({s})', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('State Value')\n",
    "    plt.title('TD Learning: 상태 가치 함수 수렴')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"최종 상태 가치:\")\n",
    "    for s in states:\n",
    "        print(f\"V({s}) = {agent.V[s]:.2f}\")\n",
    "\n",
    "demonstrate_td_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-Learning: Off-policy TD Control\n",
    "\n",
    "Q-learning은 **off-policy** 알고리즘으로, 행동 정책과 무관하게 최적 Q 함수를 학습합니다.\n",
    "\n",
    "### Q-Learning 업데이트 규칙\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### 특징\n",
    "- **Off-policy**: 탐색 정책과 무관하게 최적 정책 학습\n",
    "- **Max operator**: 다음 상태에서 최대 Q값 사용\n",
    "- **수렴 보장**: 적절한 조건 하에서 Q* 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # 학습 기록\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"ε-greedy 행동 선택\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            # 동점일 때 랜덤 선택\n",
    "            max_q = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q)[0]\n",
    "            return np.random.choice(actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning 업데이트\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        td_error = target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        self.td_errors.append(abs(td_error))\n",
    "        return td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"한 에피소드 학습\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_lengths.append(steps)\n",
    "        \n",
    "        return total_reward, steps\n",
    "\n",
    "# Q-Learning 학습\n",
    "def train_qlearning(env_type='simple', n_episodes=500):\n",
    "    env = MazeEnv(env_type)\n",
    "    agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    \n",
    "    print(f\"Q-Learning 학습 시작 ({env_type} maze)...\")\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-100:])\n",
    "            avg_steps = np.mean(agent.episode_lengths[-100:])\n",
    "            print(f\"Episode {episode+1}: 평균 보상 = {avg_reward:.2f}, 평균 스텝 = {avg_steps:.1f}\")\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# 학습 실행\n",
    "q_agent, q_env = train_qlearning('simple', n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SARSA: On-policy TD Control\n",
    "\n",
    "SARSA는 **on-policy** 알고리즘으로, 실제 따르는 정책을 학습합니다.\n",
    "\n",
    "### SARSA 업데이트 규칙\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### 특징\n",
    "- **On-policy**: 실제 행동 정책을 학습\n",
    "- **더 안전한 학습**: 탐색 중 위험 회피\n",
    "- **이름 유래**: State-Action-Reward-State-Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"SARSA 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # 학습 기록\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"ε-greedy 행동 선택\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            max_q = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q)[0]\n",
    "            return np.random.choice(actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"SARSA 업데이트\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * self.Q[next_state][next_action]\n",
    "        \n",
    "        td_error = target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        self.td_errors.append(abs(td_error))\n",
    "        return td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"한 에피소드 학습\"\"\"\n",
    "        state = env.reset()\n",
    "        action = self.get_action(state)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            next_action = self.get_action(next_state)\n",
    "            \n",
    "            self.update(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_lengths.append(steps)\n",
    "        \n",
    "        return total_reward, steps\n",
    "\n",
    "# SARSA 학습\n",
    "def train_sarsa(env_type='simple', n_episodes=500):\n",
    "    env = MazeEnv(env_type)\n",
    "    agent = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    \n",
    "    print(f\"SARSA 학습 시작 ({env_type} maze)...\")\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-100:])\n",
    "            avg_steps = np.mean(agent.episode_lengths[-100:])\n",
    "            print(f\"Episode {episode+1}: 평균 보상 = {avg_reward:.2f}, 평균 스텝 = {avg_steps:.1f}\")\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# 학습 실행\n",
    "sarsa_agent, sarsa_env = train_sarsa('simple', n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q-Learning vs SARSA 비교\n",
    "\n",
    "두 알고리즘의 차이를 실험적으로 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms():\n",
    "    \"\"\"Q-Learning과 SARSA 성능 비교\"\"\"\n",
    "    \n",
    "    # 위험한 환경 생성 (절벽 걷기)\n",
    "    class CliffWalkingEnv:\n",
    "        def __init__(self):\n",
    "            self.width = 12\n",
    "            self.height = 4\n",
    "            self.start = (3, 0)\n",
    "            self.goal = (3, 11)\n",
    "            self.agent_pos = self.start\n",
    "            self.cliff = [(3, x) for x in range(1, 11)]  # 절벽 위치\n",
    "        \n",
    "        def reset(self):\n",
    "            self.agent_pos = self.start\n",
    "            return self.agent_pos\n",
    "        \n",
    "        def step(self, action):\n",
    "            y, x = self.agent_pos\n",
    "            \n",
    "            if action == Action.UP:\n",
    "                y = max(0, y - 1)\n",
    "            elif action == Action.DOWN:\n",
    "                y = min(self.height - 1, y + 1)\n",
    "            elif action == Action.LEFT:\n",
    "                x = max(0, x - 1)\n",
    "            elif action == Action.RIGHT:\n",
    "                x = min(self.width - 1, x + 1)\n",
    "            \n",
    "            self.agent_pos = (y, x)\n",
    "            \n",
    "            # 보상 계산\n",
    "            if self.agent_pos in self.cliff:\n",
    "                reward = -100\n",
    "                self.agent_pos = self.start  # 떨어지면 시작점으로\n",
    "                done = False\n",
    "            elif self.agent_pos == self.goal:\n",
    "                reward = 0\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1\n",
    "                done = False\n",
    "            \n",
    "            return self.agent_pos, reward, done, {}\n",
    "    \n",
    "    # 두 알고리즘으로 학습\n",
    "    n_episodes = 500\n",
    "    n_runs = 10\n",
    "    \n",
    "    q_rewards_all = []\n",
    "    sarsa_rewards_all = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Q-Learning\n",
    "        env = CliffWalkingEnv()\n",
    "        q_agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = q_agent.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(Action(action))\n",
    "                q_agent.update(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            q_agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        q_rewards_all.append(q_agent.episode_rewards)\n",
    "        \n",
    "        # SARSA\n",
    "        env = CliffWalkingEnv()\n",
    "        sarsa_agent = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            action = sarsa_agent.get_action(state)\n",
    "            total_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                next_state, reward, done, _ = env.step(Action(action))\n",
    "                next_action = sarsa_agent.get_action(next_state)\n",
    "                sarsa_agent.update(state, action, reward, next_state, next_action, done)\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            sarsa_agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        sarsa_rewards_all.append(sarsa_agent.episode_rewards)\n",
    "    \n",
    "    # 평균 계산\n",
    "    q_rewards_mean = np.mean(q_rewards_all, axis=0)\n",
    "    sarsa_rewards_mean = np.mean(sarsa_rewards_all, axis=0)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 이동 평균 계산 (window=10)\n",
    "    window = 10\n",
    "    q_smooth = np.convolve(q_rewards_mean, np.ones(window)/window, mode='valid')\n",
    "    sarsa_smooth = np.convolve(sarsa_rewards_mean, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    plt.plot(q_smooth, label='Q-Learning', linewidth=2, color='blue')\n",
    "    plt.plot(sarsa_smooth, label='SARSA', linewidth=2, color='red')\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Q-Learning vs SARSA: Cliff Walking')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n분석 결과:\")\n",
    "    print(f\"Q-Learning 최종 평균 보상: {np.mean(q_rewards_mean[-50:]):.2f}\")\n",
    "    print(f\"SARSA 최종 평균 보상: {np.mean(sarsa_rewards_mean[-50:]):.2f}\")\n",
    "    print(\"\\n💡 인사이트:\")\n",
    "    print(\"- Q-Learning은 최적 경로를 학습 (절벽 가장자리)\")\n",
    "    print(\"- SARSA는 안전한 경로를 학습 (절벽에서 멀리)\")\n",
    "    print(\"- On-policy vs Off-policy의 차이를 보여줌\")\n",
    "\n",
    "compare_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. N-step TD와 TD(λ)\n",
    "\n",
    "### N-step TD\n",
    "한 스텝이 아닌 N 스텝 앞을 보고 업데이트:\n",
    "$$G_t^{(n)} = r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1} r_{t+n} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "### TD(λ)\n",
    "모든 n-step 리턴의 가중 평균:\n",
    "$$G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepQLearning:\n",
    "    \"\"\"N-step Q-Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, n_steps=3, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_steps = n_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # N-step 버퍼\n",
    "        self.state_buffer = deque(maxlen=n_steps)\n",
    "        self.action_buffer = deque(maxlen=n_steps)\n",
    "        self.reward_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, done=False, next_state=None):\n",
    "        \"\"\"N-step 업데이트\"\"\"\n",
    "        if len(self.state_buffer) < self.n_steps and not done:\n",
    "            return\n",
    "        \n",
    "        # N-step 리턴 계산\n",
    "        G = 0\n",
    "        for i in range(len(self.reward_buffer)):\n",
    "            G += (self.gamma ** i) * self.reward_buffer[i]\n",
    "        \n",
    "        if not done and next_state is not None:\n",
    "            G += (self.gamma ** len(self.reward_buffer)) * np.max(self.Q[next_state])\n",
    "        \n",
    "        # 첫 번째 상태-행동 쌍 업데이트\n",
    "        state = self.state_buffer[0]\n",
    "        action = self.action_buffer[0]\n",
    "        \n",
    "        td_error = G - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # 버퍼 초기화\n",
    "        self.state_buffer.clear()\n",
    "        self.action_buffer.clear()\n",
    "        self.reward_buffer.clear()\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            # 버퍼에 추가\n",
    "            self.state_buffer.append(state)\n",
    "            self.action_buffer.append(action)\n",
    "            self.reward_buffer.append(reward)\n",
    "            \n",
    "            # N-step 업데이트\n",
    "            self.update(done, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # 남은 버퍼 처리\n",
    "                while len(self.state_buffer) > 0:\n",
    "                    self.update(done=True)\n",
    "                    self.state_buffer.popleft()\n",
    "                    self.action_buffer.popleft()\n",
    "                    self.reward_buffer.popleft()\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "# N-step 비교 실험\n",
    "def compare_n_steps():\n",
    "    \"\"\"다양한 N 값 비교\"\"\"\n",
    "    n_values = [1, 3, 5, 10]\n",
    "    results = {}\n",
    "    \n",
    "    for n in n_values:\n",
    "        env = MazeEnv('simple')\n",
    "        agent = NStepQLearning(n_steps=n)\n",
    "        rewards = []\n",
    "        \n",
    "        print(f\"\\nTraining {n}-step Q-Learning...\")\n",
    "        for episode in tqdm(range(300)):\n",
    "            reward = agent.train_episode(env)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        results[n] = rewards\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for n in n_values:\n",
    "        # 이동 평균\n",
    "        window = 20\n",
    "        smooth = np.convolve(results[n], np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smooth, label=f'{n}-step', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('N-step Q-Learning 비교')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 N-step 효과:\")\n",
    "    print(\"- N=1: 기본 Q-learning (빠른 업데이트, 높은 분산)\")\n",
    "    print(\"- N=3-5: 균형잡힌 성능\")\n",
    "    print(\"- N=10: Monte Carlo에 가까움 (느린 업데이트, 낮은 분산)\")\n",
    "\n",
    "compare_n_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실전 적용: 열쇠-문 미로 해결\n",
    "\n",
    "복잡한 열쇠-문 미로를 Q-learning으로 해결해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values(agent, env):\n",
    "    \"\"\"Q 값 시각화\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "    for action_idx, (ax, action_name) in enumerate(zip(axes.flat, action_names)):\n",
    "        q_grid = np.zeros((env.height, env.width))\n",
    "        \n",
    "        for y in range(env.height):\n",
    "            for x in range(env.width):\n",
    "                state = (y, x, 0)  # 열쇠 없는 상태\n",
    "                q_grid[y, x] = agent.Q[state][action_idx]\n",
    "        \n",
    "        im = ax.imshow(q_grid, cmap='coolwarm', aspect='auto')\n",
    "        ax.set_title(f'Q-values for {action_name}')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # 벽 표시\n",
    "        for y in range(env.height):\n",
    "            for x in range(env.width):\n",
    "                if env.maze[y][x] == '#':\n",
    "                    ax.add_patch(plt.Rectangle((x-0.5, y-0.5), 1, 1, \n",
    "                                              fill=True, color='black', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_agent(agent, env, n_episodes=10, visualize=False):\n",
    "    \"\"\"학습된 에이전트 테스트\"\"\"\n",
    "    success_count = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        path = []\n",
    "        \n",
    "        while steps < env.max_steps:\n",
    "            action = agent.get_action(state, training=False)\n",
    "            path.append((state, Action(action)))\n",
    "            \n",
    "            state, reward, done, _ = env.step(Action(action))\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if env.agent_pos == env.goal:\n",
    "                    success_count += 1\n",
    "                break\n",
    "        \n",
    "        total_steps += steps\n",
    "        \n",
    "        if visualize and episode == 0:\n",
    "            print(f\"\\n에피소드 {episode+1} 경로:\")\n",
    "            env.reset()\n",
    "            for state, action in path[:20]:  # 처음 20스텝만\n",
    "                print(f\"State: {state}, Action: {action.name}\")\n",
    "    \n",
    "    success_rate = success_count / n_episodes * 100\n",
    "    avg_steps = total_steps / n_episodes\n",
    "    \n",
    "    print(f\"\\n테스트 결과:\")\n",
    "    print(f\"성공률: {success_rate:.1f}%\")\n",
    "    print(f\"평균 스텝: {avg_steps:.1f}\")\n",
    "    \n",
    "    return success_rate, avg_steps\n",
    "\n",
    "# 열쇠-문 미로 학습\n",
    "print(\"복잡한 열쇠-문 미로 학습...\")\n",
    "complex_env = MazeEnv('key_door')\n",
    "complex_agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "\n",
    "# 학습\n",
    "for episode in tqdm(range(1000)):\n",
    "    complex_agent.train_episode(complex_env)\n",
    "    \n",
    "    # Epsilon decay\n",
    "    if episode % 100 == 0:\n",
    "        complex_agent.epsilon *= 0.9\n",
    "\n",
    "# 학습 곡선\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "window = 50\n",
    "rewards_smooth = np.convolve(complex_agent.episode_rewards, \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "plt.plot(rewards_smooth)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('학습 곡선')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "steps_smooth = np.convolve(complex_agent.episode_lengths, \n",
    "                          np.ones(window)/window, mode='valid')\n",
    "plt.plot(steps_smooth)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.title('에피소드 길이')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Q 값 시각화\n",
    "print(\"\\nQ 값 히트맵:\")\n",
    "visualize_q_values(complex_agent, complex_env)\n",
    "\n",
    "# 테스트\n",
    "test_agent(complex_agent, complex_env, n_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 고급 기법: Double Q-Learning\n",
    "\n",
    "Q-learning의 과대평가(overestimation) 문제를 해결하는 Double Q-Learning을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearning:\n",
    "    \"\"\"Double Q-Learning 에이전트\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # 두 개의 Q 함수\n",
    "        self.Q1 = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.Q2 = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"평균 Q 값으로 행동 선택\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            # 두 Q 함수의 평균 사용\n",
    "            q_values = (self.Q1[state] + self.Q2[state]) / 2\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Double Q-learning 업데이트\"\"\"\n",
    "        # 50% 확률로 Q1 또는 Q2 업데이트\n",
    "        if random.random() < 0.5:\n",
    "            # Q1 업데이트\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Q1으로 행동 선택, Q2로 평가\n",
    "                best_action = np.argmax(self.Q1[next_state])\n",
    "                target = reward + self.gamma * self.Q2[next_state][best_action]\n",
    "            \n",
    "            td_error = target - self.Q1[state][action]\n",
    "            self.Q1[state][action] += self.alpha * td_error\n",
    "        else:\n",
    "            # Q2 업데이트\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Q2로 행동 선택, Q1으로 평가\n",
    "                best_action = np.argmax(self.Q2[next_state])\n",
    "                target = reward + self.gamma * self.Q1[next_state][best_action]\n",
    "            \n",
    "            td_error = target - self.Q2[state][action]\n",
    "            self.Q2[state][action] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "\n",
    "# Double Q-Learning vs Q-Learning 비교\n",
    "def compare_double_q():\n",
    "    \"\"\"Double Q-Learning 효과 비교\"\"\"\n",
    "    env_q = MazeEnv('key_door')\n",
    "    env_double = MazeEnv('key_door')\n",
    "    \n",
    "    q_agent = QLearningAgent()\n",
    "    double_agent = DoubleQLearning()\n",
    "    \n",
    "    n_episodes = 500\n",
    "    \n",
    "    print(\"학습 중...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        q_agent.train_episode(env_q)\n",
    "        double_agent.train_episode(env_double)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    window = 20\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    q_smooth = np.convolve(q_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    double_smooth = np.convolve(double_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    plt.plot(q_smooth, label='Q-Learning', alpha=0.7)\n",
    "    plt.plot(double_smooth, label='Double Q-Learning', alpha=0.7)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('학습 성능 비교')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Q 값 분산 비교\n",
    "    q_values = []\n",
    "    double_values = []\n",
    "    \n",
    "    for state in q_agent.Q.keys():\n",
    "        q_values.extend(q_agent.Q[state])\n",
    "    \n",
    "    for state in double_agent.Q1.keys():\n",
    "        double_values.extend((double_agent.Q1[state] + double_agent.Q2[state]) / 2)\n",
    "    \n",
    "    plt.hist(q_values, bins=30, alpha=0.5, label='Q-Learning', density=True)\n",
    "    plt.hist(double_values, bins=30, alpha=0.5, label='Double Q-Learning', density=True)\n",
    "    plt.xlabel('Q-values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Q 값 분포')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 Double Q-Learning 효과:\")\n",
    "    print(\"- 과대평가(overestimation) 감소\")\n",
    "    print(\"- 더 안정적인 학습\")\n",
    "    print(\"- Q 값 분산 감소\")\n",
    "\n",
    "compare_double_q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 요약 및 핵심 개념\n",
    "\n",
    "### 이번 노트북에서 배운 내용\n",
    "\n",
    "1. **Temporal Difference (TD) 학습**\n",
    "   - 부트스트래핑: 추정값으로 추정값 업데이트\n",
    "   - TD error를 통한 온라인 학습\n",
    "\n",
    "2. **Q-Learning**\n",
    "   - Off-policy: 최적 정책 직접 학습\n",
    "   - Max operator 사용\n",
    "   - 탐색과 무관하게 최적 Q* 수렴\n",
    "\n",
    "3. **SARSA**\n",
    "   - On-policy: 실제 따르는 정책 학습\n",
    "   - 더 안전한 학습 (위험 회피)\n",
    "   - 실제 행동 고려\n",
    "\n",
    "4. **고급 기법**\n",
    "   - N-step TD: 여러 스텝 고려\n",
    "   - Double Q-Learning: 과대평가 해결\n",
    "   - ε-greedy 전략과 탐색\n",
    "\n",
    "### Q-Learning vs SARSA 핵심 차이\n",
    "\n",
    "| 특성 | Q-Learning | SARSA |\n",
    "|-----|-----------|--------|\n",
    "| 정책 유형 | Off-policy | On-policy |\n",
    "| 업데이트 | max Q(s',a') | Q(s',a') |\n",
    "| 최적성 | 최적 정책 학습 | 실제 정책 학습 |\n",
    "| 안전성 | 위험 감수 | 위험 회피 |\n",
    "| 수렴성 | Q*로 수렴 | 정책의 Q로 수렴 |\n",
    "\n",
    "### 다음 노트북 예고\n",
    "**Notebook 3: Deep RL - DQN과 PPO**\n",
    "- Neural Network + Q-Learning = DQN\n",
    "- Experience Replay와 Target Network\n",
    "- Policy Gradient Methods\n",
    "- PPO: 현대 RL의 주력 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 체크포인트\n",
    "print(\"🎯 학습 완료 체크리스트:\")\n",
    "print(\"✅ TD 학습 이해\")\n",
    "print(\"✅ Q-Learning 구현\")\n",
    "print(\"✅ SARSA 구현\")\n",
    "print(\"✅ On-policy vs Off-policy 차이 이해\")\n",
    "print(\"✅ N-step TD 구현\")\n",
    "print(\"✅ Double Q-Learning 구현\")\n",
    "print(\"✅ 복잡한 환경에서 에이전트 훈련\")\n",
    "print(\"\\n🚀 다음 단계: Deep Learning과 RL의 결합!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}