{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: ì „í†µ RL - Q-learningê³¼ SARSA\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- Model-free ê°•í™”í•™ìŠµ ì´í•´\n",
    "- Temporal Difference (TD) í•™ìŠµ êµ¬í˜„\n",
    "- Q-learningê³¼ SARSA ì•Œê³ ë¦¬ì¦˜ ë§ˆìŠ¤í„°\n",
    "- On-policy vs Off-policy ì°¨ì´ ì´í•´\n",
    "- ì‹¤ì œ ì—ì´ì „íŠ¸ í›ˆë ¨ ë° í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model-free ê°•í™”í•™ìŠµ\n",
    "\n",
    "### Model-based vs Model-free\n",
    "\n",
    "**Model-based (ì´ì „ ë…¸íŠ¸ë¶)**\n",
    "- í™˜ê²½ì˜ ì „ì´ í™•ë¥  P(s'|s,a)ë¥¼ ì•Œê³  ìˆìŒ\n",
    "- ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ì •ì±… ê³„ì‚°\n",
    "- ì˜ˆ: Value Iteration, Policy Iteration\n",
    "\n",
    "**Model-free (ì´ë²ˆ ë…¸íŠ¸ë¶)**\n",
    "- í™˜ê²½ ëª¨ë¸ì„ ëª¨ë¦„\n",
    "- ê²½í—˜(experience)ìœ¼ë¡œë¶€í„° ì§ì ‘ í•™ìŠµ\n",
    "- ì˜ˆ: Q-learning, SARSA, Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Q-learningê³¼ SARSA í•™ìŠµ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ êµ¬ì¶•: ë¯¸ë¡œ íƒí—˜\n",
    "\n",
    "ì—´ì‡ ë¥¼ ì°¾ì•„ ë¬¸ì„ ì—´ê³  ëª©í‘œì— ë„ë‹¬í•˜ëŠ” ë¯¸ë¡œ í™˜ê²½ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "class MazeEnv:\n",
    "    \"\"\"ë¯¸ë¡œ í™˜ê²½\"\"\"\n",
    "    \n",
    "    def __init__(self, maze_type='simple'):\n",
    "        self.maze_type = maze_type\n",
    "        self.reset()\n",
    "        \n",
    "    def _create_maze(self):\n",
    "        \"\"\"ë¯¸ë¡œ ìƒì„±\"\"\"\n",
    "        if self.maze_type == 'simple':\n",
    "            # 5x5 ê°„ë‹¨í•œ ë¯¸ë¡œ\n",
    "            self.maze = [\n",
    "                ['S', '.', '.', '.', '.'],\n",
    "                ['.', '#', '#', '#', '.'],\n",
    "                ['.', '.', '.', '.', '.'],\n",
    "                ['.', '#', '#', '#', '.'],\n",
    "                ['.', '.', '.', '.', 'G']\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (4, 4)\n",
    "            self.key_pos = None\n",
    "            self.door_pos = None\n",
    "            \n",
    "        elif self.maze_type == 'key_door':\n",
    "            # 7x7 ì—´ì‡ -ë¬¸ ë¯¸ë¡œ\n",
    "            self.maze = [\n",
    "                ['S', '.', '#', '.', '.', '.', '.'],\n",
    "                ['.', '.', '#', '.', '#', '#', '.'],\n",
    "                ['.', '.', '#', '.', '#', 'K', '.'],\n",
    "                ['.', '.', 'D', '.', '#', '#', '.'],\n",
    "                ['.', '.', '#', '.', '.', '.', '.'],\n",
    "                ['.', '.', '#', '#', '#', '.', '.'],\n",
    "                ['.', '.', '.', '.', '.', '.', 'G']\n",
    "            ]\n",
    "            self.start = (0, 0)\n",
    "            self.goal = (6, 6)\n",
    "            self.key_pos = (2, 5)\n",
    "            self.door_pos = (3, 2)\n",
    "            \n",
    "        self.height = len(self.maze)\n",
    "        self.width = len(self.maze[0])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"í™˜ê²½ ì´ˆê¸°í™”\"\"\"\n",
    "        self._create_maze()\n",
    "        self.agent_pos = self.start\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        self.max_steps = self.height * self.width * 4\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"í˜„ì¬ ìƒíƒœ ë°˜í™˜\"\"\"\n",
    "        # ìƒíƒœ = (ìœ„ì¹˜, ì—´ì‡  ì†Œì§€ ì—¬ë¶€)\n",
    "        return (self.agent_pos[0], self.agent_pos[1], int(self.has_key))\n",
    "    \n",
    "    def step(self, action: Action):\n",
    "        \"\"\"í–‰ë™ ìˆ˜í–‰\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # í–‰ë™ì— ë”°ë¥¸ ì´ë™\n",
    "        y, x = self.agent_pos\n",
    "        if action == Action.UP:\n",
    "            new_pos = (y-1, x)\n",
    "        elif action == Action.DOWN:\n",
    "            new_pos = (y+1, x)\n",
    "        elif action == Action.LEFT:\n",
    "            new_pos = (y, x-1)\n",
    "        elif action == Action.RIGHT:\n",
    "            new_pos = (y, x+1)\n",
    "        \n",
    "        # ì´ë™ ê°€ëŠ¥ í™•ì¸\n",
    "        if self._is_valid_move(new_pos):\n",
    "            self.agent_pos = new_pos\n",
    "        \n",
    "        # ì—´ì‡  íšë“\n",
    "        if self.key_pos and self.agent_pos == self.key_pos and not self.has_key:\n",
    "            self.has_key = True\n",
    "        \n",
    "        # ë³´ìƒ ê³„ì‚°\n",
    "        reward = self._get_reward()\n",
    "        \n",
    "        # ì¢…ë£Œ ì¡°ê±´\n",
    "        done = (self.agent_pos == self.goal) or (self.steps >= self.max_steps)\n",
    "        \n",
    "        return self._get_state(), reward, done, {}\n",
    "    \n",
    "    def _is_valid_move(self, pos):\n",
    "        \"\"\"ì´ë™ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\"\"\"\n",
    "        y, x = pos\n",
    "        \n",
    "        # ê²½ê³„ í™•ì¸\n",
    "        if y < 0 or y >= self.height or x < 0 or x >= self.width:\n",
    "            return False\n",
    "        \n",
    "        # ë²½ í™•ì¸\n",
    "        if self.maze[y][x] == '#':\n",
    "            return False\n",
    "        \n",
    "        # ë¬¸ í™•ì¸ (ì—´ì‡  ì—†ìœ¼ë©´ í†µê³¼ ë¶ˆê°€)\n",
    "        if self.door_pos and pos == self.door_pos and not self.has_key:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        \"\"\"ë³´ìƒ ë°˜í™˜\"\"\"\n",
    "        if self.agent_pos == self.goal:\n",
    "            return 100.0  # ëª©í‘œ ë„ë‹¬\n",
    "        elif self.key_pos and self.agent_pos == self.key_pos and self.has_key:\n",
    "            return 10.0  # ì—´ì‡  íšë“\n",
    "        else:\n",
    "            return -1.0  # ê° ìŠ¤í…ë§ˆë‹¤ íŒ¨ë„í‹°\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"í™˜ê²½ ì‹œê°í™”\"\"\"\n",
    "        display = []\n",
    "        for y in range(self.height):\n",
    "            row = []\n",
    "            for x in range(self.width):\n",
    "                if (y, x) == self.agent_pos:\n",
    "                    row.append('A')\n",
    "                elif self.maze[y][x] == 'K' and not self.has_key:\n",
    "                    row.append('K')\n",
    "                elif self.maze[y][x] == 'D':\n",
    "                    row.append('D' if not self.has_key else '.')\n",
    "                else:\n",
    "                    row.append(self.maze[y][x])\n",
    "            display.append(' '.join(row))\n",
    "        \n",
    "        print('\\n'.join(display))\n",
    "        if self.has_key:\n",
    "            print(\"ğŸ”‘ ì—´ì‡  ë³´ìœ  ì¤‘\")\n",
    "\n",
    "# í™˜ê²½ í…ŒìŠ¤íŠ¸\n",
    "env = MazeEnv('simple')\n",
    "print(\"ê°„ë‹¨í•œ ë¯¸ë¡œ:\")\n",
    "env.render()\n",
    "\n",
    "env = MazeEnv('key_door')\n",
    "print(\"\\nì—´ì‡ -ë¬¸ ë¯¸ë¡œ:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Difference (TD) í•™ìŠµ\n",
    "\n",
    "TD í•™ìŠµì€ **í˜„ì¬ ì¶”ì •ê°’**ê³¼ **ë‹¤ìŒ ì¶”ì •ê°’**ì˜ ì°¨ì´ë¥¼ ì´ìš©í•´ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### TD Error\n",
    "$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "### TD Update\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot \\delta_t$$\n",
    "\n",
    "ì—¬ê¸°ì„œ Î±ëŠ” í•™ìŠµë¥ (learning rate)ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "    \"\"\"ê¸°ë³¸ TD í•™ìŠµ ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.99):\n",
    "        self.alpha = alpha  # í•™ìŠµë¥ \n",
    "        self.gamma = gamma  # í• ì¸ ì¸ì\n",
    "        self.V = defaultdict(float)  # ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜\n",
    "        self.returns = defaultdict(list)  # ì—í”¼ì†Œë“œë³„ ë¦¬í„´\n",
    "    \n",
    "    def update(self, state, reward, next_state):\n",
    "        \"\"\"TD(0) ì—…ë°ì´íŠ¸\"\"\"\n",
    "        td_error = reward + self.gamma * self.V[next_state] - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "        return td_error\n",
    "\n",
    "# TD í•™ìŠµ ë°ëª¨\n",
    "def demonstrate_td_learning():\n",
    "    \"\"\"TD í•™ìŠµ ê³¼ì • ì‹œê°í™”\"\"\"\n",
    "    agent = TDAgent(alpha=0.1, gamma=0.9)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ì²´ì¸ í™˜ê²½ (1D)\n",
    "    states = ['A', 'B', 'C', 'D', 'E']\n",
    "    rewards = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 10}\n",
    "    \n",
    "    # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹œë®¬ë ˆì´ì…˜\n",
    "    history = {s: [] for s in states}\n",
    "    \n",
    "    for episode in range(100):\n",
    "        # ëœë¤ ì‹œì‘\n",
    "        path = ['A', 'B', 'C', 'D', 'E']\n",
    "        \n",
    "        for i in range(len(path)-1):\n",
    "            state = path[i]\n",
    "            next_state = path[i+1]\n",
    "            reward = rewards[next_state]\n",
    "            \n",
    "            agent.update(state, reward, next_state)\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        for s in states:\n",
    "            history[s].append(agent.V[s])\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for s in states:\n",
    "        plt.plot(history[s], label=f'V({s})', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('State Value')\n",
    "    plt.title('TD Learning: ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ ìˆ˜ë ´')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ìµœì¢… ìƒíƒœ ê°€ì¹˜:\")\n",
    "    for s in states:\n",
    "        print(f\"V({s}) = {agent.V[s]:.2f}\")\n",
    "\n",
    "demonstrate_td_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-Learning: Off-policy TD Control\n",
    "\n",
    "Q-learningì€ **off-policy** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, í–‰ë™ ì •ì±…ê³¼ ë¬´ê´€í•˜ê²Œ ìµœì  Q í•¨ìˆ˜ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### Q-Learning ì—…ë°ì´íŠ¸ ê·œì¹™\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- **Off-policy**: íƒìƒ‰ ì •ì±…ê³¼ ë¬´ê´€í•˜ê²Œ ìµœì  ì •ì±… í•™ìŠµ\n",
    "- **Max operator**: ë‹¤ìŒ ìƒíƒœì—ì„œ ìµœëŒ€ Qê°’ ì‚¬ìš©\n",
    "- **ìˆ˜ë ´ ë³´ì¥**: ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ Q* ìˆ˜ë ´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # í•™ìŠµ ê¸°ë¡\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Îµ-greedy í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            # ë™ì ì¼ ë•Œ ëœë¤ ì„ íƒ\n",
    "            max_q = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q)[0]\n",
    "            return np.random.choice(actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        td_error = target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        self.td_errors.append(abs(td_error))\n",
    "        return td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"í•œ ì—í”¼ì†Œë“œ í•™ìŠµ\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_lengths.append(steps)\n",
    "        \n",
    "        return total_reward, steps\n",
    "\n",
    "# Q-Learning í•™ìŠµ\n",
    "def train_qlearning(env_type='simple', n_episodes=500):\n",
    "    env = MazeEnv(env_type)\n",
    "    agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    \n",
    "    print(f\"Q-Learning í•™ìŠµ ì‹œì‘ ({env_type} maze)...\")\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-100:])\n",
    "            avg_steps = np.mean(agent.episode_lengths[-100:])\n",
    "            print(f\"Episode {episode+1}: í‰ê·  ë³´ìƒ = {avg_reward:.2f}, í‰ê·  ìŠ¤í… = {avg_steps:.1f}\")\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "q_agent, q_env = train_qlearning('simple', n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SARSA: On-policy TD Control\n",
    "\n",
    "SARSAëŠ” **on-policy** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ì‹¤ì œ ë”°ë¥´ëŠ” ì •ì±…ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### SARSA ì—…ë°ì´íŠ¸ ê·œì¹™\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### íŠ¹ì§•\n",
    "- **On-policy**: ì‹¤ì œ í–‰ë™ ì •ì±…ì„ í•™ìŠµ\n",
    "- **ë” ì•ˆì „í•œ í•™ìŠµ**: íƒìƒ‰ ì¤‘ ìœ„í—˜ íšŒí”¼\n",
    "- **ì´ë¦„ ìœ ë˜**: State-Action-Reward-State-Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"SARSA ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # í•™ìŠµ ê¸°ë¡\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Îµ-greedy í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            q_values = self.Q[state]\n",
    "            max_q = np.max(q_values)\n",
    "            actions = np.where(q_values == max_q)[0]\n",
    "            return np.random.choice(actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"SARSA ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * self.Q[next_state][next_action]\n",
    "        \n",
    "        td_error = target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        self.td_errors.append(abs(td_error))\n",
    "        return td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"í•œ ì—í”¼ì†Œë“œ í•™ìŠµ\"\"\"\n",
    "        state = env.reset()\n",
    "        action = self.get_action(state)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            next_action = self.get_action(next_state)\n",
    "            \n",
    "            self.update(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_lengths.append(steps)\n",
    "        \n",
    "        return total_reward, steps\n",
    "\n",
    "# SARSA í•™ìŠµ\n",
    "def train_sarsa(env_type='simple', n_episodes=500):\n",
    "    env = MazeEnv(env_type)\n",
    "    agent = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "    \n",
    "    print(f\"SARSA í•™ìŠµ ì‹œì‘ ({env_type} maze)...\")\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-100:])\n",
    "            avg_steps = np.mean(agent.episode_lengths[-100:])\n",
    "            print(f\"Episode {episode+1}: í‰ê·  ë³´ìƒ = {avg_reward:.2f}, í‰ê·  ìŠ¤í… = {avg_steps:.1f}\")\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "sarsa_agent, sarsa_env = train_sarsa('simple', n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q-Learning vs SARSA ë¹„êµ\n",
    "\n",
    "ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ì°¨ì´ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms():\n",
    "    \"\"\"Q-Learningê³¼ SARSA ì„±ëŠ¥ ë¹„êµ\"\"\"\n",
    "    \n",
    "    # ìœ„í—˜í•œ í™˜ê²½ ìƒì„± (ì ˆë²½ ê±·ê¸°)\n",
    "    class CliffWalkingEnv:\n",
    "        def __init__(self):\n",
    "            self.width = 12\n",
    "            self.height = 4\n",
    "            self.start = (3, 0)\n",
    "            self.goal = (3, 11)\n",
    "            self.agent_pos = self.start\n",
    "            self.cliff = [(3, x) for x in range(1, 11)]  # ì ˆë²½ ìœ„ì¹˜\n",
    "        \n",
    "        def reset(self):\n",
    "            self.agent_pos = self.start\n",
    "            return self.agent_pos\n",
    "        \n",
    "        def step(self, action):\n",
    "            y, x = self.agent_pos\n",
    "            \n",
    "            if action == Action.UP:\n",
    "                y = max(0, y - 1)\n",
    "            elif action == Action.DOWN:\n",
    "                y = min(self.height - 1, y + 1)\n",
    "            elif action == Action.LEFT:\n",
    "                x = max(0, x - 1)\n",
    "            elif action == Action.RIGHT:\n",
    "                x = min(self.width - 1, x + 1)\n",
    "            \n",
    "            self.agent_pos = (y, x)\n",
    "            \n",
    "            # ë³´ìƒ ê³„ì‚°\n",
    "            if self.agent_pos in self.cliff:\n",
    "                reward = -100\n",
    "                self.agent_pos = self.start  # ë–¨ì–´ì§€ë©´ ì‹œì‘ì ìœ¼ë¡œ\n",
    "                done = False\n",
    "            elif self.agent_pos == self.goal:\n",
    "                reward = 0\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1\n",
    "                done = False\n",
    "            \n",
    "            return self.agent_pos, reward, done, {}\n",
    "    \n",
    "    # ë‘ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ\n",
    "    n_episodes = 500\n",
    "    n_runs = 10\n",
    "    \n",
    "    q_rewards_all = []\n",
    "    sarsa_rewards_all = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Q-Learning\n",
    "        env = CliffWalkingEnv()\n",
    "        q_agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = q_agent.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(Action(action))\n",
    "                q_agent.update(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            q_agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        q_rewards_all.append(q_agent.episode_rewards)\n",
    "        \n",
    "        # SARSA\n",
    "        env = CliffWalkingEnv()\n",
    "        sarsa_agent = SARSAAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            action = sarsa_agent.get_action(state)\n",
    "            total_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                next_state, reward, done, _ = env.step(Action(action))\n",
    "                next_action = sarsa_agent.get_action(next_state)\n",
    "                sarsa_agent.update(state, action, reward, next_state, next_action, done)\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            sarsa_agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        sarsa_rewards_all.append(sarsa_agent.episode_rewards)\n",
    "    \n",
    "    # í‰ê·  ê³„ì‚°\n",
    "    q_rewards_mean = np.mean(q_rewards_all, axis=0)\n",
    "    sarsa_rewards_mean = np.mean(sarsa_rewards_all, axis=0)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # ì´ë™ í‰ê·  ê³„ì‚° (window=10)\n",
    "    window = 10\n",
    "    q_smooth = np.convolve(q_rewards_mean, np.ones(window)/window, mode='valid')\n",
    "    sarsa_smooth = np.convolve(sarsa_rewards_mean, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    plt.plot(q_smooth, label='Q-Learning', linewidth=2, color='blue')\n",
    "    plt.plot(sarsa_smooth, label='SARSA', linewidth=2, color='red')\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Q-Learning vs SARSA: Cliff Walking')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\në¶„ì„ ê²°ê³¼:\")\n",
    "    print(f\"Q-Learning ìµœì¢… í‰ê·  ë³´ìƒ: {np.mean(q_rewards_mean[-50:]):.2f}\")\n",
    "    print(f\"SARSA ìµœì¢… í‰ê·  ë³´ìƒ: {np.mean(sarsa_rewards_mean[-50:]):.2f}\")\n",
    "    print(\"\\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:\")\n",
    "    print(\"- Q-Learningì€ ìµœì  ê²½ë¡œë¥¼ í•™ìŠµ (ì ˆë²½ ê°€ì¥ìë¦¬)\")\n",
    "    print(\"- SARSAëŠ” ì•ˆì „í•œ ê²½ë¡œë¥¼ í•™ìŠµ (ì ˆë²½ì—ì„œ ë©€ë¦¬)\")\n",
    "    print(\"- On-policy vs Off-policyì˜ ì°¨ì´ë¥¼ ë³´ì—¬ì¤Œ\")\n",
    "\n",
    "compare_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. N-step TDì™€ TD(Î»)\n",
    "\n",
    "### N-step TD\n",
    "í•œ ìŠ¤í…ì´ ì•„ë‹Œ N ìŠ¤í… ì•ì„ ë³´ê³  ì—…ë°ì´íŠ¸:\n",
    "$$G_t^{(n)} = r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1} r_{t+n} + \\gamma^n V(s_{t+n})$$\n",
    "\n",
    "### TD(Î»)\n",
    "ëª¨ë“  n-step ë¦¬í„´ì˜ ê°€ì¤‘ í‰ê· :\n",
    "$$G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepQLearning:\n",
    "    \"\"\"N-step Q-Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, n_steps=3, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_steps = n_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        # N-step ë²„í¼\n",
    "        self.state_buffer = deque(maxlen=n_steps)\n",
    "        self.action_buffer = deque(maxlen=n_steps)\n",
    "        self.reward_buffer = deque(maxlen=n_steps)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, done=False, next_state=None):\n",
    "        \"\"\"N-step ì—…ë°ì´íŠ¸\"\"\"\n",
    "        if len(self.state_buffer) < self.n_steps and not done:\n",
    "            return\n",
    "        \n",
    "        # N-step ë¦¬í„´ ê³„ì‚°\n",
    "        G = 0\n",
    "        for i in range(len(self.reward_buffer)):\n",
    "            G += (self.gamma ** i) * self.reward_buffer[i]\n",
    "        \n",
    "        if not done and next_state is not None:\n",
    "            G += (self.gamma ** len(self.reward_buffer)) * np.max(self.Q[next_state])\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ìƒíƒœ-í–‰ë™ ìŒ ì—…ë°ì´íŠ¸\n",
    "        state = self.state_buffer[0]\n",
    "        action = self.action_buffer[0]\n",
    "        \n",
    "        td_error = G - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # ë²„í¼ ì´ˆê¸°í™”\n",
    "        self.state_buffer.clear()\n",
    "        self.action_buffer.clear()\n",
    "        self.reward_buffer.clear()\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            # ë²„í¼ì— ì¶”ê°€\n",
    "            self.state_buffer.append(state)\n",
    "            self.action_buffer.append(action)\n",
    "            self.reward_buffer.append(reward)\n",
    "            \n",
    "            # N-step ì—…ë°ì´íŠ¸\n",
    "            self.update(done, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬\n",
    "                while len(self.state_buffer) > 0:\n",
    "                    self.update(done=True)\n",
    "                    self.state_buffer.popleft()\n",
    "                    self.action_buffer.popleft()\n",
    "                    self.reward_buffer.popleft()\n",
    "                break\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "# N-step ë¹„êµ ì‹¤í—˜\n",
    "def compare_n_steps():\n",
    "    \"\"\"ë‹¤ì–‘í•œ N ê°’ ë¹„êµ\"\"\"\n",
    "    n_values = [1, 3, 5, 10]\n",
    "    results = {}\n",
    "    \n",
    "    for n in n_values:\n",
    "        env = MazeEnv('simple')\n",
    "        agent = NStepQLearning(n_steps=n)\n",
    "        rewards = []\n",
    "        \n",
    "        print(f\"\\nTraining {n}-step Q-Learning...\")\n",
    "        for episode in tqdm(range(300)):\n",
    "            reward = agent.train_episode(env)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        results[n] = rewards\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for n in n_values:\n",
    "        # ì´ë™ í‰ê· \n",
    "        window = 20\n",
    "        smooth = np.convolve(results[n], np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smooth, label=f'{n}-step', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('N-step Q-Learning ë¹„êµ')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ N-step íš¨ê³¼:\")\n",
    "    print(\"- N=1: ê¸°ë³¸ Q-learning (ë¹ ë¥¸ ì—…ë°ì´íŠ¸, ë†’ì€ ë¶„ì‚°)\")\n",
    "    print(\"- N=3-5: ê· í˜•ì¡íŒ ì„±ëŠ¥\")\n",
    "    print(\"- N=10: Monte Carloì— ê°€ê¹Œì›€ (ëŠë¦° ì—…ë°ì´íŠ¸, ë‚®ì€ ë¶„ì‚°)\")\n",
    "\n",
    "compare_n_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì‹¤ì „ ì ìš©: ì—´ì‡ -ë¬¸ ë¯¸ë¡œ í•´ê²°\n",
    "\n",
    "ë³µì¡í•œ ì—´ì‡ -ë¬¸ ë¯¸ë¡œë¥¼ Q-learningìœ¼ë¡œ í•´ê²°í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_values(agent, env):\n",
    "    \"\"\"Q ê°’ ì‹œê°í™”\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    action_names = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "    for action_idx, (ax, action_name) in enumerate(zip(axes.flat, action_names)):\n",
    "        q_grid = np.zeros((env.height, env.width))\n",
    "        \n",
    "        for y in range(env.height):\n",
    "            for x in range(env.width):\n",
    "                state = (y, x, 0)  # ì—´ì‡  ì—†ëŠ” ìƒíƒœ\n",
    "                q_grid[y, x] = agent.Q[state][action_idx]\n",
    "        \n",
    "        im = ax.imshow(q_grid, cmap='coolwarm', aspect='auto')\n",
    "        ax.set_title(f'Q-values for {action_name}')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # ë²½ í‘œì‹œ\n",
    "        for y in range(env.height):\n",
    "            for x in range(env.width):\n",
    "                if env.maze[y][x] == '#':\n",
    "                    ax.add_patch(plt.Rectangle((x-0.5, y-0.5), 1, 1, \n",
    "                                              fill=True, color='black', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_agent(agent, env, n_episodes=10, visualize=False):\n",
    "    \"\"\"í•™ìŠµëœ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    success_count = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        path = []\n",
    "        \n",
    "        while steps < env.max_steps:\n",
    "            action = agent.get_action(state, training=False)\n",
    "            path.append((state, Action(action)))\n",
    "            \n",
    "            state, reward, done, _ = env.step(Action(action))\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if env.agent_pos == env.goal:\n",
    "                    success_count += 1\n",
    "                break\n",
    "        \n",
    "        total_steps += steps\n",
    "        \n",
    "        if visualize and episode == 0:\n",
    "            print(f\"\\nì—í”¼ì†Œë“œ {episode+1} ê²½ë¡œ:\")\n",
    "            env.reset()\n",
    "            for state, action in path[:20]:  # ì²˜ìŒ 20ìŠ¤í…ë§Œ\n",
    "                print(f\"State: {state}, Action: {action.name}\")\n",
    "    \n",
    "    success_rate = success_count / n_episodes * 100\n",
    "    avg_steps = total_steps / n_episodes\n",
    "    \n",
    "    print(f\"\\ní…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "    print(f\"ì„±ê³µë¥ : {success_rate:.1f}%\")\n",
    "    print(f\"í‰ê·  ìŠ¤í…: {avg_steps:.1f}\")\n",
    "    \n",
    "    return success_rate, avg_steps\n",
    "\n",
    "# ì—´ì‡ -ë¬¸ ë¯¸ë¡œ í•™ìŠµ\n",
    "print(\"ë³µì¡í•œ ì—´ì‡ -ë¬¸ ë¯¸ë¡œ í•™ìŠµ...\")\n",
    "complex_env = MazeEnv('key_door')\n",
    "complex_agent = QLearningAgent(n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "\n",
    "# í•™ìŠµ\n",
    "for episode in tqdm(range(1000)):\n",
    "    complex_agent.train_episode(complex_env)\n",
    "    \n",
    "    # Epsilon decay\n",
    "    if episode % 100 == 0:\n",
    "        complex_agent.epsilon *= 0.9\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„ \n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "window = 50\n",
    "rewards_smooth = np.convolve(complex_agent.episode_rewards, \n",
    "                             np.ones(window)/window, mode='valid')\n",
    "plt.plot(rewards_smooth)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('í•™ìŠµ ê³¡ì„ ')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "steps_smooth = np.convolve(complex_agent.episode_lengths, \n",
    "                          np.ones(window)/window, mode='valid')\n",
    "plt.plot(steps_smooth)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.title('ì—í”¼ì†Œë“œ ê¸¸ì´')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Q ê°’ ì‹œê°í™”\n",
    "print(\"\\nQ ê°’ íˆíŠ¸ë§µ:\")\n",
    "visualize_q_values(complex_agent, complex_env)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_agent(complex_agent, complex_env, n_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê³ ê¸‰ ê¸°ë²•: Double Q-Learning\n",
    "\n",
    "Q-learningì˜ ê³¼ëŒ€í‰ê°€(overestimation) ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” Double Q-Learningì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearning:\n",
    "    \"\"\"Double Q-Learning ì—ì´ì „íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions=4, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # ë‘ ê°œì˜ Q í•¨ìˆ˜\n",
    "        self.Q1 = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.Q2 = defaultdict(lambda: np.zeros(n_actions))\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"í‰ê·  Q ê°’ìœ¼ë¡œ í–‰ë™ ì„ íƒ\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            # ë‘ Q í•¨ìˆ˜ì˜ í‰ê·  ì‚¬ìš©\n",
    "            q_values = (self.Q1[state] + self.Q2[state]) / 2\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Double Q-learning ì—…ë°ì´íŠ¸\"\"\"\n",
    "        # 50% í™•ë¥ ë¡œ Q1 ë˜ëŠ” Q2 ì—…ë°ì´íŠ¸\n",
    "        if random.random() < 0.5:\n",
    "            # Q1 ì—…ë°ì´íŠ¸\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Q1ìœ¼ë¡œ í–‰ë™ ì„ íƒ, Q2ë¡œ í‰ê°€\n",
    "                best_action = np.argmax(self.Q1[next_state])\n",
    "                target = reward + self.gamma * self.Q2[next_state][best_action]\n",
    "            \n",
    "            td_error = target - self.Q1[state][action]\n",
    "            self.Q1[state][action] += self.alpha * td_error\n",
    "        else:\n",
    "            # Q2 ì—…ë°ì´íŠ¸\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Q2ë¡œ í–‰ë™ ì„ íƒ, Q1ìœ¼ë¡œ í‰ê°€\n",
    "                best_action = np.argmax(self.Q2[next_state])\n",
    "                target = reward + self.gamma * self.Q1[next_state][best_action]\n",
    "            \n",
    "            td_error = target - self.Q2[state][action]\n",
    "            self.Q2[state][action] += self.alpha * td_error\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(Action(action))\n",
    "            \n",
    "            self.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        return total_reward\n",
    "\n",
    "# Double Q-Learning vs Q-Learning ë¹„êµ\n",
    "def compare_double_q():\n",
    "    \"\"\"Double Q-Learning íš¨ê³¼ ë¹„êµ\"\"\"\n",
    "    env_q = MazeEnv('key_door')\n",
    "    env_double = MazeEnv('key_door')\n",
    "    \n",
    "    q_agent = QLearningAgent()\n",
    "    double_agent = DoubleQLearning()\n",
    "    \n",
    "    n_episodes = 500\n",
    "    \n",
    "    print(\"í•™ìŠµ ì¤‘...\")\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        q_agent.train_episode(env_q)\n",
    "        double_agent.train_episode(env_double)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    window = 20\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    q_smooth = np.convolve(q_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    double_smooth = np.convolve(double_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    plt.plot(q_smooth, label='Q-Learning', alpha=0.7)\n",
    "    plt.plot(double_smooth, label='Double Q-Learning', alpha=0.7)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('í•™ìŠµ ì„±ëŠ¥ ë¹„êµ')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Q ê°’ ë¶„ì‚° ë¹„êµ\n",
    "    q_values = []\n",
    "    double_values = []\n",
    "    \n",
    "    for state in q_agent.Q.keys():\n",
    "        q_values.extend(q_agent.Q[state])\n",
    "    \n",
    "    for state in double_agent.Q1.keys():\n",
    "        double_values.extend((double_agent.Q1[state] + double_agent.Q2[state]) / 2)\n",
    "    \n",
    "    plt.hist(q_values, bins=30, alpha=0.5, label='Q-Learning', density=True)\n",
    "    plt.hist(double_values, bins=30, alpha=0.5, label='Double Q-Learning', density=True)\n",
    "    plt.xlabel('Q-values')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Q ê°’ ë¶„í¬')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Double Q-Learning íš¨ê³¼:\")\n",
    "    print(\"- ê³¼ëŒ€í‰ê°€(overestimation) ê°ì†Œ\")\n",
    "    print(\"- ë” ì•ˆì •ì ì¸ í•™ìŠµ\")\n",
    "    print(\"- Q ê°’ ë¶„ì‚° ê°ì†Œ\")\n",
    "\n",
    "compare_double_q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìš”ì•½ ë° í•µì‹¬ ê°œë…\n",
    "\n",
    "### ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "1. **Temporal Difference (TD) í•™ìŠµ**\n",
    "   - ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘: ì¶”ì •ê°’ìœ¼ë¡œ ì¶”ì •ê°’ ì—…ë°ì´íŠ¸\n",
    "   - TD errorë¥¼ í†µí•œ ì˜¨ë¼ì¸ í•™ìŠµ\n",
    "\n",
    "2. **Q-Learning**\n",
    "   - Off-policy: ìµœì  ì •ì±… ì§ì ‘ í•™ìŠµ\n",
    "   - Max operator ì‚¬ìš©\n",
    "   - íƒìƒ‰ê³¼ ë¬´ê´€í•˜ê²Œ ìµœì  Q* ìˆ˜ë ´\n",
    "\n",
    "3. **SARSA**\n",
    "   - On-policy: ì‹¤ì œ ë”°ë¥´ëŠ” ì •ì±… í•™ìŠµ\n",
    "   - ë” ì•ˆì „í•œ í•™ìŠµ (ìœ„í—˜ íšŒí”¼)\n",
    "   - ì‹¤ì œ í–‰ë™ ê³ ë ¤\n",
    "\n",
    "4. **ê³ ê¸‰ ê¸°ë²•**\n",
    "   - N-step TD: ì—¬ëŸ¬ ìŠ¤í… ê³ ë ¤\n",
    "   - Double Q-Learning: ê³¼ëŒ€í‰ê°€ í•´ê²°\n",
    "   - Îµ-greedy ì „ëµê³¼ íƒìƒ‰\n",
    "\n",
    "### Q-Learning vs SARSA í•µì‹¬ ì°¨ì´\n",
    "\n",
    "| íŠ¹ì„± | Q-Learning | SARSA |\n",
    "|-----|-----------|--------|\n",
    "| ì •ì±… ìœ í˜• | Off-policy | On-policy |\n",
    "| ì—…ë°ì´íŠ¸ | max Q(s',a') | Q(s',a') |\n",
    "| ìµœì ì„± | ìµœì  ì •ì±… í•™ìŠµ | ì‹¤ì œ ì •ì±… í•™ìŠµ |\n",
    "| ì•ˆì „ì„± | ìœ„í—˜ ê°ìˆ˜ | ìœ„í—˜ íšŒí”¼ |\n",
    "| ìˆ˜ë ´ì„± | Q*ë¡œ ìˆ˜ë ´ | ì •ì±…ì˜ Që¡œ ìˆ˜ë ´ |\n",
    "\n",
    "### ë‹¤ìŒ ë…¸íŠ¸ë¶ ì˜ˆê³ \n",
    "**Notebook 3: Deep RL - DQNê³¼ PPO**\n",
    "- Neural Network + Q-Learning = DQN\n",
    "- Experience Replayì™€ Target Network\n",
    "- Policy Gradient Methods\n",
    "- PPO: í˜„ëŒ€ RLì˜ ì£¼ë ¥ ì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\n",
    "print(\"ğŸ¯ í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(\"âœ… TD í•™ìŠµ ì´í•´\")\n",
    "print(\"âœ… Q-Learning êµ¬í˜„\")\n",
    "print(\"âœ… SARSA êµ¬í˜„\")\n",
    "print(\"âœ… On-policy vs Off-policy ì°¨ì´ ì´í•´\")\n",
    "print(\"âœ… N-step TD êµ¬í˜„\")\n",
    "print(\"âœ… Double Q-Learning êµ¬í˜„\")\n",
    "print(\"âœ… ë³µì¡í•œ í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ í›ˆë ¨\")\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: Deep Learningê³¼ RLì˜ ê²°í•©!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}