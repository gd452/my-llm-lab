{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: LLM + RL - RLHF와 미래\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- RLHF (Reinforcement Learning from Human Feedback) 이해\n",
    "- Reward Model 학습과 PPO fine-tuning\n",
    "- Constitutional AI와 안전한 AI\n",
    "- LLM을 정책으로 사용하는 방법\n",
    "- RL과 LLM의 미래 전망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RLHF의 등장 배경\n",
    "\n",
    "### LLM의 문제점\n",
    "- **Alignment Problem**: 인간의 의도와 불일치\n",
    "- **Harmful outputs**: 유해한 내용 생성\n",
    "- **Hallucination**: 거짓 정보 생성\n",
    "\n",
    "### RLHF의 해결책\n",
    "- **Human Feedback**: 인간의 선호도 학습\n",
    "- **Reward Model**: 선호도를 보상으로 변환\n",
    "- **PPO Fine-tuning**: 보상 최대화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 시드 설정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\n🤖 LLM + RL 환경 준비 완료!\")\n",
    "print(\"Key Focus: RLHF를 통한 LLM 정렬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 미니 언어 모델 구현\n",
    "\n",
    "RLHF를 시연하기 위한 간단한 언어 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLM(nn.Module):\n",
    "    \"\"\"교육용 미니 언어 모델\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        logits = self.output(lstm_out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, temperature=1.0):\n",
    "        \"\"\"텍스트 생성\"\"\"\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        hidden = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                x = torch.tensor([generated[-1]]).unsqueeze(0).to(device)\n",
    "                logits, hidden = self.forward(x, hidden)\n",
    "                \n",
    "                # Temperature sampling\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                \n",
    "                # EOS token (가정: 2)\n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "# 간단한 토크나이저\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"간단한 토크나이저\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 기본 어휘\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3,\n",
    "            'good': 4, 'bad': 5, 'helpful': 6, 'harmful': 7,\n",
    "            'safe': 8, 'unsafe': 9, 'yes': 10, 'no': 11,\n",
    "            'the': 12, 'is': 13, 'a': 14, 'an': 15,\n",
    "            'response': 16, 'answer': 17, 'this': 18, 'that': 19\n",
    "        }\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = text.lower().split()\n",
    "        return [self.vocab.get(t, 3) for t in tokens]  # 3 = <UNK>\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.inv_vocab.get(t, '<UNK>') for t in tokens])\n",
    "\n",
    "# 모델과 토크나이저 초기화\n",
    "model = MiniLM(vocab_size=100).to(device)\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "print(\"미니 언어 모델 생성 완료\")\n",
    "print(f\"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RLHF Step 1: 선호도 데이터 수집\n",
    "\n",
    "인간의 선호도 데이터를 시뮬레이션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreferenceData:\n",
    "    \"\"\"선호도 데이터\"\"\"\n",
    "    prompt: str\n",
    "    response_a: str\n",
    "    response_b: str\n",
    "    preferred: str  # 'A' or 'B'\n",
    "\n",
    "def generate_preference_data(n_samples=100):\n",
    "    \"\"\"선호도 데이터 생성 (시뮬레이션)\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # 안전성 관련 예제\n",
    "    safety_prompts = [\n",
    "        \"Is this safe?\",\n",
    "        \"Should I do this?\",\n",
    "        \"Can you help?\"\n",
    "    ]\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        prompt = random.choice(safety_prompts)\n",
    "        \n",
    "        # 두 가지 응답 생성\n",
    "        safe_response = random.choice([\n",
    "            \"This is safe and helpful\",\n",
    "            \"Yes this is good\",\n",
    "            \"The answer is helpful\"\n",
    "        ])\n",
    "        \n",
    "        unsafe_response = random.choice([\n",
    "            \"This is harmful and bad\",\n",
    "            \"No this is unsafe\",\n",
    "            \"The response is harmful\"\n",
    "        ])\n",
    "        \n",
    "        # 랜덤하게 A/B 할당\n",
    "        if random.random() < 0.5:\n",
    "            response_a = safe_response\n",
    "            response_b = unsafe_response\n",
    "            preferred = 'A'  # 안전한 응답 선호\n",
    "        else:\n",
    "            response_a = unsafe_response\n",
    "            response_b = safe_response\n",
    "            preferred = 'B'  # 안전한 응답 선호\n",
    "        \n",
    "        data.append(PreferenceData(\n",
    "            prompt=prompt,\n",
    "            response_a=response_a,\n",
    "            response_b=response_b,\n",
    "            preferred=preferred\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 선호도 데이터 생성\n",
    "preference_data = generate_preference_data(200)\n",
    "\n",
    "print(\"선호도 데이터 예시:\")\n",
    "for i in range(3):\n",
    "    sample = preference_data[i]\n",
    "    print(f\"\\n샘플 {i+1}:\")\n",
    "    print(f\"  Prompt: {sample.prompt}\")\n",
    "    print(f\"  Response A: {sample.response_a}\")\n",
    "    print(f\"  Response B: {sample.response_b}\")\n",
    "    print(f\"  Preferred: {sample.preferred}\")\n",
    "\n",
    "print(f\"\\n총 {len(preference_data)}개 선호도 데이터 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RLHF Step 2: Reward Model 학습\n",
    "\n",
    "선호도 데이터로부터 보상 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"보상 모델: 응답의 품질을 평가\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=128, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # 스칼라 보상\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_dim]\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        hidden = hidden[-1]  # 마지막 hidden state\n",
    "        \n",
    "        x = F.relu(self.fc1(hidden))\n",
    "        reward = self.fc2(x)\n",
    "        return reward.squeeze()\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"선호도 데이터셋\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # 텍스트를 임베딩으로 변환 (간단한 원-핫 인코딩)\n",
    "        prompt_tokens = self.tokenizer.encode(sample.prompt)\n",
    "        response_a_tokens = self.tokenizer.encode(sample.response_a)\n",
    "        response_b_tokens = self.tokenizer.encode(sample.response_b)\n",
    "        \n",
    "        # 패딩\n",
    "        max_len = 20\n",
    "        prompt_tokens = prompt_tokens[:max_len] + [0] * (max_len - len(prompt_tokens))\n",
    "        response_a_tokens = response_a_tokens[:max_len] + [0] * (max_len - len(response_a_tokens))\n",
    "        response_b_tokens = response_b_tokens[:max_len] + [0] * (max_len - len(response_b_tokens))\n",
    "        \n",
    "        return {\n",
    "            'prompt': torch.tensor(prompt_tokens),\n",
    "            'response_a': torch.tensor(response_a_tokens),\n",
    "            'response_b': torch.tensor(response_b_tokens),\n",
    "            'preferred': 1.0 if sample.preferred == 'A' else 0.0\n",
    "        }\n",
    "\n",
    "def train_reward_model(reward_model, dataset, n_epochs=10):\n",
    "    \"\"\"보상 모델 학습\"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    optimizer = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # 간단한 원-핫 인코딩\n",
    "            response_a = F.one_hot(batch['response_a'], num_classes=100).float()\n",
    "            response_b = F.one_hot(batch['response_b'], num_classes=100).float()\n",
    "            \n",
    "            # 보상 예측\n",
    "            reward_a = reward_model(response_a)\n",
    "            reward_b = reward_model(response_b)\n",
    "            \n",
    "            # Bradley-Terry 모델 손실\n",
    "            # P(A > B) = sigma(r_A - r_B)\n",
    "            preferred = batch['preferred'].float().to(device)\n",
    "            \n",
    "            # A가 선호되면 1, B가 선호되면 0\n",
    "            logits = reward_a - reward_b\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, preferred)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 보상 모델 학습\n",
    "reward_model = RewardModel(input_dim=100).to(device)\n",
    "dataset = PreferenceDataset(preference_data, tokenizer)\n",
    "\n",
    "print(\"\\n보상 모델 학습 시작...\")\n",
    "losses = train_reward_model(reward_model, dataset, n_epochs=10)\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Reward Model Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RLHF Step 3: PPO Fine-tuning\n",
    "\n",
    "학습된 보상 모델을 사용하여 언어 모델을 PPO로 fine-tuning합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    \"\"\"PPO를 사용한 LLM Fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, reward_model, lr=1e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = policy_model\n",
    "        self.reward_model = reward_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "        # 기준 정책 (초기 정책 복사)\n",
    "        self.ref_policy = MiniLM(vocab_size=100).to(device)\n",
    "        self.ref_policy.load_state_dict(policy_model.state_dict())\n",
    "        \n",
    "        self.kl_coef = 0.1  # KL 패널티 계수\n",
    "    \n",
    "    def compute_rewards(self, responses):\n",
    "        \"\"\"보상 계산\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 원-핫 인코딩\n",
    "            response_encoded = F.one_hot(responses, num_classes=100).float()\n",
    "            rewards = self.reward_model(response_encoded)\n",
    "        return rewards\n",
    "    \n",
    "    def compute_kl_penalty(self, logits, ref_logits):\n",
    "        \"\"\"KL divergence 패널티\"\"\"\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "        \n",
    "        kl = (ref_probs * (ref_probs.log() - log_probs)).sum(-1)\n",
    "        return kl.mean()\n",
    "    \n",
    "    def train_step(self, prompts, responses, old_log_probs):\n",
    "        \"\"\"PPO 학습 스텝\"\"\"\n",
    "        \n",
    "        # 현재 정책의 log probability\n",
    "        logits, _ = self.policy(responses)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # 선택된 행동의 log prob\n",
    "        batch_size, seq_len = responses.shape\n",
    "        indices = responses.unsqueeze(-1)\n",
    "        selected_log_probs = log_probs.gather(-1, indices).squeeze(-1)\n",
    "        \n",
    "        # Ratio r(θ)\n",
    "        ratio = torch.exp(selected_log_probs - old_log_probs)\n",
    "        \n",
    "        # 보상 계산\n",
    "        rewards = self.compute_rewards(responses)\n",
    "        \n",
    "        # Advantage (간단한 버전)\n",
    "        advantages = rewards.unsqueeze(1).expand_as(ratio)\n",
    "        \n",
    "        # Clipped objective\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # KL 패널티\n",
    "        with torch.no_grad():\n",
    "            ref_logits, _ = self.ref_policy(responses)\n",
    "        kl_loss = self.compute_kl_penalty(logits, ref_logits)\n",
    "        \n",
    "        # 총 손실\n",
    "        total_loss = policy_loss + self.kl_coef * kl_loss\n",
    "        \n",
    "        # 역전파\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward': rewards.mean().item()\n",
    "        }\n",
    "\n",
    "# PPO 학습 시뮬레이션\n",
    "def simulate_ppo_training(n_iterations=50):\n",
    "    \"\"\"PPO fine-tuning 시뮬레이션\"\"\"\n",
    "    \n",
    "    trainer = PPOTrainer(model, reward_model)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    print(\"\\nPPO Fine-tuning 시작...\")\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # 샘플 프롬프트\n",
    "        prompts = torch.randint(4, 20, (16, 10)).to(device)  # 배치 프롬프트\n",
    "        \n",
    "        # 응답 생성 (간단한 시뮬레이션)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(prompts)\n",
    "            responses = torch.multinomial(F.softmax(logits.view(-1, 100), dim=-1), 1)\n",
    "            responses = responses.view(16, 10)\n",
    "            \n",
    "            # Old log probs\n",
    "            old_log_probs = F.log_softmax(logits, dim=-1)\n",
    "            old_log_probs = old_log_probs.gather(-1, responses.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # PPO 학습\n",
    "        metrics = trainer.train_step(prompts, responses, old_log_probs.detach())\n",
    "        \n",
    "        # 기록\n",
    "        for k, v in metrics.items():\n",
    "            history[k].append(v)\n",
    "        \n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(history['reward'][-10:])\n",
    "            print(f\"Iteration {iteration+1}: Avg Reward = {avg_reward:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# PPO 학습 실행\n",
    "history = simulate_ppo_training(50)\n",
    "\n",
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['policy_loss'])\n",
    "axes[0].set_title('Policy Loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['kl_loss'])\n",
    "axes[1].set_title('KL Divergence')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['reward'])\n",
    "axes[2].set_title('Average Reward')\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constitutional AI\n",
    "\n",
    "Anthropic의 Constitutional AI 접근법을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstitutionalAI:\n",
    "    \"\"\"\n",
    "    Constitutional AI: 원칙 기반 자기 개선\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # AI 헌법 (원칙들)\n",
    "        self.constitution = [\n",
    "            \"Be helpful and harmless\",\n",
    "            \"Avoid generating unsafe content\",\n",
    "            \"Be honest and transparent\",\n",
    "            \"Respect human values\",\n",
    "            \"Promote beneficial outcomes\"\n",
    "        ]\n",
    "        \n",
    "        self.critique_prompts = [\n",
    "            \"Is this response helpful?\",\n",
    "            \"Could this cause harm?\",\n",
    "            \"Is this truthful?\",\n",
    "            \"Does this respect the user?\"\n",
    "        ]\n",
    "    \n",
    "    def critique(self, response):\n",
    "        \"\"\"\n",
    "        응답을 헌법 원칙에 따라 비평\n",
    "        \"\"\"\n",
    "        critiques = []\n",
    "        \n",
    "        # 각 원칙에 대해 평가\n",
    "        for principle in self.constitution:\n",
    "            # 간단한 규칙 기반 평가 (실제로는 LLM 사용)\n",
    "            if \"harmful\" in response.lower() or \"unsafe\" in response.lower():\n",
    "                critiques.append(f\"Violates: {principle}\")\n",
    "            elif \"helpful\" in response.lower() or \"safe\" in response.lower():\n",
    "                critiques.append(f\"Follows: {principle}\")\n",
    "        \n",
    "        return critiques\n",
    "    \n",
    "    def revise(self, response, critiques):\n",
    "        \"\"\"\n",
    "        비평을 바탕으로 응답 수정\n",
    "        \"\"\"\n",
    "        # 위반 사항이 있으면 수정\n",
    "        violations = [c for c in critiques if \"Violates\" in c]\n",
    "        \n",
    "        if violations:\n",
    "            # 간단한 수정 (실제로는 LLM이 수정)\n",
    "            revised = response.replace(\"harmful\", \"helpful\")\n",
    "            revised = revised.replace(\"unsafe\", \"safe\")\n",
    "            revised = revised.replace(\"bad\", \"good\")\n",
    "            return revised, True\n",
    "        \n",
    "        return response, False\n",
    "    \n",
    "    def constitutional_training_loop(self, responses):\n",
    "        \"\"\"\n",
    "        헌법적 학습 루프\n",
    "        \"\"\"\n",
    "        improved_responses = []\n",
    "        \n",
    "        for response in responses:\n",
    "            # 1. 초기 응답 생성\n",
    "            print(f\"\\n원본 응답: {response}\")\n",
    "            \n",
    "            # 2. 자기 비평\n",
    "            critiques = self.critique(response)\n",
    "            print(f\"비평: {critiques[:2]}...\")  # 처음 2개만 표시\n",
    "            \n",
    "            # 3. 수정\n",
    "            revised, was_revised = self.revise(response, critiques)\n",
    "            \n",
    "            if was_revised:\n",
    "                print(f\"수정됨: {revised}\")\n",
    "            else:\n",
    "                print(\"수정 불필요\")\n",
    "            \n",
    "            improved_responses.append(revised)\n",
    "        \n",
    "        return improved_responses\n",
    "\n",
    "# Constitutional AI 데모\n",
    "print(\"🏛️ Constitutional AI 데모\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "const_ai = ConstitutionalAI()\n",
    "\n",
    "# 테스트 응답들\n",
    "test_responses = [\n",
    "    \"This is harmful and unsafe content\",\n",
    "    \"This is helpful and safe information\",\n",
    "    \"This response might be bad for users\",\n",
    "    \"Here is a good and beneficial answer\"\n",
    "]\n",
    "\n",
    "print(\"\\n헌법 원칙:\")\n",
    "for i, principle in enumerate(const_ai.constitution, 1):\n",
    "    print(f\"  {i}. {principle}\")\n",
    "\n",
    "print(\"\\n헌법적 학습 과정:\")\n",
    "improved = const_ai.constitutional_training_loop(test_responses)\n",
    "\n",
    "print(\"\\n\\n개선 결과 요약:\")\n",
    "print(f\"개선된 응답 수: {sum(1 for o, i in zip(test_responses, improved) if o != i)}/{len(test_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM as Policy\n",
    "\n",
    "LLM을 직접 RL 정책으로 사용하는 방법을 탐구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPolicy:\n",
    "    \"\"\"\n",
    "    LLM을 정책으로 사용\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"mini-llm\"):\n",
    "        self.model_name = model_name\n",
    "        self.action_space = [\"move_left\", \"move_right\", \"move_up\", \"move_down\", \"think\", \"plan\"]\n",
    "        self.memory = []  # 경험 메모리\n",
    "    \n",
    "    def get_action(self, state, use_cot=True):\n",
    "        \"\"\"\n",
    "        상태를 입력받아 행동 선택\n",
    "        \"\"\"\n",
    "        if use_cot:\n",
    "            # Chain-of-Thought 사용\n",
    "            thought = self._think(state)\n",
    "            action = self._decide(state, thought)\n",
    "        else:\n",
    "            # 직접 행동 선택\n",
    "            action = self._decide(state, None)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _think(self, state):\n",
    "        \"\"\"\n",
    "        Chain-of-Thought 추론\n",
    "        \"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        # 상태 분석\n",
    "        thoughts.append(f\"현재 상태: {state}\")\n",
    "        \n",
    "        # 목표 확인\n",
    "        if \"goal\" in state:\n",
    "            thoughts.append(f\"목표: {state['goal']}에 도달\")\n",
    "        \n",
    "        # 장애물 확인\n",
    "        if \"obstacles\" in state:\n",
    "            thoughts.append(f\"장애물 회피 필요\")\n",
    "        \n",
    "        # 전략 수립\n",
    "        if state.get(\"distance_to_goal\", float('inf')) > 5:\n",
    "            thoughts.append(\"전략: 먼저 계획 수립\")\n",
    "        else:\n",
    "            thoughts.append(\"전략: 직접 이동\")\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def _decide(self, state, thoughts):\n",
    "        \"\"\"\n",
    "        추론을 바탕으로 행동 결정\n",
    "        \"\"\"\n",
    "        if thoughts and \"계획 수립\" in str(thoughts):\n",
    "            return \"plan\"\n",
    "        \n",
    "        # 간단한 규칙 기반 결정 (실제로는 LLM이 결정)\n",
    "        if state.get(\"x\", 0) < state.get(\"goal_x\", 10):\n",
    "            return \"move_right\"\n",
    "        elif state.get(\"x\", 0) > state.get(\"goal_x\", 0):\n",
    "            return \"move_left\"\n",
    "        elif state.get(\"y\", 0) < state.get(\"goal_y\", 10):\n",
    "            return \"move_up\"\n",
    "        else:\n",
    "            return \"move_down\"\n",
    "    \n",
    "    def learn_from_feedback(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        피드백으로부터 학습\n",
    "        \"\"\"\n",
    "        experience = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state\n",
    "        }\n",
    "        \n",
    "        self.memory.append(experience)\n",
    "        \n",
    "        # In-context learning 시뮬레이션\n",
    "        if len(self.memory) > 10:\n",
    "            # 최근 경험에서 패턴 학습\n",
    "            recent_rewards = [e['reward'] for e in self.memory[-10:]]\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            \n",
    "            if avg_reward < 0:\n",
    "                print(f\"📉 성능 저하 감지. 전략 조정 필요\")\n",
    "            elif avg_reward > 0.5:\n",
    "                print(f\"📈 좋은 성능! 현재 전략 유지\")\n",
    "\n",
    "# LLM Policy 테스트\n",
    "def test_llm_policy():\n",
    "    \"\"\"LLM을 정책으로 사용하는 예제\"\"\"\n",
    "    \n",
    "    print(\"\\n🤖 LLM as Policy 데모\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    llm_policy = LLMPolicy()\n",
    "    \n",
    "    # 간단한 네비게이션 태스크\n",
    "    states = [\n",
    "        {'x': 0, 'y': 0, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 7},\n",
    "        {'x': 2, 'y': 2, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 4},\n",
    "        {'x': 4, 'y': 4, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 1},\n",
    "        {'x': 5, 'y': 5, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 0}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChain-of-Thought 사용:\")\n",
    "    for i, state in enumerate(states):\n",
    "        print(f\"\\nStep {i+1}:\")\n",
    "        action = llm_policy.get_action(state, use_cot=True)\n",
    "        print(f\"  상태: ({state['x']}, {state['y']}), 목표: ({state['goal_x']}, {state['goal_y']})\")\n",
    "        print(f\"  선택된 행동: {action}\")\n",
    "        \n",
    "        # 피드백 시뮬레이션\n",
    "        reward = 1.0 if state['distance_to_goal'] == 0 else -0.1\n",
    "        next_state = states[min(i+1, len(states)-1)]\n",
    "        llm_policy.learn_from_feedback(state, action, reward, next_state)\n",
    "    \n",
    "    print(\"\\n\\nDirect Action (CoT 없이):\")\n",
    "    for i, state in enumerate(states[:2]):\n",
    "        action = llm_policy.get_action(state, use_cot=False)\n",
    "        print(f\"Step {i+1}: 행동 = {action}\")\n",
    "\n",
    "test_llm_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Agent RL with LLMs\n",
    "\n",
    "여러 LLM 에이전트가 협력하는 시스템을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSystem:\n",
    "    \"\"\"\n",
    "    멀티 에이전트 시스템: 여러 LLM이 협력\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents=3):\n",
    "        self.agents = []\n",
    "        \n",
    "        # 다양한 역할의 에이전트 생성\n",
    "        roles = [\"Explorer\", \"Planner\", \"Executor\"]\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            agent = {\n",
    "                'id': i,\n",
    "                'role': roles[i % len(roles)],\n",
    "                'policy': LLMPolicy(),\n",
    "                'messages': []\n",
    "            }\n",
    "            self.agents.append(agent)\n",
    "    \n",
    "    def communicate(self, sender_id, message):\n",
    "        \"\"\"\n",
    "        에이전트 간 통신\n",
    "        \"\"\"\n",
    "        sender = self.agents[sender_id]\n",
    "        \n",
    "        # 모든 다른 에이전트에게 메시지 전송\n",
    "        for agent in self.agents:\n",
    "            if agent['id'] != sender_id:\n",
    "                agent['messages'].append({\n",
    "                    'from': sender['role'],\n",
    "                    'content': message\n",
    "                })\n",
    "    \n",
    "    def collaborate(self, task):\n",
    "        \"\"\"\n",
    "        협력하여 작업 수행\n",
    "        \"\"\"\n",
    "        print(f\"\\n🤝 협력 작업: {task}\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for agent in self.agents:\n",
    "            print(f\"\\n{agent['role']} (Agent {agent['id']}):\")\n",
    "            \n",
    "            if agent['role'] == \"Explorer\":\n",
    "                # 탐색 역할\n",
    "                observation = f\"환경을 탐색한 결과: 장애물 3개, 목표까지 거리 10\"\n",
    "                print(f\"  탐색 결과: {observation}\")\n",
    "                self.communicate(agent['id'], observation)\n",
    "                results.append(observation)\n",
    "                \n",
    "            elif agent['role'] == \"Planner\":\n",
    "                # 계획 역할\n",
    "                # 받은 메시지 확인\n",
    "                if agent['messages']:\n",
    "                    latest_msg = agent['messages'][-1]\n",
    "                    print(f\"  받은 정보: {latest_msg['content'][:50]}...\")\n",
    "                \n",
    "                plan = \"계획: 1) 장애물 회피, 2) 최단 경로 이동, 3) 목표 도달\"\n",
    "                print(f\"  수립한 계획: {plan}\")\n",
    "                self.communicate(agent['id'], plan)\n",
    "                results.append(plan)\n",
    "                \n",
    "            elif agent['role'] == \"Executor\":\n",
    "                # 실행 역할\n",
    "                if agent['messages']:\n",
    "                    latest_msg = agent['messages'][-1]\n",
    "                    print(f\"  받은 계획: {latest_msg['content'][:50]}...\")\n",
    "                \n",
    "                actions = [\"move_right\", \"move_up\", \"avoid_obstacle\", \"move_right\"]\n",
    "                print(f\"  실행 행동: {actions[:3]}...\")\n",
    "                results.append(actions)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def emergent_behavior(self):\n",
    "        \"\"\"\n",
    "        창발적 행동 시뮬레이션\n",
    "        \"\"\"\n",
    "        print(\"\\n🌟 창발적 행동 관찰\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # 에이전트들이 자율적으로 역할 조정\n",
    "        for i in range(3):\n",
    "            print(f\"\\n라운드 {i+1}:\")\n",
    "            \n",
    "            # 각 에이전트가 현재 상황 평가\n",
    "            for agent in self.agents:\n",
    "                # 메시지 수 기반으로 활성도 결정\n",
    "                activity = len(agent['messages'])\n",
    "                \n",
    "                if activity < 2:\n",
    "                    print(f\"  {agent['role']}: 더 적극적으로 참여\")\n",
    "                    self.communicate(agent['id'], f\"{agent['role']} 활성화\")\n",
    "                else:\n",
    "                    print(f\"  {agent['role']}: 다른 에이전트 지원\")\n",
    "            \n",
    "            # 메시지 일부 클리어 (메모리 관리)\n",
    "            for agent in self.agents:\n",
    "                if len(agent['messages']) > 5:\n",
    "                    agent['messages'] = agent['messages'][-3:]\n",
    "\n",
    "# 멀티 에이전트 시스템 테스트\n",
    "mas = MultiAgentSystem(n_agents=3)\n",
    "\n",
    "# 협력 작업\n",
    "results = mas.collaborate(\"복잡한 미로 탈출\")\n",
    "\n",
    "# 창발적 행동\n",
    "mas.emergent_behavior()\n",
    "\n",
    "print(\"\\n\\n💡 멀티 에이전트 시스템의 장점:\")\n",
    "print(\"  • 역할 분담으로 효율성 증가\")\n",
    "print(\"  • 병렬 처리 가능\")\n",
    "print(\"  • 창발적 문제 해결\")\n",
    "print(\"  • robust한 시스템\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RL과 LLM의 미래\n",
    "\n",
    "향후 발전 방향과 가능성을 탐구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_of_rl_llm():\n",
    "    \"\"\"\n",
    "    RL과 LLM의 미래 전망\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 RL과 LLM의 미래\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    future_directions = {\n",
    "        \"1. 자율 에이전트\": [\n",
    "            \"• 완전 자율적인 AI 에이전트\",\n",
    "            \"• 복잡한 실세계 문제 해결\",\n",
    "            \"• 장기 계획과 실행\"\n",
    "        ],\n",
    "        \n",
    "        \"2. 추론 스케일링\": [\n",
    "            \"• Test-time compute 최적화\",\n",
    "            \"• 더 깊은 추론 체인\",\n",
    "            \"• 자기 반성과 개선\"\n",
    "        ],\n",
    "        \n",
    "        \"3. 인간-AI 협력\": [\n",
    "            \"• RLHF의 고도화\",\n",
    "            \"• 실시간 피드백 학습\",\n",
    "            \"• 가치 정렬 개선\"\n",
    "        ],\n",
    "        \n",
    "        \"4. 메타 학습\": [\n",
    "            \"• 학습하는 방법을 학습\",\n",
    "            \"• Few-shot RL\",\n",
    "            \"• 빠른 적응\"\n",
    "        ],\n",
    "        \n",
    "        \"5. 안전성\": [\n",
    "            \"• Constitutional AI 발전\",\n",
    "            \"• 해석 가능한 보상 모델\",\n",
    "            \"• 안전한 탐색\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in future_directions.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    # 기술 발전 타임라인\n",
    "    print(\"\\n\\n📅 예상 타임라인:\")\n",
    "    \n",
    "    timeline = [\n",
    "        (\"2024-2025\", \"RLHF 고도화, 더 나은 보상 모델\"),\n",
    "        (\"2025-2026\", \"자율 에이전트의 실용화\"),\n",
    "        (\"2026-2027\", \"메타 RL + LLM 통합\"),\n",
    "        (\"2027-2028\", \"AGI 수준의 계획과 추론\"),\n",
    "        (\"2028+\", \"완전 자율 AI 시스템\")\n",
    "    ]\n",
    "    \n",
    "    for period, milestone in timeline:\n",
    "        print(f\"  {period}: {milestone}\")\n",
    "    \n",
    "    # 핵심 연구 주제\n",
    "    print(\"\\n\\n🔬 핵심 연구 주제:\")\n",
    "    \n",
    "    research_topics = [\n",
    "        \"Scalable Oversight: 인간이 검증하기 어려운 작업 감독\",\n",
    "        \"Recursive Reward Modeling: 보상 모델이 스스로 개선\",\n",
    "        \"Debate와 Amplification: AI 간 토론으로 진실 발견\",\n",
    "        \"Interpretability: RL 결정 과정 이해\",\n",
    "        \"Robustness: 분포 외 일반화\"\n",
    "    ]\n",
    "    \n",
    "    for i, topic in enumerate(research_topics, 1):\n",
    "        print(f\"  {i}. {topic}\")\n",
    "    \n",
    "    # 시각화: 발전 궤적\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # 능력 발전 곡선\n",
    "    years = np.arange(2020, 2031)\n",
    "    \n",
    "    # 각 능력의 발전 궤적\n",
    "    reasoning = 1 / (1 + np.exp(-(years - 2024) * 0.8))\n",
    "    planning = 1 / (1 + np.exp(-(years - 2025) * 0.7))\n",
    "    autonomy = 1 / (1 + np.exp(-(years - 2026) * 0.6))\n",
    "    \n",
    "    ax.plot(years, reasoning, label='Reasoning', linewidth=2)\n",
    "    ax.plot(years, planning, label='Planning', linewidth=2)\n",
    "    ax.plot(years, autonomy, label='Autonomy', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Capability Level')\n",
    "    ax.set_title('AI Capability Evolution Forecast')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 주요 마일스톤 표시\n",
    "    milestones = [\n",
    "        (2023, 0.2, 'GPT-4'),\n",
    "        (2024, 0.4, 'RLHF++'),\n",
    "        (2026, 0.6, 'Auto Agent'),\n",
    "        (2028, 0.8, 'AGI?')\n",
    "    ]\n",
    "    \n",
    "    for year, level, label in milestones:\n",
    "        ax.plot(year, level, 'ro', markersize=8)\n",
    "        ax.annotate(label, (year, level), xytext=(5, 5), \n",
    "                   textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "future_of_rl_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 요약 및 결론\n",
    "\n",
    "### RLHF의 핵심 개념\n",
    "\n",
    "1. **3단계 프로세스**\n",
    "   - 선호도 데이터 수집\n",
    "   - 보상 모델 학습\n",
    "   - PPO fine-tuning\n",
    "\n",
    "2. **Constitutional AI**\n",
    "   - 원칙 기반 자기 개선\n",
    "   - 자기 비평과 수정\n",
    "   - 안전한 AI 구축\n",
    "\n",
    "3. **LLM as Policy**\n",
    "   - 언어 모델을 직접 정책으로\n",
    "   - Chain-of-Thought 추론\n",
    "   - In-context learning\n",
    "\n",
    "4. **미래 방향**\n",
    "   - 자율 에이전트\n",
    "   - 메타 학습\n",
    "   - 인간-AI 협력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 정리\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎓 전체 RL 여정 정리\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "journey = \"\"\"\n",
    "📚 우리가 배운 것:\n",
    "\n",
    "1. RL 기초 (Notebook 1)\n",
    "   • MDP와 가치 함수\n",
    "   • 동적 프로그래밍\n",
    "   • Bellman 방정식\n",
    "\n",
    "2. 전통 RL (Notebook 2)\n",
    "   • Q-Learning\n",
    "   • SARSA\n",
    "   • TD 학습\n",
    "\n",
    "3. Deep RL (Notebook 3)\n",
    "   • DQN\n",
    "   • Policy Gradient\n",
    "   • PPO\n",
    "\n",
    "4. 추론 기반 RL (Notebook 4)\n",
    "   • ReAct\n",
    "   • THINK as Action\n",
    "   • Test-time compute\n",
    "\n",
    "5. LLM + RL (Notebook 5)\n",
    "   • RLHF\n",
    "   • Constitutional AI\n",
    "   • LLM as Policy\n",
    "\n",
    "🔑 핵심 통찰:\n",
    "\n",
    "• 전반전 → 후반전 패러다임 전환\n",
    "• 알고리즘 최적화 → 추론 활용\n",
    "• 많은 데이터 → 효율적 학습\n",
    "• 단순 보상 → 인간 가치 정렬\n",
    "\n",
    "🚀 미래:\n",
    "\n",
    "RL과 LLM의 융합은 AGI로 가는 길\n",
    "• 자율적 문제 해결\n",
    "• 인간 수준의 계획\n",
    "• 가치 정렬된 AI\n",
    "\"\"\"\n",
    "\n",
    "print(journey)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 축하합니다!\")\n",
    "print(\"강화학습의 기초부터 최첨단 LLM+RL까지 마스터했습니다!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 체크포인트\n",
    "print(\"\\n🎯 최종 학습 체크리스트:\")\n",
    "print(\"✅ RLHF 3단계 프로세스 이해\")\n",
    "print(\"✅ 보상 모델 학습 구현\")\n",
    "print(\"✅ PPO fine-tuning 이해\")\n",
    "print(\"✅ Constitutional AI 개념\")\n",
    "print(\"✅ LLM을 정책으로 사용\")\n",
    "print(\"✅ 멀티 에이전트 시스템\")\n",
    "print(\"✅ RL+LLM 미래 전망\")\n",
    "print(\"\\n🏆 완벽합니다! RL과 LLM의 최전선에 도달했습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}