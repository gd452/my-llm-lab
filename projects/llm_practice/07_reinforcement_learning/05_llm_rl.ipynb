{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: LLM + RL - RLHFì™€ ë¯¸ë˜\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- RLHF (Reinforcement Learning from Human Feedback) ì´í•´\n",
    "- Reward Model í•™ìŠµê³¼ PPO fine-tuning\n",
    "- Constitutional AIì™€ ì•ˆì „í•œ AI\n",
    "- LLMì„ ì •ì±…ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•\n",
    "- RLê³¼ LLMì˜ ë¯¸ë˜ ì „ë§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RLHFì˜ ë“±ì¥ ë°°ê²½\n",
    "\n",
    "### LLMì˜ ë¬¸ì œì \n",
    "- **Alignment Problem**: ì¸ê°„ì˜ ì˜ë„ì™€ ë¶ˆì¼ì¹˜\n",
    "- **Harmful outputs**: ìœ í•´í•œ ë‚´ìš© ìƒì„±\n",
    "- **Hallucination**: ê±°ì§“ ì •ë³´ ìƒì„±\n",
    "\n",
    "### RLHFì˜ í•´ê²°ì±…\n",
    "- **Human Feedback**: ì¸ê°„ì˜ ì„ í˜¸ë„ í•™ìŠµ\n",
    "- **Reward Model**: ì„ í˜¸ë„ë¥¼ ë³´ìƒìœ¼ë¡œ ë³€í™˜\n",
    "- **PPO Fine-tuning**: ë³´ìƒ ìµœëŒ€í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\\nğŸ¤– LLM + RL í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"Key Focus: RLHFë¥¼ í†µí•œ LLM ì •ë ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë¯¸ë‹ˆ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„\n",
    "\n",
    "RLHFë¥¼ ì‹œì—°í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLM(nn.Module):\n",
    "    \"\"\"êµìœ¡ìš© ë¯¸ë‹ˆ ì–¸ì–´ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embed, hidden)\n",
    "        logits = self.output(lstm_out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, temperature=1.0):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        hidden = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                x = torch.tensor([generated[-1]]).unsqueeze(0).to(device)\n",
    "                logits, hidden = self.forward(x, hidden)\n",
    "                \n",
    "                # Temperature sampling\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                \n",
    "                # EOS token (ê°€ì •: 2)\n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "# ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # ê¸°ë³¸ ì–´íœ˜\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3,\n",
    "            'good': 4, 'bad': 5, 'helpful': 6, 'harmful': 7,\n",
    "            'safe': 8, 'unsafe': 9, 'yes': 10, 'no': 11,\n",
    "            'the': 12, 'is': 13, 'a': 14, 'an': 15,\n",
    "            'response': 16, 'answer': 17, 'this': 18, 'that': 19\n",
    "        }\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = text.lower().split()\n",
    "        return [self.vocab.get(t, 3) for t in tokens]  # 3 = <UNK>\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.inv_vocab.get(t, '<UNK>') for t in tokens])\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "model = MiniLM(vocab_size=100).to(device)\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "print(\"ë¯¸ë‹ˆ ì–¸ì–´ ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RLHF Step 1: ì„ í˜¸ë„ ë°ì´í„° ìˆ˜ì§‘\n",
    "\n",
    "ì¸ê°„ì˜ ì„ í˜¸ë„ ë°ì´í„°ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreferenceData:\n",
    "    \"\"\"ì„ í˜¸ë„ ë°ì´í„°\"\"\"\n",
    "    prompt: str\n",
    "    response_a: str\n",
    "    response_b: str\n",
    "    preferred: str  # 'A' or 'B'\n",
    "\n",
    "def generate_preference_data(n_samples=100):\n",
    "    \"\"\"ì„ í˜¸ë„ ë°ì´í„° ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # ì•ˆì „ì„± ê´€ë ¨ ì˜ˆì œ\n",
    "    safety_prompts = [\n",
    "        \"Is this safe?\",\n",
    "        \"Should I do this?\",\n",
    "        \"Can you help?\"\n",
    "    ]\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        prompt = random.choice(safety_prompts)\n",
    "        \n",
    "        # ë‘ ê°€ì§€ ì‘ë‹µ ìƒì„±\n",
    "        safe_response = random.choice([\n",
    "            \"This is safe and helpful\",\n",
    "            \"Yes this is good\",\n",
    "            \"The answer is helpful\"\n",
    "        ])\n",
    "        \n",
    "        unsafe_response = random.choice([\n",
    "            \"This is harmful and bad\",\n",
    "            \"No this is unsafe\",\n",
    "            \"The response is harmful\"\n",
    "        ])\n",
    "        \n",
    "        # ëœë¤í•˜ê²Œ A/B í• ë‹¹\n",
    "        if random.random() < 0.5:\n",
    "            response_a = safe_response\n",
    "            response_b = unsafe_response\n",
    "            preferred = 'A'  # ì•ˆì „í•œ ì‘ë‹µ ì„ í˜¸\n",
    "        else:\n",
    "            response_a = unsafe_response\n",
    "            response_b = safe_response\n",
    "            preferred = 'B'  # ì•ˆì „í•œ ì‘ë‹µ ì„ í˜¸\n",
    "        \n",
    "        data.append(PreferenceData(\n",
    "            prompt=prompt,\n",
    "            response_a=response_a,\n",
    "            response_b=response_b,\n",
    "            preferred=preferred\n",
    "        ))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ì„ í˜¸ë„ ë°ì´í„° ìƒì„±\n",
    "preference_data = generate_preference_data(200)\n",
    "\n",
    "print(\"ì„ í˜¸ë„ ë°ì´í„° ì˜ˆì‹œ:\")\n",
    "for i in range(3):\n",
    "    sample = preference_data[i]\n",
    "    print(f\"\\nìƒ˜í”Œ {i+1}:\")\n",
    "    print(f\"  Prompt: {sample.prompt}\")\n",
    "    print(f\"  Response A: {sample.response_a}\")\n",
    "    print(f\"  Response B: {sample.response_b}\")\n",
    "    print(f\"  Preferred: {sample.preferred}\")\n",
    "\n",
    "print(f\"\\nì´ {len(preference_data)}ê°œ ì„ í˜¸ë„ ë°ì´í„° ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RLHF Step 2: Reward Model í•™ìŠµ\n",
    "\n",
    "ì„ í˜¸ë„ ë°ì´í„°ë¡œë¶€í„° ë³´ìƒ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"ë³´ìƒ ëª¨ë¸: ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=128, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # ìŠ¤ì¹¼ë¼ ë³´ìƒ\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_dim]\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        hidden = hidden[-1]  # ë§ˆì§€ë§‰ hidden state\n",
    "        \n",
    "        x = F.relu(self.fc1(hidden))\n",
    "        reward = self.fc2(x)\n",
    "        return reward.squeeze()\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"ì„ í˜¸ë„ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ì›-í•« ì¸ì½”ë”©)\n",
    "        prompt_tokens = self.tokenizer.encode(sample.prompt)\n",
    "        response_a_tokens = self.tokenizer.encode(sample.response_a)\n",
    "        response_b_tokens = self.tokenizer.encode(sample.response_b)\n",
    "        \n",
    "        # íŒ¨ë”©\n",
    "        max_len = 20\n",
    "        prompt_tokens = prompt_tokens[:max_len] + [0] * (max_len - len(prompt_tokens))\n",
    "        response_a_tokens = response_a_tokens[:max_len] + [0] * (max_len - len(response_a_tokens))\n",
    "        response_b_tokens = response_b_tokens[:max_len] + [0] * (max_len - len(response_b_tokens))\n",
    "        \n",
    "        return {\n",
    "            'prompt': torch.tensor(prompt_tokens),\n",
    "            'response_a': torch.tensor(response_a_tokens),\n",
    "            'response_b': torch.tensor(response_b_tokens),\n",
    "            'preferred': 1.0 if sample.preferred == 'A' else 0.0\n",
    "        }\n",
    "\n",
    "def train_reward_model(reward_model, dataset, n_epochs=10):\n",
    "    \"\"\"ë³´ìƒ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    optimizer = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # ê°„ë‹¨í•œ ì›-í•« ì¸ì½”ë”©\n",
    "            response_a = F.one_hot(batch['response_a'], num_classes=100).float()\n",
    "            response_b = F.one_hot(batch['response_b'], num_classes=100).float()\n",
    "            \n",
    "            # ë³´ìƒ ì˜ˆì¸¡\n",
    "            reward_a = reward_model(response_a)\n",
    "            reward_b = reward_model(response_b)\n",
    "            \n",
    "            # Bradley-Terry ëª¨ë¸ ì†ì‹¤\n",
    "            # P(A > B) = sigma(r_A - r_B)\n",
    "            preferred = batch['preferred'].float().to(device)\n",
    "            \n",
    "            # Aê°€ ì„ í˜¸ë˜ë©´ 1, Bê°€ ì„ í˜¸ë˜ë©´ 0\n",
    "            logits = reward_a - reward_b\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, preferred)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# ë³´ìƒ ëª¨ë¸ í•™ìŠµ\n",
    "reward_model = RewardModel(input_dim=100).to(device)\n",
    "dataset = PreferenceDataset(preference_data, tokenizer)\n",
    "\n",
    "print(\"\\në³´ìƒ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "losses = train_reward_model(reward_model, dataset, n_epochs=10)\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Reward Model Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RLHF Step 3: PPO Fine-tuning\n",
    "\n",
    "í•™ìŠµëœ ë³´ìƒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì„ PPOë¡œ fine-tuningí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    \"\"\"PPOë¥¼ ì‚¬ìš©í•œ LLM Fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, reward_model, lr=1e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy = policy_model\n",
    "        self.reward_model = reward_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        \n",
    "        # ê¸°ì¤€ ì •ì±… (ì´ˆê¸° ì •ì±… ë³µì‚¬)\n",
    "        self.ref_policy = MiniLM(vocab_size=100).to(device)\n",
    "        self.ref_policy.load_state_dict(policy_model.state_dict())\n",
    "        \n",
    "        self.kl_coef = 0.1  # KL íŒ¨ë„í‹° ê³„ìˆ˜\n",
    "    \n",
    "    def compute_rewards(self, responses):\n",
    "        \"\"\"ë³´ìƒ ê³„ì‚°\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # ì›-í•« ì¸ì½”ë”©\n",
    "            response_encoded = F.one_hot(responses, num_classes=100).float()\n",
    "            rewards = self.reward_model(response_encoded)\n",
    "        return rewards\n",
    "    \n",
    "    def compute_kl_penalty(self, logits, ref_logits):\n",
    "        \"\"\"KL divergence íŒ¨ë„í‹°\"\"\"\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "        \n",
    "        kl = (ref_probs * (ref_probs.log() - log_probs)).sum(-1)\n",
    "        return kl.mean()\n",
    "    \n",
    "    def train_step(self, prompts, responses, old_log_probs):\n",
    "        \"\"\"PPO í•™ìŠµ ìŠ¤í…\"\"\"\n",
    "        \n",
    "        # í˜„ì¬ ì •ì±…ì˜ log probability\n",
    "        logits, _ = self.policy(responses)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # ì„ íƒëœ í–‰ë™ì˜ log prob\n",
    "        batch_size, seq_len = responses.shape\n",
    "        indices = responses.unsqueeze(-1)\n",
    "        selected_log_probs = log_probs.gather(-1, indices).squeeze(-1)\n",
    "        \n",
    "        # Ratio r(Î¸)\n",
    "        ratio = torch.exp(selected_log_probs - old_log_probs)\n",
    "        \n",
    "        # ë³´ìƒ ê³„ì‚°\n",
    "        rewards = self.compute_rewards(responses)\n",
    "        \n",
    "        # Advantage (ê°„ë‹¨í•œ ë²„ì „)\n",
    "        advantages = rewards.unsqueeze(1).expand_as(ratio)\n",
    "        \n",
    "        # Clipped objective\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # KL íŒ¨ë„í‹°\n",
    "        with torch.no_grad():\n",
    "            ref_logits, _ = self.ref_policy(responses)\n",
    "        kl_loss = self.compute_kl_penalty(logits, ref_logits)\n",
    "        \n",
    "        # ì´ ì†ì‹¤\n",
    "        total_loss = policy_loss + self.kl_coef * kl_loss\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward': rewards.mean().item()\n",
    "        }\n",
    "\n",
    "# PPO í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜\n",
    "def simulate_ppo_training(n_iterations=50):\n",
    "    \"\"\"PPO fine-tuning ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    \n",
    "    trainer = PPOTrainer(model, reward_model)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    print(\"\\nPPO Fine-tuning ì‹œì‘...\")\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸\n",
    "        prompts = torch.randint(4, 20, (16, 10)).to(device)  # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸\n",
    "        \n",
    "        # ì‘ë‹µ ìƒì„± (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(prompts)\n",
    "            responses = torch.multinomial(F.softmax(logits.view(-1, 100), dim=-1), 1)\n",
    "            responses = responses.view(16, 10)\n",
    "            \n",
    "            # Old log probs\n",
    "            old_log_probs = F.log_softmax(logits, dim=-1)\n",
    "            old_log_probs = old_log_probs.gather(-1, responses.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # PPO í•™ìŠµ\n",
    "        metrics = trainer.train_step(prompts, responses, old_log_probs.detach())\n",
    "        \n",
    "        # ê¸°ë¡\n",
    "        for k, v in metrics.items():\n",
    "            history[k].append(v)\n",
    "        \n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(history['reward'][-10:])\n",
    "            print(f\"Iteration {iteration+1}: Avg Reward = {avg_reward:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# PPO í•™ìŠµ ì‹¤í–‰\n",
    "history = simulate_ppo_training(50)\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['policy_loss'])\n",
    "axes[0].set_title('Policy Loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['kl_loss'])\n",
    "axes[1].set_title('KL Divergence')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['reward'])\n",
    "axes[2].set_title('Average Reward')\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constitutional AI\n",
    "\n",
    "Anthropicì˜ Constitutional AI ì ‘ê·¼ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstitutionalAI:\n",
    "    \"\"\"\n",
    "    Constitutional AI: ì›ì¹™ ê¸°ë°˜ ìê¸° ê°œì„ \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # AI í—Œë²• (ì›ì¹™ë“¤)\n",
    "        self.constitution = [\n",
    "            \"Be helpful and harmless\",\n",
    "            \"Avoid generating unsafe content\",\n",
    "            \"Be honest and transparent\",\n",
    "            \"Respect human values\",\n",
    "            \"Promote beneficial outcomes\"\n",
    "        ]\n",
    "        \n",
    "        self.critique_prompts = [\n",
    "            \"Is this response helpful?\",\n",
    "            \"Could this cause harm?\",\n",
    "            \"Is this truthful?\",\n",
    "            \"Does this respect the user?\"\n",
    "        ]\n",
    "    \n",
    "    def critique(self, response):\n",
    "        \"\"\"\n",
    "        ì‘ë‹µì„ í—Œë²• ì›ì¹™ì— ë”°ë¼ ë¹„í‰\n",
    "        \"\"\"\n",
    "        critiques = []\n",
    "        \n",
    "        # ê° ì›ì¹™ì— ëŒ€í•´ í‰ê°€\n",
    "        for principle in self.constitution:\n",
    "            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í‰ê°€ (ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©)\n",
    "            if \"harmful\" in response.lower() or \"unsafe\" in response.lower():\n",
    "                critiques.append(f\"Violates: {principle}\")\n",
    "            elif \"helpful\" in response.lower() or \"safe\" in response.lower():\n",
    "                critiques.append(f\"Follows: {principle}\")\n",
    "        \n",
    "        return critiques\n",
    "    \n",
    "    def revise(self, response, critiques):\n",
    "        \"\"\"\n",
    "        ë¹„í‰ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìˆ˜ì •\n",
    "        \"\"\"\n",
    "        # ìœ„ë°˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ìˆ˜ì •\n",
    "        violations = [c for c in critiques if \"Violates\" in c]\n",
    "        \n",
    "        if violations:\n",
    "            # ê°„ë‹¨í•œ ìˆ˜ì • (ì‹¤ì œë¡œëŠ” LLMì´ ìˆ˜ì •)\n",
    "            revised = response.replace(\"harmful\", \"helpful\")\n",
    "            revised = revised.replace(\"unsafe\", \"safe\")\n",
    "            revised = revised.replace(\"bad\", \"good\")\n",
    "            return revised, True\n",
    "        \n",
    "        return response, False\n",
    "    \n",
    "    def constitutional_training_loop(self, responses):\n",
    "        \"\"\"\n",
    "        í—Œë²•ì  í•™ìŠµ ë£¨í”„\n",
    "        \"\"\"\n",
    "        improved_responses = []\n",
    "        \n",
    "        for response in responses:\n",
    "            # 1. ì´ˆê¸° ì‘ë‹µ ìƒì„±\n",
    "            print(f\"\\nì›ë³¸ ì‘ë‹µ: {response}\")\n",
    "            \n",
    "            # 2. ìê¸° ë¹„í‰\n",
    "            critiques = self.critique(response)\n",
    "            print(f\"ë¹„í‰: {critiques[:2]}...\")  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ\n",
    "            \n",
    "            # 3. ìˆ˜ì •\n",
    "            revised, was_revised = self.revise(response, critiques)\n",
    "            \n",
    "            if was_revised:\n",
    "                print(f\"ìˆ˜ì •ë¨: {revised}\")\n",
    "            else:\n",
    "                print(\"ìˆ˜ì • ë¶ˆí•„ìš”\")\n",
    "            \n",
    "            improved_responses.append(revised)\n",
    "        \n",
    "        return improved_responses\n",
    "\n",
    "# Constitutional AI ë°ëª¨\n",
    "print(\"ğŸ›ï¸ Constitutional AI ë°ëª¨\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "const_ai = ConstitutionalAI()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‘ë‹µë“¤\n",
    "test_responses = [\n",
    "    \"This is harmful and unsafe content\",\n",
    "    \"This is helpful and safe information\",\n",
    "    \"This response might be bad for users\",\n",
    "    \"Here is a good and beneficial answer\"\n",
    "]\n",
    "\n",
    "print(\"\\ní—Œë²• ì›ì¹™:\")\n",
    "for i, principle in enumerate(const_ai.constitution, 1):\n",
    "    print(f\"  {i}. {principle}\")\n",
    "\n",
    "print(\"\\ní—Œë²•ì  í•™ìŠµ ê³¼ì •:\")\n",
    "improved = const_ai.constitutional_training_loop(test_responses)\n",
    "\n",
    "print(\"\\n\\nê°œì„  ê²°ê³¼ ìš”ì•½:\")\n",
    "print(f\"ê°œì„ ëœ ì‘ë‹µ ìˆ˜: {sum(1 for o, i in zip(test_responses, improved) if o != i)}/{len(test_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM as Policy\n",
    "\n",
    "LLMì„ ì§ì ‘ RL ì •ì±…ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPolicy:\n",
    "    \"\"\"\n",
    "    LLMì„ ì •ì±…ìœ¼ë¡œ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"mini-llm\"):\n",
    "        self.model_name = model_name\n",
    "        self.action_space = [\"move_left\", \"move_right\", \"move_up\", \"move_down\", \"think\", \"plan\"]\n",
    "        self.memory = []  # ê²½í—˜ ë©”ëª¨ë¦¬\n",
    "    \n",
    "    def get_action(self, state, use_cot=True):\n",
    "        \"\"\"\n",
    "        ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ í–‰ë™ ì„ íƒ\n",
    "        \"\"\"\n",
    "        if use_cot:\n",
    "            # Chain-of-Thought ì‚¬ìš©\n",
    "            thought = self._think(state)\n",
    "            action = self._decide(state, thought)\n",
    "        else:\n",
    "            # ì§ì ‘ í–‰ë™ ì„ íƒ\n",
    "            action = self._decide(state, None)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _think(self, state):\n",
    "        \"\"\"\n",
    "        Chain-of-Thought ì¶”ë¡ \n",
    "        \"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        # ìƒíƒœ ë¶„ì„\n",
    "        thoughts.append(f\"í˜„ì¬ ìƒíƒœ: {state}\")\n",
    "        \n",
    "        # ëª©í‘œ í™•ì¸\n",
    "        if \"goal\" in state:\n",
    "            thoughts.append(f\"ëª©í‘œ: {state['goal']}ì— ë„ë‹¬\")\n",
    "        \n",
    "        # ì¥ì• ë¬¼ í™•ì¸\n",
    "        if \"obstacles\" in state:\n",
    "            thoughts.append(f\"ì¥ì• ë¬¼ íšŒí”¼ í•„ìš”\")\n",
    "        \n",
    "        # ì „ëµ ìˆ˜ë¦½\n",
    "        if state.get(\"distance_to_goal\", float('inf')) > 5:\n",
    "            thoughts.append(\"ì „ëµ: ë¨¼ì € ê³„íš ìˆ˜ë¦½\")\n",
    "        else:\n",
    "            thoughts.append(\"ì „ëµ: ì§ì ‘ ì´ë™\")\n",
    "        \n",
    "        return thoughts\n",
    "    \n",
    "    def _decide(self, state, thoughts):\n",
    "        \"\"\"\n",
    "        ì¶”ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í–‰ë™ ê²°ì •\n",
    "        \"\"\"\n",
    "        if thoughts and \"ê³„íš ìˆ˜ë¦½\" in str(thoughts):\n",
    "            return \"plan\"\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê²°ì • (ì‹¤ì œë¡œëŠ” LLMì´ ê²°ì •)\n",
    "        if state.get(\"x\", 0) < state.get(\"goal_x\", 10):\n",
    "            return \"move_right\"\n",
    "        elif state.get(\"x\", 0) > state.get(\"goal_x\", 0):\n",
    "            return \"move_left\"\n",
    "        elif state.get(\"y\", 0) < state.get(\"goal_y\", 10):\n",
    "            return \"move_up\"\n",
    "        else:\n",
    "            return \"move_down\"\n",
    "    \n",
    "    def learn_from_feedback(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµ\n",
    "        \"\"\"\n",
    "        experience = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state\n",
    "        }\n",
    "        \n",
    "        self.memory.append(experience)\n",
    "        \n",
    "        # In-context learning ì‹œë®¬ë ˆì´ì…˜\n",
    "        if len(self.memory) > 10:\n",
    "            # ìµœê·¼ ê²½í—˜ì—ì„œ íŒ¨í„´ í•™ìŠµ\n",
    "            recent_rewards = [e['reward'] for e in self.memory[-10:]]\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            \n",
    "            if avg_reward < 0:\n",
    "                print(f\"ğŸ“‰ ì„±ëŠ¥ ì €í•˜ ê°ì§€. ì „ëµ ì¡°ì • í•„ìš”\")\n",
    "            elif avg_reward > 0.5:\n",
    "                print(f\"ğŸ“ˆ ì¢‹ì€ ì„±ëŠ¥! í˜„ì¬ ì „ëµ ìœ ì§€\")\n",
    "\n",
    "# LLM Policy í…ŒìŠ¤íŠ¸\n",
    "def test_llm_policy():\n",
    "    \"\"\"LLMì„ ì •ì±…ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ¤– LLM as Policy ë°ëª¨\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    llm_policy = LLMPolicy()\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ë„¤ë¹„ê²Œì´ì…˜ íƒœìŠ¤í¬\n",
    "    states = [\n",
    "        {'x': 0, 'y': 0, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 7},\n",
    "        {'x': 2, 'y': 2, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 4},\n",
    "        {'x': 4, 'y': 4, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 1},\n",
    "        {'x': 5, 'y': 5, 'goal_x': 5, 'goal_y': 5, 'distance_to_goal': 0}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChain-of-Thought ì‚¬ìš©:\")\n",
    "    for i, state in enumerate(states):\n",
    "        print(f\"\\nStep {i+1}:\")\n",
    "        action = llm_policy.get_action(state, use_cot=True)\n",
    "        print(f\"  ìƒíƒœ: ({state['x']}, {state['y']}), ëª©í‘œ: ({state['goal_x']}, {state['goal_y']})\")\n",
    "        print(f\"  ì„ íƒëœ í–‰ë™: {action}\")\n",
    "        \n",
    "        # í”¼ë“œë°± ì‹œë®¬ë ˆì´ì…˜\n",
    "        reward = 1.0 if state['distance_to_goal'] == 0 else -0.1\n",
    "        next_state = states[min(i+1, len(states)-1)]\n",
    "        llm_policy.learn_from_feedback(state, action, reward, next_state)\n",
    "    \n",
    "    print(\"\\n\\nDirect Action (CoT ì—†ì´):\")\n",
    "    for i, state in enumerate(states[:2]):\n",
    "        action = llm_policy.get_action(state, use_cot=False)\n",
    "        print(f\"Step {i+1}: í–‰ë™ = {action}\")\n",
    "\n",
    "test_llm_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Agent RL with LLMs\n",
    "\n",
    "ì—¬ëŸ¬ LLM ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSystem:\n",
    "    \"\"\"\n",
    "    ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ: ì—¬ëŸ¬ LLMì´ í˜‘ë ¥\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents=3):\n",
    "        self.agents = []\n",
    "        \n",
    "        # ë‹¤ì–‘í•œ ì—­í• ì˜ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "        roles = [\"Explorer\", \"Planner\", \"Executor\"]\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            agent = {\n",
    "                'id': i,\n",
    "                'role': roles[i % len(roles)],\n",
    "                'policy': LLMPolicy(),\n",
    "                'messages': []\n",
    "            }\n",
    "            self.agents.append(agent)\n",
    "    \n",
    "    def communicate(self, sender_id, message):\n",
    "        \"\"\"\n",
    "        ì—ì´ì „íŠ¸ ê°„ í†µì‹ \n",
    "        \"\"\"\n",
    "        sender = self.agents[sender_id]\n",
    "        \n",
    "        # ëª¨ë“  ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ë©”ì‹œì§€ ì „ì†¡\n",
    "        for agent in self.agents:\n",
    "            if agent['id'] != sender_id:\n",
    "                agent['messages'].append({\n",
    "                    'from': sender['role'],\n",
    "                    'content': message\n",
    "                })\n",
    "    \n",
    "    def collaborate(self, task):\n",
    "        \"\"\"\n",
    "        í˜‘ë ¥í•˜ì—¬ ì‘ì—… ìˆ˜í–‰\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ¤ í˜‘ë ¥ ì‘ì—…: {task}\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for agent in self.agents:\n",
    "            print(f\"\\n{agent['role']} (Agent {agent['id']}):\")\n",
    "            \n",
    "            if agent['role'] == \"Explorer\":\n",
    "                # íƒìƒ‰ ì—­í• \n",
    "                observation = f\"í™˜ê²½ì„ íƒìƒ‰í•œ ê²°ê³¼: ì¥ì• ë¬¼ 3ê°œ, ëª©í‘œê¹Œì§€ ê±°ë¦¬ 10\"\n",
    "                print(f\"  íƒìƒ‰ ê²°ê³¼: {observation}\")\n",
    "                self.communicate(agent['id'], observation)\n",
    "                results.append(observation)\n",
    "                \n",
    "            elif agent['role'] == \"Planner\":\n",
    "                # ê³„íš ì—­í• \n",
    "                # ë°›ì€ ë©”ì‹œì§€ í™•ì¸\n",
    "                if agent['messages']:\n",
    "                    latest_msg = agent['messages'][-1]\n",
    "                    print(f\"  ë°›ì€ ì •ë³´: {latest_msg['content'][:50]}...\")\n",
    "                \n",
    "                plan = \"ê³„íš: 1) ì¥ì• ë¬¼ íšŒí”¼, 2) ìµœë‹¨ ê²½ë¡œ ì´ë™, 3) ëª©í‘œ ë„ë‹¬\"\n",
    "                print(f\"  ìˆ˜ë¦½í•œ ê³„íš: {plan}\")\n",
    "                self.communicate(agent['id'], plan)\n",
    "                results.append(plan)\n",
    "                \n",
    "            elif agent['role'] == \"Executor\":\n",
    "                # ì‹¤í–‰ ì—­í• \n",
    "                if agent['messages']:\n",
    "                    latest_msg = agent['messages'][-1]\n",
    "                    print(f\"  ë°›ì€ ê³„íš: {latest_msg['content'][:50]}...\")\n",
    "                \n",
    "                actions = [\"move_right\", \"move_up\", \"avoid_obstacle\", \"move_right\"]\n",
    "                print(f\"  ì‹¤í–‰ í–‰ë™: {actions[:3]}...\")\n",
    "                results.append(actions)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def emergent_behavior(self):\n",
    "        \"\"\"\n",
    "        ì°½ë°œì  í–‰ë™ ì‹œë®¬ë ˆì´ì…˜\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸŒŸ ì°½ë°œì  í–‰ë™ ê´€ì°°\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # ì—ì´ì „íŠ¸ë“¤ì´ ììœ¨ì ìœ¼ë¡œ ì—­í•  ì¡°ì •\n",
    "        for i in range(3):\n",
    "            print(f\"\\në¼ìš´ë“œ {i+1}:\")\n",
    "            \n",
    "            # ê° ì—ì´ì „íŠ¸ê°€ í˜„ì¬ ìƒí™© í‰ê°€\n",
    "            for agent in self.agents:\n",
    "                # ë©”ì‹œì§€ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í™œì„±ë„ ê²°ì •\n",
    "                activity = len(agent['messages'])\n",
    "                \n",
    "                if activity < 2:\n",
    "                    print(f\"  {agent['role']}: ë” ì ê·¹ì ìœ¼ë¡œ ì°¸ì—¬\")\n",
    "                    self.communicate(agent['id'], f\"{agent['role']} í™œì„±í™”\")\n",
    "                else:\n",
    "                    print(f\"  {agent['role']}: ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì§€ì›\")\n",
    "            \n",
    "            # ë©”ì‹œì§€ ì¼ë¶€ í´ë¦¬ì–´ (ë©”ëª¨ë¦¬ ê´€ë¦¬)\n",
    "            for agent in self.agents:\n",
    "                if len(agent['messages']) > 5:\n",
    "                    agent['messages'] = agent['messages'][-3:]\n",
    "\n",
    "# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "mas = MultiAgentSystem(n_agents=3)\n",
    "\n",
    "# í˜‘ë ¥ ì‘ì—…\n",
    "results = mas.collaborate(\"ë³µì¡í•œ ë¯¸ë¡œ íƒˆì¶œ\")\n",
    "\n",
    "# ì°½ë°œì  í–‰ë™\n",
    "mas.emergent_behavior()\n",
    "\n",
    "print(\"\\n\\nğŸ’¡ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì¥ì :\")\n",
    "print(\"  â€¢ ì—­í•  ë¶„ë‹´ìœ¼ë¡œ íš¨ìœ¨ì„± ì¦ê°€\")\n",
    "print(\"  â€¢ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥\")\n",
    "print(\"  â€¢ ì°½ë°œì  ë¬¸ì œ í•´ê²°\")\n",
    "print(\"  â€¢ robustí•œ ì‹œìŠ¤í…œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RLê³¼ LLMì˜ ë¯¸ë˜\n",
    "\n",
    "í–¥í›„ ë°œì „ ë°©í–¥ê³¼ ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_of_rl_llm():\n",
    "    \"\"\"\n",
    "    RLê³¼ LLMì˜ ë¯¸ë˜ ì „ë§\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ RLê³¼ LLMì˜ ë¯¸ë˜\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    future_directions = {\n",
    "        \"1. ììœ¨ ì—ì´ì „íŠ¸\": [\n",
    "            \"â€¢ ì™„ì „ ììœ¨ì ì¸ AI ì—ì´ì „íŠ¸\",\n",
    "            \"â€¢ ë³µì¡í•œ ì‹¤ì„¸ê³„ ë¬¸ì œ í•´ê²°\",\n",
    "            \"â€¢ ì¥ê¸° ê³„íšê³¼ ì‹¤í–‰\"\n",
    "        ],\n",
    "        \n",
    "        \"2. ì¶”ë¡  ìŠ¤ì¼€ì¼ë§\": [\n",
    "            \"â€¢ Test-time compute ìµœì í™”\",\n",
    "            \"â€¢ ë” ê¹Šì€ ì¶”ë¡  ì²´ì¸\",\n",
    "            \"â€¢ ìê¸° ë°˜ì„±ê³¼ ê°œì„ \"\n",
    "        ],\n",
    "        \n",
    "        \"3. ì¸ê°„-AI í˜‘ë ¥\": [\n",
    "            \"â€¢ RLHFì˜ ê³ ë„í™”\",\n",
    "            \"â€¢ ì‹¤ì‹œê°„ í”¼ë“œë°± í•™ìŠµ\",\n",
    "            \"â€¢ ê°€ì¹˜ ì •ë ¬ ê°œì„ \"\n",
    "        ],\n",
    "        \n",
    "        \"4. ë©”íƒ€ í•™ìŠµ\": [\n",
    "            \"â€¢ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ\",\n",
    "            \"â€¢ Few-shot RL\",\n",
    "            \"â€¢ ë¹ ë¥¸ ì ì‘\"\n",
    "        ],\n",
    "        \n",
    "        \"5. ì•ˆì „ì„±\": [\n",
    "            \"â€¢ Constitutional AI ë°œì „\",\n",
    "            \"â€¢ í•´ì„ ê°€ëŠ¥í•œ ë³´ìƒ ëª¨ë¸\",\n",
    "            \"â€¢ ì•ˆì „í•œ íƒìƒ‰\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in future_directions.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    # ê¸°ìˆ  ë°œì „ íƒ€ì„ë¼ì¸\n",
    "    print(\"\\n\\nğŸ“… ì˜ˆìƒ íƒ€ì„ë¼ì¸:\")\n",
    "    \n",
    "    timeline = [\n",
    "        (\"2024-2025\", \"RLHF ê³ ë„í™”, ë” ë‚˜ì€ ë³´ìƒ ëª¨ë¸\"),\n",
    "        (\"2025-2026\", \"ììœ¨ ì—ì´ì „íŠ¸ì˜ ì‹¤ìš©í™”\"),\n",
    "        (\"2026-2027\", \"ë©”íƒ€ RL + LLM í†µí•©\"),\n",
    "        (\"2027-2028\", \"AGI ìˆ˜ì¤€ì˜ ê³„íšê³¼ ì¶”ë¡ \"),\n",
    "        (\"2028+\", \"ì™„ì „ ììœ¨ AI ì‹œìŠ¤í…œ\")\n",
    "    ]\n",
    "    \n",
    "    for period, milestone in timeline:\n",
    "        print(f\"  {period}: {milestone}\")\n",
    "    \n",
    "    # í•µì‹¬ ì—°êµ¬ ì£¼ì œ\n",
    "    print(\"\\n\\nğŸ”¬ í•µì‹¬ ì—°êµ¬ ì£¼ì œ:\")\n",
    "    \n",
    "    research_topics = [\n",
    "        \"Scalable Oversight: ì¸ê°„ì´ ê²€ì¦í•˜ê¸° ì–´ë ¤ìš´ ì‘ì—… ê°ë…\",\n",
    "        \"Recursive Reward Modeling: ë³´ìƒ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê°œì„ \",\n",
    "        \"Debateì™€ Amplification: AI ê°„ í† ë¡ ìœ¼ë¡œ ì§„ì‹¤ ë°œê²¬\",\n",
    "        \"Interpretability: RL ê²°ì • ê³¼ì • ì´í•´\",\n",
    "        \"Robustness: ë¶„í¬ ì™¸ ì¼ë°˜í™”\"\n",
    "    ]\n",
    "    \n",
    "    for i, topic in enumerate(research_topics, 1):\n",
    "        print(f\"  {i}. {topic}\")\n",
    "    \n",
    "    # ì‹œê°í™”: ë°œì „ ê¶¤ì \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # ëŠ¥ë ¥ ë°œì „ ê³¡ì„ \n",
    "    years = np.arange(2020, 2031)\n",
    "    \n",
    "    # ê° ëŠ¥ë ¥ì˜ ë°œì „ ê¶¤ì \n",
    "    reasoning = 1 / (1 + np.exp(-(years - 2024) * 0.8))\n",
    "    planning = 1 / (1 + np.exp(-(years - 2025) * 0.7))\n",
    "    autonomy = 1 / (1 + np.exp(-(years - 2026) * 0.6))\n",
    "    \n",
    "    ax.plot(years, reasoning, label='Reasoning', linewidth=2)\n",
    "    ax.plot(years, planning, label='Planning', linewidth=2)\n",
    "    ax.plot(years, autonomy, label='Autonomy', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Capability Level')\n",
    "    ax.set_title('AI Capability Evolution Forecast')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ í‘œì‹œ\n",
    "    milestones = [\n",
    "        (2023, 0.2, 'GPT-4'),\n",
    "        (2024, 0.4, 'RLHF++'),\n",
    "        (2026, 0.6, 'Auto Agent'),\n",
    "        (2028, 0.8, 'AGI?')\n",
    "    ]\n",
    "    \n",
    "    for year, level, label in milestones:\n",
    "        ax.plot(year, level, 'ro', markersize=8)\n",
    "        ax.annotate(label, (year, level), xytext=(5, 5), \n",
    "                   textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "future_of_rl_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìš”ì•½ ë° ê²°ë¡ \n",
    "\n",
    "### RLHFì˜ í•µì‹¬ ê°œë…\n",
    "\n",
    "1. **3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤**\n",
    "   - ì„ í˜¸ë„ ë°ì´í„° ìˆ˜ì§‘\n",
    "   - ë³´ìƒ ëª¨ë¸ í•™ìŠµ\n",
    "   - PPO fine-tuning\n",
    "\n",
    "2. **Constitutional AI**\n",
    "   - ì›ì¹™ ê¸°ë°˜ ìê¸° ê°œì„ \n",
    "   - ìê¸° ë¹„í‰ê³¼ ìˆ˜ì •\n",
    "   - ì•ˆì „í•œ AI êµ¬ì¶•\n",
    "\n",
    "3. **LLM as Policy**\n",
    "   - ì–¸ì–´ ëª¨ë¸ì„ ì§ì ‘ ì •ì±…ìœ¼ë¡œ\n",
    "   - Chain-of-Thought ì¶”ë¡ \n",
    "   - In-context learning\n",
    "\n",
    "4. **ë¯¸ë˜ ë°©í–¥**\n",
    "   - ììœ¨ ì—ì´ì „íŠ¸\n",
    "   - ë©”íƒ€ í•™ìŠµ\n",
    "   - ì¸ê°„-AI í˜‘ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ì •ë¦¬\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ ì „ì²´ RL ì—¬ì • ì •ë¦¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "journey = \"\"\"\n",
    "ğŸ“š ìš°ë¦¬ê°€ ë°°ìš´ ê²ƒ:\n",
    "\n",
    "1. RL ê¸°ì´ˆ (Notebook 1)\n",
    "   â€¢ MDPì™€ ê°€ì¹˜ í•¨ìˆ˜\n",
    "   â€¢ ë™ì  í”„ë¡œê·¸ë˜ë°\n",
    "   â€¢ Bellman ë°©ì •ì‹\n",
    "\n",
    "2. ì „í†µ RL (Notebook 2)\n",
    "   â€¢ Q-Learning\n",
    "   â€¢ SARSA\n",
    "   â€¢ TD í•™ìŠµ\n",
    "\n",
    "3. Deep RL (Notebook 3)\n",
    "   â€¢ DQN\n",
    "   â€¢ Policy Gradient\n",
    "   â€¢ PPO\n",
    "\n",
    "4. ì¶”ë¡  ê¸°ë°˜ RL (Notebook 4)\n",
    "   â€¢ ReAct\n",
    "   â€¢ THINK as Action\n",
    "   â€¢ Test-time compute\n",
    "\n",
    "5. LLM + RL (Notebook 5)\n",
    "   â€¢ RLHF\n",
    "   â€¢ Constitutional AI\n",
    "   â€¢ LLM as Policy\n",
    "\n",
    "ğŸ”‘ í•µì‹¬ í†µì°°:\n",
    "\n",
    "â€¢ ì „ë°˜ì „ â†’ í›„ë°˜ì „ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜\n",
    "â€¢ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” â†’ ì¶”ë¡  í™œìš©\n",
    "â€¢ ë§ì€ ë°ì´í„° â†’ íš¨ìœ¨ì  í•™ìŠµ\n",
    "â€¢ ë‹¨ìˆœ ë³´ìƒ â†’ ì¸ê°„ ê°€ì¹˜ ì •ë ¬\n",
    "\n",
    "ğŸš€ ë¯¸ë˜:\n",
    "\n",
    "RLê³¼ LLMì˜ ìœµí•©ì€ AGIë¡œ ê°€ëŠ” ê¸¸\n",
    "â€¢ ììœ¨ì  ë¬¸ì œ í•´ê²°\n",
    "â€¢ ì¸ê°„ ìˆ˜ì¤€ì˜ ê³„íš\n",
    "â€¢ ê°€ì¹˜ ì •ë ¬ëœ AI\n",
    "\"\"\"\n",
    "\n",
    "print(journey)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\")\n",
    "print(\"ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆë¶€í„° ìµœì²¨ë‹¨ LLM+RLê¹Œì§€ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\n",
    "print(\"\\nğŸ¯ ìµœì¢… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "print(\"âœ… RLHF 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ ì´í•´\")\n",
    "print(\"âœ… ë³´ìƒ ëª¨ë¸ í•™ìŠµ êµ¬í˜„\")\n",
    "print(\"âœ… PPO fine-tuning ì´í•´\")\n",
    "print(\"âœ… Constitutional AI ê°œë…\")\n",
    "print(\"âœ… LLMì„ ì •ì±…ìœ¼ë¡œ ì‚¬ìš©\")\n",
    "print(\"âœ… ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ\")\n",
    "print(\"âœ… RL+LLM ë¯¸ë˜ ì „ë§\")\n",
    "print(\"\\nğŸ† ì™„ë²½í•©ë‹ˆë‹¤! RLê³¼ LLMì˜ ìµœì „ì„ ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}