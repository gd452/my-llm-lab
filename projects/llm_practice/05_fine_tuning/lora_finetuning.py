"""
LoRA (Low-Rank Adaptation) íŒŒì¸íŠœë‹ ì˜ˆì œ
íš¨ìœ¨ì ì¸ LLM íŒŒì¸íŠœë‹ ê¸°ë²• ì‹¤ìŠµ
"""

import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset, load_dataset
import json
from typing import List, Dict, Any, Optional
import numpy as np

class LoRAFineTuner:
    """LoRA íŒŒì¸íŠœë‹ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model_name: str = "microsoft/phi-2",  # ì‘ì€ ëª¨ë¸ë¡œ ì‹¤ìŠµ
                 use_gpu: bool = torch.cuda.is_available()):
        
        self.model_name = model_name
        self.device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = None
        self.peft_model = None
        
    def load_model(self, load_in_8bit: bool = False):
        """ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì •"""
        
        print(f"Loading model: {self.model_name}")
        
        # ëª¨ë¸ ë¡œë“œ ì„¤ì •
        if load_in_8bit:
            # 8-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                load_in_8bit=True,
                device_map="auto"
            )
            self.model = prepare_model_for_kbit_training(self.model)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            self.model.to(self.device)
        
        print(f"Model loaded: {self.model.__class__.__name__}")
        
    def setup_lora(self, 
                   r: int = 8,
                   lora_alpha: int = 32,
                   lora_dropout: float = 0.1,
                   target_modules: Optional[List[str]] = None):
        """LoRA êµ¬ì„± ì„¤ì •"""
        
        if self.model is None:
            raise ValueError("Model not loaded. Call load_model() first.")
        
        # LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=r,  # LoRA rank
            lora_alpha=lora_alpha,  # LoRA scaling
            lora_dropout=lora_dropout,
            target_modules=target_modules or ["q_proj", "v_proj"],  # íƒ€ê²Ÿ ë ˆì´ì–´
        )
        
        # PEFT ëª¨ë¸ ìƒì„±
        self.peft_model = get_peft_model(self.model, peft_config)
        self.peft_model.print_trainable_parameters()
        
        return self.peft_model
    
    def prepare_dataset(self, data: List[Dict[str, str]]) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        
        def tokenize_function(examples):
            # ì…ë ¥ê³¼ ì¶œë ¥ì„ ê²°í•©
            texts = []
            for instruction, output in zip(examples['instruction'], examples['output']):
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
                texts.append(text)
            
            # í† í¬ë‚˜ì´ì§•
            model_inputs = self.tokenizer(
                texts,
                max_length=512,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            
            # ë ˆì´ë¸” ì„¤ì • (ì…ë ¥ê³¼ ë™ì¼)
            model_inputs["labels"] = model_inputs["input_ids"].clone()
            
            return model_inputs
        
        # Dataset ìƒì„±
        dataset = Dataset.from_dict({
            'instruction': [d['instruction'] for d in data],
            'output': [d['output'] for d in data]
        })
        
        # í† í¬ë‚˜ì´ì§•
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def train(self, 
              train_dataset: Dataset,
              eval_dataset: Optional[Dataset] = None,
              output_dir: str = "./lora_model",
              num_epochs: int = 3,
              batch_size: int = 4,
              learning_rate: float = 2e-4):
        """ëª¨ë¸ í›ˆë ¨"""
        
        if self.peft_model is None:
            raise ValueError("LoRA not configured. Call setup_lora() first.")
        
        # í›ˆë ¨ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=1,
            warmup_steps=100,
            logging_steps=10,
            save_strategy="epoch",
            evaluation_strategy="epoch" if eval_dataset else "no",
            learning_rate=learning_rate,
            fp16=self.device == "cuda",
            push_to_hub=False,
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Trainer ì´ˆê¸°í™”
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # í›ˆë ¨ ì‹œì‘
        print("Starting training...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print(f"Saving model to {output_dir}")
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        return trainer
    
    def generate(self, prompt: str, max_length: int = 100) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        
        model = self.peft_model if self.peft_model else self.model
        
        if model is None:
            raise ValueError("No model loaded")
        
        # í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text

class DatasetCreator:
    """íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ìƒì„±"""
    
    @staticmethod
    def create_instruction_dataset(domain: str = "python") -> List[Dict[str, str]]:
        """ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ ìƒì„±"""
        
        datasets = {
            "python": [
                {
                    "instruction": "Write a Python function to calculate factorial",
                    "output": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n-1)"
                },
                {
                    "instruction": "Explain list comprehension in Python",
                    "output": "List comprehension is a concise way to create lists. It consists of brackets containing an expression followed by a for clause. Example: [x**2 for x in range(10)]"
                },
                {
                    "instruction": "How to handle exceptions in Python?",
                    "output": "Use try-except blocks. The try block contains code that might raise an exception, and except block handles it. Example: try: risky_operation() except Exception as e: handle_error(e)"
                }
            ],
            "sql": [
                {
                    "instruction": "Write a SQL query to find duplicate records",
                    "output": "SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name HAVING COUNT(*) > 1;"
                },
                {
                    "instruction": "Explain JOIN types in SQL",
                    "output": "SQL has several JOIN types: INNER JOIN returns matching records, LEFT JOIN returns all from left table, RIGHT JOIN returns all from right table, FULL JOIN returns all records when there's a match in either table."
                }
            ],
            "general": [
                {
                    "instruction": "Explain machine learning in simple terms",
                    "output": "Machine learning is teaching computers to learn from examples instead of being explicitly programmed. Like showing many pictures of cats to teach recognition."
                },
                {
                    "instruction": "What is the difference between AI and ML?",
                    "output": "AI is the broader concept of machines being able to carry out smart tasks. ML is a subset of AI that focuses on learning from data to improve performance."
                }
            ]
        }
        
        return datasets.get(domain, datasets["general"])
    
    @staticmethod
    def create_qa_dataset() -> List[Dict[str, str]]:
        """Q&A ë°ì´í„°ì…‹ ìƒì„±"""
        
        return [
            {
                "instruction": "Q: What is Docker?\nA:",
                "output": "Docker is a platform for developing, shipping, and running applications in containers. Containers package code and dependencies together."
            },
            {
                "instruction": "Q: How does Git work?\nA:",
                "output": "Git is a distributed version control system. It tracks changes in files, allows collaboration, and maintains history through commits, branches, and merges."
            },
            {
                "instruction": "Q: What is REST API?\nA:",
                "output": "REST API is an architectural style for web services using HTTP methods (GET, POST, PUT, DELETE) to perform operations on resources identified by URLs."
            }
        ]

def demo_lora_setup():
    """LoRA ì„¤ì • ë°ëª¨"""
    
    print("ğŸ”§ LoRA íŒŒì¸íŠœë‹ ì„¤ì • ë°ëª¨")
    print("=" * 60)
    
    # íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”
    finetuner = LoRAFineTuner(model_name="microsoft/phi-2")
    
    print("\n1. ëª¨ë¸ ë¡œë“œ:")
    finetuner.load_model(load_in_8bit=False)
    
    print("\n2. LoRA ì„¤ì •:")
    finetuner.setup_lora(
        r=8,  # LoRA rank
        lora_alpha=32,
        lora_dropout=0.1
    )
    
    print("\n3. ë°ì´í„°ì…‹ ì¤€ë¹„:")
    dataset_creator = DatasetCreator()
    training_data = dataset_creator.create_instruction_dataset("python")
    
    train_dataset = finetuner.prepare_dataset(training_data)
    print(f"  ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    print(f"  ì²« ë²ˆì§¸ ìƒ˜í”Œ í‚¤: {list(train_dataset[0].keys())}")
    
    print("\nâœ… LoRA íŒŒì¸íŠœë‹ ì¤€ë¹„ ì™„ë£Œ!")
    print("ì‹¤ì œ í›ˆë ¨ì„ ì‹œì‘í•˜ë ¤ë©´ 'python lora_finetuning.py train' ì‹¤í–‰")

def demo_training():
    """ì‹¤ì œ í›ˆë ¨ ë°ëª¨ (ì£¼ì˜: GPUì™€ ì‹œê°„ í•„ìš”)"""
    
    print("ğŸš€ LoRA íŒŒì¸íŠœë‹ í›ˆë ¨ ë°ëª¨")
    print("=" * 60)
    print("âš ï¸  ì£¼ì˜: ì´ ë°ëª¨ëŠ” GPUì™€ ìƒë‹¹í•œ ì‹œê°„ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    
    # ê°„ë‹¨í•œ ì˜ˆì œë§Œ ì‹¤í–‰
    print("\ní›ˆë ¨ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜:")
    print("1. ëª¨ë¸ ë¡œë“œ âœ“")
    print("2. LoRA ì–´ëŒ‘í„° ì ìš© âœ“")
    print("3. ë°ì´í„°ì…‹ ì¤€ë¹„ âœ“")
    print("4. í›ˆë ¨ ì‹œì‘...")
    print("   Epoch 1/3: loss=2.34")
    print("   Epoch 2/3: loss=1.89")
    print("   Epoch 3/3: loss=1.45")
    print("5. ëª¨ë¸ ì €ì¥ âœ“")
    
    print("\nì‹¤ì œ í›ˆë ¨ ì½”ë“œ:")
    print("""
    finetuner = LoRAFineTuner()
    finetuner.load_model()
    finetuner.setup_lora()
    
    train_data = DatasetCreator.create_instruction_dataset()
    train_dataset = finetuner.prepare_dataset(train_data)
    
    trainer = finetuner.train(
        train_dataset=train_dataset,
        num_epochs=3,
        batch_size=4
    )
    """)

def explain_lora_concepts():
    """LoRA ê°œë… ì„¤ëª…"""
    
    print("\nğŸ“š LoRA (Low-Rank Adaptation) ê°œë…")
    print("=" * 60)
    
    concepts = {
        "1. LoRAë€?": """
    - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê¸°ë²•
    - ì›ë³¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •í•˜ê³  ì‘ì€ ì–´ëŒ‘í„°ë§Œ í•™ìŠµ
    - ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ëŸ‰ì„ í¬ê²Œ ì¤„ì„ (ì•½ 10,000ë°° ì ì€ íŒŒë¼ë¯¸í„°)
        """,
        
        "2. í•µì‹¬ ì›ë¦¬": """
    - ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ì €ì°¨ì› í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´
    - W' = W + BA (Bì™€ AëŠ” ì‘ì€ í–‰ë ¬)
    - Rank rì´ ì‘ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ
        """,
        
        "3. ì¥ì ": """
    - ì ì€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©
    - ë¹ ë¥¸ í›ˆë ¨ ì†ë„
    - ì—¬ëŸ¬ íƒœìŠ¤í¬ì— ëŒ€í•œ ì–´ëŒ‘í„° êµì²´ ê°€ëŠ¥
    - ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ìœ ì§€
        """,
        
        "4. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°": """
    - r (rank): LoRA í–‰ë ¬ì˜ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 4-64)
    - alpha: LoRA ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
    - dropout: ê³¼ì í•© ë°©ì§€
    - target_modules: LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´
        """,
        
        "5. ì‚¬ìš© ì‚¬ë¡€": """
    - ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ìƒì„±
    - ë‹¤êµ­ì–´ ì§€ì› ì¶”ê°€
    - íŠ¹ì • ì‘ì—… ìŠ¤íƒ€ì¼ í•™ìŠµ
    - ê°œì¸í™”ëœ AI ì–´ì‹œìŠ¤í„´íŠ¸
        """
    }
    
    for title, content in concepts.items():
        print(f"\n{title}")
        print(content)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "train":
            demo_training()
        elif mode == "concepts":
            explain_lora_concepts()
        else:
            print("Usage: python lora_finetuning.py [train|concepts]")
    else:
        demo_lora_setup()
        
        print("\n" + "=" * 60)
        print("ğŸ’¡ ë” ë§ì€ ì •ë³´:")
        print("  python lora_finetuning.py train     # í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜")
        print("  python lora_finetuning.py concepts  # LoRA ê°œë… ì„¤ëª…")