{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "1. RAGì˜ ê°œë…ê³¼ í•„ìš”ì„± ì´í•´\n",
    "2. ì„ë² ë”©ê³¼ ë²¡í„° ê²€ìƒ‰ êµ¬í˜„\n",
    "3. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "4. ì‹¤ì „ RAG ì‹œìŠ¤í…œ ê°œë°œ\n",
    "\n",
    "## ğŸ“š RAGë€ ë¬´ì—‡ì¸ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜\n",
    "\n",
    "### RAGì˜ í•µì‹¬ ê°œë…\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              RAG Pipeline                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   1. ë¬¸ì„œ ìˆ˜ì§‘     â†’    2. ì²­í‚¹              â”‚\n",
    "â”‚   (Documents)          (Chunking)           â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   3. ì„ë² ë”© ìƒì„±   â†’    4. ë²¡í„° DB ì €ì¥      â”‚\n",
    "â”‚   (Embedding)          (Vector Store)       â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   5. ì¿¼ë¦¬ ì„ë² ë”©   â†’    6. ìœ ì‚¬ë„ ê²€ìƒ‰       â”‚\n",
    "â”‚   (Query Embed)        (Similarity Search)  â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   7. ì»¨í…ìŠ¤íŠ¸ ìƒì„±  â†’   8. LLM ì‘ë‹µ ìƒì„±     â”‚\n",
    "â”‚   (Context)            (Generation)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ì™œ RAGê°€ í•„ìš”í•œê°€?\n",
    "\n",
    "1. **ìµœì‹  ì •ë³´**: LLMì˜ í•™ìŠµ ë°ì´í„° ì‹œì  ì´í›„ ì •ë³´ ì œê³µ\n",
    "2. **ì •í™•ì„±**: í™˜ê°(Hallucination) ê°ì†Œ\n",
    "3. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ í™œìš©\n",
    "4. **ì¶œì²˜ ì œê³µ**: ë‹µë³€ì˜ ê·¼ê±° ëª…ì‹œ ê°€ëŠ¥\n",
    "5. **ë¹„ìš© íš¨ìœ¨**: íŒŒì¸íŠœë‹ ì—†ì´ ì§€ì‹ í™•ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy ë²„ì „ ì¶©ëŒ í•´ê²° ë° RAG ì‹œìŠ¤í…œ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "# \n",
    "# ì£¼ì˜: NumPy 2.xëŠ” ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ íŒ¨í‚¤ì§€ì™€ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ \n",
    "# 1.x ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œí•˜ê³  í˜¸í™˜ë˜ëŠ” ë²„ì „ ì¡°í•©ìœ¼ë¡œ ì„¤ì¹˜\n",
    "\n",
    "# 1. ê¸°ì¡´ íŒ¨í‚¤ì§€ ì œê±° ë° NumPy ë‹¤ìš´ê·¸ë ˆì´ë“œ\n",
    "!pip uninstall numpy torch torchvision transformers sentence-transformers -y\n",
    "!pip install numpy==1.24.3\n",
    "\n",
    "# 2. í˜¸í™˜ë˜ëŠ” ë²„ì „ ì¡°í•©ìœ¼ë¡œ ì¬ì„¤ì¹˜\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers==4.37.0 sentence-transformers==2.2.2\n",
    "\n",
    "# 3. ì¶”ê°€ í•„ìš” íŒ¨í‚¤ì§€\n",
    "!pip install chromadb langchain langchain-community ollama scikit-learn\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 2.3.2 ì •ìƒ ë¡œë“œë¨\n",
      "SentenceTransformer ì •ìƒ ë¡œë“œë¨\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"numpy {np.__version__} ì •ìƒ ë¡œë“œë¨\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"SentenceTransformer ì •ìƒ ë¡œë“œë¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì¹˜ í™•ì¸:\n",
      "âœ“ NumPy: 2.3.2\n",
      "âœ“ PyTorch: 2.8.0\n",
      "âœ“ torchvision: 0.23.0\n",
      "âœ“ transformers: 4.56.0\n",
      "âœ“ sentence-transformers ì •ìƒ ë¡œë“œë¨\n",
      "\n",
      "ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559b0c3033346bea9cb06244d485dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2d3dd9426e4db98e7581c65ff6297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b22a6a754c349619bbd8516e3a91bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be3032348ea4410910c9fbe38de6459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b7f9b09a4d4abc8307bab38e3ec140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1a0c9691b944a5839f091b84d47994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801176743ff5487397b1ac030cccceb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f77edb26ad0493db8746ae31578e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85e124cfa20431e903923f2ee5ca819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbda99d66a64021afd188f1a80dabd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9ca9d310648019f1e670c85a802c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ì„ë² ë”© ìƒì„± ì„±ê³µ: shape=(1, 384)\n",
      "\n",
      "âœ… ëª¨ë“  íŒ¨í‚¤ì§€ê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ì´ì œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸ ë° í˜¸í™˜ì„± ê²€ì‚¬\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def verify_installation():\n",
    "    print(\"ì„¤ì¹˜ í™•ì¸:\")\n",
    "    \n",
    "    try:\n",
    "        # NumPy í™•ì¸\n",
    "        import numpy\n",
    "        print(f\"âœ“ NumPy: {numpy.__version__}\")\n",
    "        \n",
    "        # PyTorch í™•ì¸\n",
    "        import torch\n",
    "        print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "        \n",
    "        # Torchvision í™•ì¸\n",
    "        import torchvision\n",
    "        print(f\"âœ“ torchvision: {torchvision.__version__}\")\n",
    "        \n",
    "        # Transformers í™•ì¸\n",
    "        import transformers\n",
    "        print(f\"âœ“ transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # Sentence Transformers í…ŒìŠ¤íŠ¸\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        print(\"âœ“ sentence-transformers ì •ìƒ ë¡œë“œë¨\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\nì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        test_text = [\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥\"]\n",
    "        embedding = model.encode(test_text)\n",
    "        print(f\"âœ“ ì„ë² ë”© ìƒì„± ì„±ê³µ: shape={embedding.shape}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        print(\"\\ní•´ê²° ë°©ë²•:\")\n",
    "        print(\"1. ì»¤ë„ì„ ì¬ì‹œì‘í•˜ì„¸ìš” (Kernel > Restart)\")\n",
    "        print(\"2. ìœ„ì˜ ì„¤ì¹˜ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "        print(\"3. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰:\")\n",
    "        print(\"   pip uninstall numpy torch torchvision transformers sentence-transformers -y\")\n",
    "        print(\"   pip install numpy==1.24.3\")\n",
    "        print(\"   pip install torch==2.1.0 torchvision==0.16.0\")\n",
    "        print(\"   pip install transformers==4.37.0 sentence-transformers==2.2.2\")\n",
    "        return False\n",
    "\n",
    "# ì„¤ì¹˜ í™•ì¸ ì‹¤í–‰\n",
    "if verify_installation():\n",
    "    print(\"\\nâœ… ëª¨ë“  íŒ¨í‚¤ì§€ê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"ì´ì œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì¹˜ í™•ì¸:\n",
      "âœ“ ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œë¨\n",
      "PyTorch: 2.8.0\n",
      "torchvision: 0.23.0\n",
      "transformers: 4.56.0\n",
      "âœ“ SentenceTransformer ì •ìƒ ì‘ë™\n"
     ]
    }
   ],
   "source": [
    "# íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸ ë° í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°\n",
    "\n",
    "def verify_installation():\n",
    "    print(\"ì„¤ì¹˜ í™•ì¸:\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        print(\"âœ“ ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œë¨\")\n",
    "        print(f\"PyTorch: {torch.__version__}\")\n",
    "        print(f\"torchvision: {torchvision.__version__}\")\n",
    "        print(f\"transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"âœ“ SentenceTransformer ì •ìƒ ì‘ë™\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install sentence-transformers chromadb langchain langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„ë² ë”© ê¸°ì´ˆ\n",
    "\n",
    "### ì„ë² ë”©ì´ë€?\n",
    "í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë‹´ì€ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ì°¨ì›: (4, 384)\n",
      "ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): [-0.06128301  0.02552284 -0.02994079 -0.02846024 -0.04995072 -0.13498838\n",
      "  0.03824809  0.03610769 -0.06114284 -0.05800613]\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "texts = [\n",
    "    \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤\",\n",
    "    \"íŒŒì´ì¬ì€ ì½”ë”© ì–¸ì–´ì…ë‹ˆë‹¤\",\n",
    "    \"ê³ ì–‘ì´ëŠ” ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {embeddings.shape}\")\n",
    "print(f\"ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i < j:\n",
    "            print(f\"'{text1}' vs '{text2}': {similarities[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"knowledge_base\"):\n",
    "        # ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸\n",
    "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        \n",
    "        # ì„ë² ë”© ëª¨ë¸\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(collection_name)\n",
    "            print(f\"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œë¨\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(collection_name)\n",
    "            print(f\"ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„±ë¨\")\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€\"\"\"\n",
    "        embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # ChromaDBì— ì¶”ê°€\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        print(f\"{len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\")\n",
    "    \n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"ìœ ì‚¬ë„ ê²€ìƒ‰\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        return results\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "vector_store = VectorStore(\"rag_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "knowledge_base = [\n",
    "    \"Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ìœ¼ë¡œ ìœ ëª…í•˜ë©°, ë“¤ì—¬ì“°ê¸°ë¡œ ì½”ë“œ ë¸”ë¡ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ê¸°ë²•ì…ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\",\n",
    "    \"ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë‹´ì€ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "# metadatas = [\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"ML\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"DL\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"RAG\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"LangChain\", \"category\": \"tool\"},\n",
    "#     {\"topic\": \"VectorDB\", \"category\": \"database\"},\n",
    "#     {\"topic\": \"Embedding\", \"category\": \"AI\"}\n",
    "# ]\n",
    "\n",
    "# ë²¡í„° DBì— ì¶”ê°€\n",
    "# vector_store.add_documents(knowledge_base, metadatas)\n",
    "vector_store.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "queries = [\n",
    "    \"íŒŒì´ì¬ì˜ íŠ¹ì§•ì€?\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„\",\n",
    "    \"RAG ì‹œìŠ¤í…œì´ë€?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nğŸ” Query: {query}\")\n",
    "    results = vector_store.search(query, n_results=2)\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        distance = results['distances'][0][i] if results['distances'] else 0\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "        print(f\"  [{i+1}] (ê±°ë¦¬: {distance:.3f})\")\n",
    "        print(f\"      {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"ì™„ì „í•œ RAG ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        # LLM ì´ˆê¸°í™”\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        \n",
    "        # ë²¡í„° ìŠ¤í† ì–´\n",
    "        self.vector_store = VectorStore(\"rag_system\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def add_document(self, text: str, source: str = \"unknown\"):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€ (ìë™ ì²­í‚¹)\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # ë²¡í„° DBì— ì¶”ê°€\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, use_thinking: bool = False):\n",
    "        \"\"\"RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "        \n",
    "        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        search_results = self.vector_store.search(question, n_results=top_k)\n",
    "        \n",
    "        # 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context_docs = search_results['documents'][0] if search_results['documents'] else []\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "        \n",
    "        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        prompt = f\"\"\"\n",
    "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  \"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "        \n",
    "        # Thinking Mode ì ìš©\n",
    "        if use_thinking:\n",
    "            prompt = f\"/think {prompt}\"\n",
    "        \n",
    "        # 4. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"sources\": context_docs,\n",
    "            \"num_sources\": len(context_docs)\n",
    "        }\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = RAGSystem()\n",
    "print(\"âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì¶”ê°€\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
    "    \n",
    "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
    "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
    "    3. íœ´ê°€: ì—°ì°¨ 15ì¼, ë³‘ê°€ 10ì¼\n",
    "    4. êµìœ¡ ì§€ì›: ì—°ê°„ 200ë§Œì› í•œë„\n",
    "    5. íšŒì˜: ë§¤ì£¼ ì›”ìš”ì¼ 10ì‹œ íŒ€ ë¯¸íŒ…\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    í”„ë¡œì íŠ¸ ê°€ì´ë“œë¼ì¸\n",
    "    \n",
    "    1. ì½”ë“œ ë¦¬ë·°: ëª¨ë“  PRì€ 2ëª… ì´ìƒì˜ ë¦¬ë·° í•„ìš”\n",
    "    2. í…ŒìŠ¤íŠ¸: ì½”ë“œ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ìœ ì§€\n",
    "    3. ë¬¸ì„œí™”: ëª¨ë“  ê³µê°œ APIëŠ” ë¬¸ì„œí™” í•„ìˆ˜\n",
    "    4. ë¸Œëœì¹˜: feature/*, bugfix/*, hotfix/* ê·œì¹™ ì¤€ìˆ˜\n",
    "    5. ë°°í¬: ë§¤ì£¼ í™”ìš”ì¼, ëª©ìš”ì¼ ì •ê¸° ë°°í¬\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    ê¸°ìˆ  ìŠ¤íƒ\n",
    "    \n",
    "    - ë°±ì—”ë“œ: Python (FastAPI), PostgreSQL\n",
    "    - í”„ë¡ íŠ¸ì—”ë“œ: React, TypeScript, TailwindCSS\n",
    "    - ì¸í”„ë¼: AWS, Docker, Kubernetes\n",
    "    - CI/CD: GitHub Actions, ArgoCD\n",
    "    - ëª¨ë‹ˆí„°ë§: Prometheus, Grafana, Sentry\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    chunks = rag.add_document(doc, source=f\"document_{i+1}\")\n",
    "    print(f\"ë¬¸ì„œ {i+1}: {chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "questions = [\n",
    "    \"ì¬íƒê·¼ë¬´ëŠ” ì–¸ì œ ê°€ëŠ¥í•œê°€ìš”?\",\n",
    "    \"ì½”ë“œ ë¦¬ë·° ê·œì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ëŠ” ì–´ë–¤ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?\",\n",
    "    \"ì ì‹¬ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?\",\n",
    "    \"CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\"  # ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì§ˆë¬¸\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ë‹µë³€: {result['answer']}\")\n",
    "    print(f\"\\nğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ ({result['num_sources']}ê°œ):\")\n",
    "    for i, source in enumerate(result['sources'][:2]):\n",
    "        print(f\"  [{i+1}] {source[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê³ ê¸‰ RAG ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"ê³ ê¸‰ RAG ê¸°ë²• êµ¬í˜„\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\"):\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ + í‚¤ì›Œë“œ)\"\"\"\n",
    "        \n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # 1. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25 ê°„ë‹¨ êµ¬í˜„)\n",
    "        query_words = set(query.lower().split())\n",
    "        keyword_scores = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc.lower().split())\n",
    "            overlap = len(query_words & doc_words)\n",
    "            keyword_scores.append(overlap / max(len(query_words), 1))\n",
    "        \n",
    "        keyword_scores = np.array(keyword_scores)\n",
    "        \n",
    "        # 3. ì ìˆ˜ ê²°í•© (ê°€ì¤‘ í‰ê· )\n",
    "        combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
    "        \n",
    "        # 4. ìƒìœ„ kê°œ ì„ íƒ\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": combined_scores[idx],\n",
    "                \"semantic_score\": semantic_scores[idx],\n",
    "                \"keyword_score\": keyword_scores[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def query_expansion(self, query: str) -> List[str]:\n",
    "        \"\"\"ì¿¼ë¦¬ í™•ì¥ (ê´€ë ¨ ìš©ì–´ ì¶”ê°€)\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ ìš©ì–´ë‚˜ ë™ì˜ì–´ë¥¼ 3ê°œ ì œì‹œí•´ì£¼ì„¸ìš”.\n",
    "ê° ìš©ì–´ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "ìœ ì‚¬ ìš©ì–´:\n",
    "\"\"\"\n",
    "        \n",
    "        expansion = self.llm.invoke(prompt)\n",
    "        expanded_terms = [term.strip() for term in expansion.split(',')]\n",
    "        \n",
    "        return [query] + expanded_terms[:3]\n",
    "    \n",
    "    def rerank_results(self, query: str, documents: List[str]) -> List[Dict]:\n",
    "        \"\"\"ì¬ìˆœìœ„ ì§€ì • (Cross-encoder ìŠ¤íƒ€ì¼)\"\"\"\n",
    "        \n",
    "        reranked = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ LLMìœ¼ë¡œ í‰ê°€\n",
    "            prompt = f\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 0-10 ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.\n",
    "ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "ë¬¸ì„œ: {doc[:200]}\n",
    "\n",
    "ì ìˆ˜:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                score = float(self.llm.invoke(prompt).strip())\n",
    "            except:\n",
    "                score = 5.0\n",
    "            \n",
    "            reranked.append({\n",
    "                \"document\": doc,\n",
    "                \"relevance_score\": score\n",
    "            })\n",
    "        \n",
    "        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬\n",
    "        reranked.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        new_embeddings = self.embedder.encode(documents)\n",
    "        \n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "\n",
    "# ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "adv_rag = AdvancedRAG()\n",
    "\n",
    "# ë¬¸ì„œ ì¶”ê°€\n",
    "tech_docs = [\n",
    "    \"Pythonì€ ë™ì  íƒ€ì´í•‘ì„ ì§€ì›í•˜ëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"JavaScriptëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"DockerëŠ” ì»¨í…Œì´ë„ˆí™” ê¸°ìˆ ì„ ì œê³µí•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.\",\n",
    "    \"KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤.\",\n",
    "    \"Gitì€ ë¶„ì‚° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "adv_rag.add_documents(tech_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "query = \"ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ë„êµ¬\"\n",
    "\n",
    "print(\"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "results = adv_rag.hybrid_search(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] ì¢…í•© ì ìˆ˜: {result['score']:.3f}\")\n",
    "    print(f\"    ì˜ë¯¸ ì ìˆ˜: {result['semantic_score']:.3f}\")\n",
    "    print(f\"    í‚¤ì›Œë“œ ì ìˆ˜: {result['keyword_score']:.3f}\")\n",
    "    print(f\"    ë¬¸ì„œ: {result['document']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸\n",
    "original_query = \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\"\n",
    "\n",
    "print(f\"ì›ë³¸ ì¿¼ë¦¬: {original_query}\")\n",
    "expanded = adv_rag.query_expansion(original_query)\n",
    "print(f\"í™•ì¥ëœ ì¿¼ë¦¬: {expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ì „ í”„ë¡œì íŠ¸: PDF ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQA:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag = RAGSystem()\n",
    "        self.sources = {}\n",
    "    \n",
    "    def load_text_file(self, filepath: str, source_name: str = None):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            source = source_name or filepath\n",
    "            chunks = self.rag.add_document(content, source)\n",
    "            self.sources[source] = chunks\n",
    "            \n",
    "            print(f\"âœ… '{source}' ë¡œë“œ ì™„ë£Œ ({chunks} ì²­í¬)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"ëŒ€í™”í˜• Q&A ì„¸ì…˜\"\"\"\n",
    "        print(\"\\nğŸ’¬ ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ\")\n",
    "        print(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('ì¢…ë£Œ' ì…ë ¥ì‹œ ì¢…ë£Œ)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"ì§ˆë¬¸: \")\n",
    "            \n",
    "            if question.lower() in ['ì¢…ë£Œ', 'exit', 'quit']:\n",
    "                print(\"ğŸ‘‹ Q&A ì„¸ì…˜ ì¢…ë£Œ\")\n",
    "                break\n",
    "            \n",
    "            # ë‹µë³€ ìƒì„±\n",
    "            result = self.rag.query(question)\n",
    "            \n",
    "            print(f\"\\në‹µë³€: {result['answer']}\\n\")\n",
    "            print(f\"(ì°¸ê³ : {result['num_sources']}ê°œ ì†ŒìŠ¤ í™œìš©)\\n\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def batch_qa(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"ë°°ì¹˜ Q&A ì²˜ë¦¬\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"ì²˜ë¦¬ì¤‘ [{i}/{len(questions)}]: {question[:50]}...\")\n",
    "            result = self.rag.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "doc_qa = DocumentQA()\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "sample_doc = \"\"\"\n",
    "RAG ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "1. ì†Œê°œ\n",
    "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
    "ê¸°ì¡´ LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
    "- ë¬¸ì„œ ì²˜ë¦¬: í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• \n",
    "- ì„ë² ë”©: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- ë²¡í„° DB: ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰\n",
    "- LLM: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±\n",
    "\n",
    "3. ì¥ì \n",
    "- í™˜ê° í˜„ìƒ ê°ì†Œ\n",
    "- ì¶œì²˜ ì œê³µ ê°€ëŠ¥\n",
    "- ë„ë©”ì¸ íŠ¹í™” ê°€ëŠ¥\n",
    "- ì‹¤ì‹œê°„ ì •ë³´ ë°˜ì˜\n",
    "\n",
    "4. í™œìš© ë¶„ì•¼\n",
    "- ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡\n",
    "- ê¸°ìˆ  ë¬¸ì„œ Q&A\n",
    "- ë²•ë¥  ìë¬¸ ì‹œìŠ¤í…œ\n",
    "- ì˜ë£Œ ì •ë³´ ê²€ìƒ‰\n",
    "\"\"\"\n",
    "\n",
    "# ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "with open('rag_guide.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "doc_qa.load_text_file('rag_guide.txt', 'RAG ê°€ì´ë“œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ Q&A í…ŒìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"RAGì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ”?\",\n",
    "    \"RAG ì‹œìŠ¤í…œì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "    \"RAGëŠ” ì–´ë–¤ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "results = doc_qa.batch_qa(test_questions)\n",
    "\n",
    "print(\"\\nğŸ“Š ë°°ì¹˜ Q&A ê²°ê³¼:\")\n",
    "for i, (q, r) in enumerate(zip(test_questions, results), 1):\n",
    "    print(f\"\\n[{i}] Q: {q}\")\n",
    "    print(f\"    A: {r['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG ì‹œìŠ¤í…œ í‰ê°€ ë° ê°œì„ \n",
    "\n",
    "### í‰ê°€ ë©”íŠ¸ë¦­\n",
    "1. **ê²€ìƒ‰ í’ˆì§ˆ**: Precision, Recall, MRR\n",
    "2. **ìƒì„± í’ˆì§ˆ**: BLEU, ROUGE, ì˜ë¯¸ì  ìœ ì‚¬ë„\n",
    "3. **ì „ì²´ í’ˆì§ˆ**: ì •í™•ë„, ê´€ë ¨ì„±, ì™„ì „ì„±\n",
    "\n",
    "### ê°œì„  ë°©ë²•\n",
    "1. **ì²­í‚¹ ìµœì í™”**: ë¬¸ì„œ íŠ¹ì„±ì— ë§ëŠ” ì²­í¬ í¬ê¸°\n",
    "2. **ì„ë² ë”© ëª¨ë¸ ì„ íƒ**: ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©\n",
    "3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ì˜ë¯¸ + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©\n",
    "4. **ì¬ìˆœìœ„**: Cross-encoderë¡œ ì •í™•ë„ í–¥ìƒ\n",
    "5. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë” ë‚˜ì€ ì»¨í…ìŠ¤íŠ¸ í™œìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. **ê¸°ë³¸ ê³¼ì œ**:\n",
    "   - ìì‹ ì˜ ë¬¸ì„œë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "   - ë‹¤ì–‘í•œ ì²­í¬ í¬ê¸° ì‹¤í—˜\n",
    "   - ê²€ìƒ‰ ê²°ê³¼ í‰ê°€\n",
    "\n",
    "2. **ì‹¬í™” ê³¼ì œ**:\n",
    "   - ë©€í‹°ëª¨ë‹¬ RAG (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)\n",
    "   - ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ\n",
    "   - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ RAG\n",
    "\n",
    "3. **í”„ë¡œì íŠ¸**:\n",
    "   - ê¸°ìˆ  ë¬¸ì„œ Q&A ë´‡\n",
    "   - ë…¼ë¬¸ ê²€ìƒ‰ ë° ìš”ì•½ ì‹œìŠ¤í…œ\n",
    "   - ì½”ë“œë² ì´ìŠ¤ ì§€ì‹ ì–´ì‹œìŠ¤í„´íŠ¸\n",
    "\n",
    "## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "- [RAG ë…¼ë¬¸](https://arxiv.org/abs/2005.11401)\n",
    "- [ChromaDB ë¬¸ì„œ](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [LlamaIndex RAG ê°€ì´ë“œ](https://gpt-index.readthedocs.io/)\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LoRA íŒŒì¸íŠœë‹**ì„ í†µí•´ ëª¨ë¸ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
