{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RAG (Retrieval-Augmented Generation) 시스템 구축\n",
    "\n",
    "## 🎯 학습 목표\n",
    "1. RAG의 개념과 필요성 이해\n",
    "2. 임베딩과 벡터 검색 구현\n",
    "3. 문서 처리 파이프라인 구축\n",
    "4. 실전 RAG 시스템 개발\n",
    "\n",
    "## 📚 RAG란 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG 시스템 아키텍처\n",
    "\n",
    "### RAG의 핵심 개념\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              RAG Pipeline                   │\n",
    "├─────────────────────────────────────────────┤\n",
    "│   1. 문서 수집     →    2. 청킹              │\n",
    "│   (Documents)          (Chunking)           │\n",
    "│                                             │\n",
    "│   3. 임베딩 생성   →    4. 벡터 DB 저장      │\n",
    "│   (Embedding)          (Vector Store)       │\n",
    "│                                             │\n",
    "│   5. 쿼리 임베딩   →    6. 유사도 검색       │\n",
    "│   (Query Embed)        (Similarity Search)  │\n",
    "│                                             │\n",
    "│   7. 컨텍스트 생성  →   8. LLM 응답 생성     │\n",
    "│   (Context)            (Generation)         │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 왜 RAG가 필요한가?\n",
    "\n",
    "1. **최신 정보**: LLM의 학습 데이터 시점 이후 정보 제공\n",
    "2. **정확성**: 환각(Hallucination) 감소\n",
    "3. **커스터마이징**: 도메인 특화 지식 활용\n",
    "4. **출처 제공**: 답변의 근거 명시 가능\n",
    "5. **비용 효율**: 파인튜닝 없이 지식 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conda로 호환되는 버전 설치\n",
    "- conda install numpy=1.24\n",
    "- conda install pytorch torchvision torchaudio -c pytorch\n",
    "- conda install -c conda-forge transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설치 확인:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 모든 패키지 정상 로드됨\n",
      "PyTorch: 2.2.2\n",
      "torchvision: 0.17.2\n",
      "transformers: 4.53.3\n",
      "✓ SentenceTransformer 정상 작동\n"
     ]
    }
   ],
   "source": [
    "def verify_installation():\n",
    "    print(\"설치 확인:\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        print(\"✓ 모든 패키지 정상 로드됨\")\n",
    "        print(f\"PyTorch: {torch.__version__}\")\n",
    "        print(f\"torchvision: {torchvision.__version__}\")\n",
    "        print(f\"transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # 간단한 테스트\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ SentenceTransformer 정상 작동\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 에러 발생: {e}\")\n",
    "\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 설치\n",
    "!pip install sentence-transformers chromadb langchain langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 패키지 로드 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"✅ 패키지 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 기초\n",
    "\n",
    "### 임베딩이란?\n",
    "텍스트를 의미를 담은 벡터(숫자 배열)로 변환하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 차원: (4, 384)\n",
      "첫 번째 텍스트의 임베딩 (처음 10개 값): [-0.06128298  0.02552282 -0.0299408  -0.02846022 -0.04995069 -0.13498837\n",
      "  0.03824809  0.03610768 -0.06114289 -0.05800608]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 텍스트 임베딩 생성\n",
    "texts = [\n",
    "    \"Python은 프로그래밍 언어입니다\",\n",
    "    \"파이썬은 코딩 언어입니다\",\n",
    "    \"고양이는 동물입니다\",\n",
    "    \"머신러닝은 AI의 한 분야입니다\"\n",
    "]\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "print(f\"임베딩 차원: {embeddings.shape}\")\n",
    "print(f\"첫 번째 텍스트의 임베딩 (처음 10개 값): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 간 유사도 매트릭스:\n",
      "'Python은 프로그래밍 언어입니다' vs '파이썬은 코딩 언어입니다': 0.450\n",
      "'Python은 프로그래밍 언어입니다' vs '고양이는 동물입니다': 0.447\n",
      "'Python은 프로그래밍 언어입니다' vs '머신러닝은 AI의 한 분야입니다': 0.416\n",
      "'파이썬은 코딩 언어입니다' vs '고양이는 동물입니다': 0.691\n",
      "'파이썬은 코딩 언어입니다' vs '머신러닝은 AI의 한 분야입니다': 0.561\n",
      "'고양이는 동물입니다' vs '머신러닝은 AI의 한 분야입니다': 0.565\n"
     ]
    }
   ],
   "source": [
    "# 유사도 계산\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"텍스트 간 유사도 매트릭스:\")\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i < j:\n",
    "            print(f\"'{text1}' vs '{text2}': {similarities[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 벡터 데이터베이스 구축\n",
    "\n",
    "ChromaDB를 사용하여 벡터 검색 시스템을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 컬렉션 'rag_demo' 로드됨\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"knowledge_base\"):\n",
    "        # 새로운 ChromaDB 클라이언트\n",
    "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        \n",
    "        # 임베딩 모델\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 컬렉션 생성/로드\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(collection_name)\n",
    "            print(f\"기존 컬렉션 '{collection_name}' 로드됨\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(collection_name)\n",
    "            print(f\"새 컬렉션 '{collection_name}' 생성됨\")\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"문서 추가\"\"\"\n",
    "        embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # ChromaDB에 추가\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        print(f\"{len(documents)}개 문서 추가됨\")\n",
    "    \n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"유사도 검색\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        return results\n",
    "\n",
    "# 사용 예시\n",
    "vector_store = VectorStore(\"rag_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8개 문서 추가됨\n"
     ]
    }
   ],
   "source": [
    "# 지식 베이스 구축\n",
    "knowledge_base = [\n",
    "    \"Python은 1991년 귀도 반 로섬이 개발한 고급 프로그래밍 언어입니다.\",\n",
    "    \"Python은 간결하고 읽기 쉬운 문법으로 유명하며, 들여쓰기로 코드 블록을 구분합니다.\",\n",
    "    \"머신러닝은 데이터에서 패턴을 학습하는 인공지능의 한 분야입니다.\",\n",
    "    \"딥러닝은 인공 신경망을 사용하는 머신러닝의 하위 분야입니다.\",\n",
    "    \"RAG는 Retrieval-Augmented Generation의 약자로, 검색 기반 생성 기법입니다.\",\n",
    "    \"LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.\",\n",
    "    \"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 시스템입니다.\",\n",
    "    \"임베딩은 텍스트를 의미를 담은 벡터로 변환하는 과정입니다.\"\n",
    "]\n",
    "\n",
    "# 메타데이터 추가\n",
    "# metadatas = [\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"ML\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"DL\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"RAG\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"LangChain\", \"category\": \"tool\"},\n",
    "#     {\"topic\": \"VectorDB\", \"category\": \"database\"},\n",
    "#     {\"topic\": \"Embedding\", \"category\": \"AI\"}\n",
    "# ]\n",
    "\n",
    "# 벡터 DB에 추가\n",
    "# vector_store.add_documents(knowledge_base, metadatas)\n",
    "vector_store.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: 파이썬의 특징은?\n",
      "  [1] (거리: 0.956)\n",
      "      머신러닝은 데이터에서 패턴을 학습하는 인공지능의 한 분야입니다....\n",
      "  [2] (거리: 1.008)\n",
      "      벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 시스템입니다....\n",
      "\n",
      "🔍 Query: 인공지능과 머신러닝의 관계\n",
      "  [1] (거리: 0.343)\n",
      "      딥러닝은 인공 신경망을 사용하는 머신러닝의 하위 분야입니다....\n",
      "  [2] (거리: 0.409)\n",
      "      머신러닝은 데이터에서 패턴을 학습하는 인공지능의 한 분야입니다....\n",
      "\n",
      "🔍 Query: RAG 시스템이란?\n",
      "  [1] (거리: 1.132)\n",
      "      머신러닝은 데이터에서 패턴을 학습하는 인공지능의 한 분야입니다....\n",
      "  [2] (거리: 1.155)\n",
      "      벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 시스템입니다....\n"
     ]
    }
   ],
   "source": [
    "# 검색 테스트\n",
    "queries = [\n",
    "    \"파이썬의 특징은?\",\n",
    "    \"인공지능과 머신러닝의 관계\",\n",
    "    \"RAG 시스템이란?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    results = vector_store.search(query, n_results=2)\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        distance = results['distances'][0][i] if results['distances'] else 0\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "        print(f\"  [{i+1}] (거리: {distance:.3f})\")\n",
    "        print(f\"      {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 완전한 RAG 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 컬렉션 'rag_system' 로드됨\n",
      "✅ RAG 시스템 준비 완료\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"완전한 RAG 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        # LLM 초기화\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        \n",
    "        # 벡터 스토어\n",
    "        self.vector_store = VectorStore(\"rag_system\")\n",
    "        \n",
    "        # 텍스트 분할기\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def add_document(self, text: str, source: str = \"unknown\"):\n",
    "        \"\"\"문서 추가 (자동 청킹)\"\"\"\n",
    "        # 텍스트를 청크로 분할\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # 벡터 DB에 추가\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, use_thinking: bool = False):\n",
    "        \"\"\"RAG 기반 질의응답\"\"\"\n",
    "        \n",
    "        # 1. 관련 문서 검색\n",
    "        search_results = self.vector_store.search(question, n_results=top_k)\n",
    "        \n",
    "        # 2. 컨텍스트 생성\n",
    "        context_docs = search_results['documents'][0] if search_results['documents'] else []\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "        \n",
    "        # 3. 프롬프트 구성\n",
    "        prompt = f\"\"\"\n",
    "다음 컨텍스트를 참고하여 질문에 답변해주세요.\n",
    "컨텍스트에 없는 내용은 추측하지 말고 \"정보가 없습니다\"라고 답하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "        \n",
    "        # Thinking Mode 적용\n",
    "        if use_thinking:\n",
    "            prompt = f\"/think {prompt}\"\n",
    "        \n",
    "        # 4. LLM으로 응답 생성\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"sources\": context_docs,\n",
    "            \"num_sources\": len(context_docs)\n",
    "        }\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = RAGSystem()\n",
    "print(\"✅ RAG 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1개 문서 추가됨\n",
      "문서 1: 1개 청크로 분할\n",
      "1개 문서 추가됨\n",
      "문서 2: 1개 청크로 분할\n",
      "1개 문서 추가됨\n",
      "문서 3: 1개 청크로 분할\n"
     ]
    }
   ],
   "source": [
    "# 문서 추가\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    회사 규정 문서\n",
    "    \n",
    "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
    "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
    "    3. 휴가: 연차 15일, 병가 10일\n",
    "    4. 교육 지원: 연간 200만원 한도\n",
    "    5. 회의: 매주 월요일 10시 팀 미팅\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    프로젝트 가이드라인\n",
    "    \n",
    "    1. 코드 리뷰: 모든 PR은 2명 이상의 리뷰 필요\n",
    "    2. 테스트: 코드 커버리지 80% 이상 유지\n",
    "    3. 문서화: 모든 공개 API는 문서화 필수\n",
    "    4. 브랜치: feature/*, bugfix/*, hotfix/* 규칙 준수\n",
    "    5. 배포: 매주 화요일, 목요일 정기 배포\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    기술 스택\n",
    "    \n",
    "    - 백엔드: Python (FastAPI), PostgreSQL\n",
    "    - 프론트엔드: React, TypeScript, TailwindCSS\n",
    "    - 인프라: AWS, Docker, Kubernetes\n",
    "    - CI/CD: GitHub Actions, ArgoCD\n",
    "    - 모니터링: Prometheus, Grafana, Sentry\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    chunks = rag.add_document(doc, source=f\"document_{i+1}\")\n",
    "    print(f\"문서 {i+1}: {chunks}개 청크로 분할\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "❓ 질문: 재택근무는 언제 가능한가요?\n",
      "\n",
      "💡 답변: <think>\n",
      "Okay, let's see. The user is asking about when remote work is possible based on the provided context. The context mentions that 재택근무 (remote work) is allowed twice a week, specifically on Monday and Friday, and it's recommended. So the answer should state that it's possible on those days. I need to make sure not to add any information that's not in the context. The user might be looking for the exact days, so I should mention Monday and Friday. Also, since the context says \"권장\" which means recommended, maybe include that it's recommended to work from home on those days. But the question is just asking when it's possible, so the main points are the days. Let me check again. The context says \"주 2회 가능 (월/금 권장)\" which translates to \"possible twice a week (Monday/Friday recommended)\". So the answer should be that remote work is possible on Monday and Friday, and those are the recommended days. I should present that clearly without any extra guesses.\n",
      "</think>\n",
      "\n",
      "재택근무는 주 2회 가능하며, 월요일과 금요일이 권장됩니다.\n",
      "\n",
      "📚 참고한 소스 (1개):\n",
      "  [1] 회사 규정 문서\n",
      "\n",
      "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
      "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
      "    3. 휴가: 연차 15일,...\n",
      "\n",
      "============================================================\n",
      "❓ 질문: 코드 리뷰 규칙은 무엇인가요?\n",
      "\n",
      "💡 답변: <think>\n",
      "Okay, let's see. The user is asking about the code review rules based on the provided context. I need to check the context first.\n",
      "\n",
      "Looking at the context, it's a company regulations document with sections on working hours, remote work, vacation, education support, and meetings. The sections listed don't mention anything about code review rules. The topics covered are all about general workplace policies, not specific development practices or code review procedures. \n",
      "\n",
      "Since the question is about code review rules and the context doesn't have any information on that, I should respond that the information isn't available. I mustn't make up any rules or assume anything beyond what's provided. The answer needs to be straightforward, just stating that the information isn't present in the given context.\n",
      "</think>\n",
      "\n",
      "정보가 없습니다.\n",
      "\n",
      "📚 참고한 소스 (1개):\n",
      "  [1] 회사 규정 문서\n",
      "\n",
      "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
      "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
      "    3. 휴가: 연차 15일,...\n",
      "\n",
      "============================================================\n",
      "❓ 질문: 우리 회사는 어떤 프로그래밍 언어를 사용하나요?\n",
      "\n",
      "💡 답변: <think>\n",
      "Okay, let's see. The user is asking which programming languages the company uses. I need to check the provided context to find the answer.\n",
      "\n",
      "Looking at the context, it's a company regulations document. The sections mentioned are working hours, remote work, vacation days, education support, and meetings. There's nothing about programming languages here. The context doesn't mention any specific technologies or languages used by the company. \n",
      "\n",
      "Since the question is about programming languages and the context doesn't have that information, I should respond that the information isn't available. The user might be assuming that the document includes tech stack details, but based on the given content, there's no such data. So the correct answer is to state that the information isn't provided.\n",
      "</think>\n",
      "\n",
      "정보가 없습니다.\n",
      "\n",
      "📚 참고한 소스 (1개):\n",
      "  [1] 회사 규정 문서\n",
      "\n",
      "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
      "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
      "    3. 휴가: 연차 15일,...\n",
      "\n",
      "============================================================\n",
      "❓ 질문: 점심시간은 언제인가요?\n",
      "\n",
      "💡 답변: <think>\n",
      "Okay, let's see. The user is asking about the lunch break time based on the provided context. The context is a company regulations document. Let me check each section.\n",
      "\n",
      "Looking at point 1: 근무 시간 is 9 AM to 6 PM, with 점심시간 12시-1시. So that's 12 PM to 1 PM. The other points are about remote work, vacation days, education support, and meetings. The question is straightforward, just asking for the lunch time. Since the context explicitly states the lunch break is from 12 to 1, I should just answer that. No need to guess anything else. The answer is 12시-1시.\n",
      "</think>\n",
      "\n",
      "점심시간은 오후 12시부터 오후 1시까지입니다.\n",
      "\n",
      "📚 참고한 소스 (1개):\n",
      "  [1] 회사 규정 문서\n",
      "\n",
      "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
      "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
      "    3. 휴가: 연차 15일,...\n",
      "\n",
      "============================================================\n",
      "❓ 질문: CEO는 누구인가요?\n",
      "\n",
      "💡 답변: <think>\n",
      "Okay, let's see. The user is asking about the CEO of the company. I need to check the provided context to find any mention of the CEO. The context given is a company regulations document with sections on working hours, remote work, vacation, education support, and meetings. \n",
      "\n",
      "Looking through each point:\n",
      "1. Working hours: 9 AM to 6 PM, lunch break 12-1 PM.\n",
      "2. Remote work: 2 days a week, Monday and Friday recommended.\n",
      "3. Vacation: 15 annual days, 10 sick days.\n",
      "4. Education support: up to 2 million won per year.\n",
      "5. Meetings: weekly Monday 10 AM team meeting.\n",
      "\n",
      "None of these sections mention the CEO's name or any information about the CEO. The document is about policies and procedures, not personnel details. Since the context doesn't have any information about the CEO, the correct answer should be that there's no information available.\n",
      "</think>\n",
      "\n",
      "정보가 없습니다.\n",
      "\n",
      "📚 참고한 소스 (1개):\n",
      "  [1] 회사 규정 문서\n",
      "\n",
      "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
      "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
      "    3. 휴가: 연차 15일,...\n"
     ]
    }
   ],
   "source": [
    "# RAG 시스템 테스트\n",
    "questions = [\n",
    "    \"재택근무는 언제 가능한가요?\",\n",
    "    \"코드 리뷰 규칙은 무엇인가요?\",\n",
    "    \"우리 회사는 어떤 프로그래밍 언어를 사용하나요?\",\n",
    "    \"점심시간은 언제인가요?\",\n",
    "    \"CEO는 누구인가요?\"  # 컨텍스트에 없는 질문\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"❓ 질문: {question}\")\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    \n",
    "    print(f\"\\n💡 답변: {result['answer']}\")\n",
    "    print(f\"\\n📚 참고한 소스 ({result['num_sources']}개):\")\n",
    "    for i, source in enumerate(result['sources'][:2]):\n",
    "        print(f\"  [{i+1}] {source[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 고급 RAG 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"고급 RAG 기법 구현\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\"):\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5):\n",
    "        \"\"\"하이브리드 검색 (의미 + 키워드)\"\"\"\n",
    "        \n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # 1. 의미 기반 검색\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # 2. 키워드 기반 검색 (BM25 간단 구현)\n",
    "        query_words = set(query.lower().split())\n",
    "        keyword_scores = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc.lower().split())\n",
    "            overlap = len(query_words & doc_words)\n",
    "            keyword_scores.append(overlap / max(len(query_words), 1))\n",
    "        \n",
    "        keyword_scores = np.array(keyword_scores)\n",
    "        \n",
    "        # 3. 점수 결합 (가중 평균)\n",
    "        combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
    "        \n",
    "        # 4. 상위 k개 선택\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": combined_scores[idx],\n",
    "                \"semantic_score\": semantic_scores[idx],\n",
    "                \"keyword_score\": keyword_scores[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def query_expansion(self, query: str) -> List[str]:\n",
    "        \"\"\"쿼리 확장 (관련 용어 추가)\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "다음 질문과 관련된 유사 용어나 동의어를 3개 제시해주세요.\n",
    "각 용어는 쉼표로 구분해주세요.\n",
    "\n",
    "질문: {query}\n",
    "유사 용어:\n",
    "\"\"\"\n",
    "        \n",
    "        expansion = self.llm.invoke(prompt)\n",
    "        expanded_terms = [term.strip() for term in expansion.split(',')]\n",
    "        \n",
    "        return [query] + expanded_terms[:3]\n",
    "    \n",
    "    def rerank_results(self, query: str, documents: List[str]) -> List[Dict]:\n",
    "        \"\"\"재순위 지정 (Cross-encoder 스타일)\"\"\"\n",
    "        \n",
    "        reranked = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # 쿼리와 문서의 관련성을 LLM으로 평가\n",
    "            prompt = f\"\"\"\n",
    "다음 문서가 질문에 얼마나 관련이 있는지 0-10 점수로 평가해주세요.\n",
    "숫자만 답하세요.\n",
    "\n",
    "질문: {query}\n",
    "문서: {doc[:200]}\n",
    "\n",
    "점수:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                score = float(self.llm.invoke(prompt).strip())\n",
    "            except:\n",
    "                score = 5.0\n",
    "            \n",
    "            reranked.append({\n",
    "                \"document\": doc,\n",
    "                \"relevance_score\": score\n",
    "            })\n",
    "        \n",
    "        # 점수 기준 정렬\n",
    "        reranked.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"문서 추가\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        new_embeddings = self.embedder.encode(documents)\n",
    "        \n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "\n",
    "# 고급 RAG 시스템 초기화\n",
    "adv_rag = AdvancedRAG()\n",
    "\n",
    "# 문서 추가\n",
    "tech_docs = [\n",
    "    \"Python은 동적 타이핑을 지원하는 인터프리터 언어입니다.\",\n",
    "    \"JavaScript는 웹 브라우저에서 실행되는 스크립트 언어입니다.\",\n",
    "    \"Docker는 컨테이너화 기술을 제공하는 플랫폼입니다.\",\n",
    "    \"Kubernetes는 컨테이너 오케스트레이션 도구입니다.\",\n",
    "    \"Git은 분산 버전 관리 시스템입니다.\"\n",
    "]\n",
    "\n",
    "adv_rag.add_documents(tech_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 하이브리드 검색 결과:\n",
      "\n",
      "[1] 종합 점수: 0.431\n",
      "    의미 점수: 0.473\n",
      "    키워드 점수: 0.333\n",
      "    문서: Git은 분산 버전 관리 시스템입니다.\n",
      "\n",
      "[2] 종합 점수: 0.361\n",
      "    의미 점수: 0.516\n",
      "    키워드 점수: 0.000\n",
      "    문서: JavaScript는 웹 브라우저에서 실행되는 스크립트 언어입니다.\n",
      "\n",
      "[3] 종합 점수: 0.355\n",
      "    의미 점수: 0.364\n",
      "    키워드 점수: 0.333\n",
      "    문서: Kubernetes는 컨테이너 오케스트레이션 도구입니다.\n"
     ]
    }
   ],
   "source": [
    "# 하이브리드 검색 테스트\n",
    "query = \"컨테이너 관리 도구\"\n",
    "\n",
    "print(\"🔍 하이브리드 검색 결과:\")\n",
    "results = adv_rag.hybrid_search(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] 종합 점수: {result['score']:.3f}\")\n",
    "    print(f\"    의미 점수: {result['semantic_score']:.3f}\")\n",
    "    print(f\"    키워드 점수: {result['keyword_score']:.3f}\")\n",
    "    print(f\"    문서: {result['document']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 쿼리: 프로그래밍 언어\n",
      "확장된 쿼리: ['프로그래밍 언어', '<think>\\nOkay', 'the user is asking for three similar terms or synonyms related to \"programming language.\" Let me start by recalling what a programming language is. It\\'s a formal language used to write instructions that a computer can execute. So', 'synonyms would be terms that refer to the same concept but with different names.\\n\\nFirst']\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 확장 테스트\n",
    "original_query = \"프로그래밍 언어\"\n",
    "\n",
    "print(f\"원본 쿼리: {original_query}\")\n",
    "expanded = adv_rag.query_expansion(original_query)\n",
    "print(f\"확장된 쿼리: {expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실전 프로젝트: PDF 기반 Q&A 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 컬렉션 'rag_system' 로드됨\n",
      "1개 문서 추가됨\n",
      "✅ 'RAG 가이드' 로드 완료 (1 청크)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DocumentQA:\n",
    "    \"\"\"문서 기반 Q&A 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag = RAGSystem()\n",
    "        self.sources = {}\n",
    "    \n",
    "    def load_text_file(self, filepath: str, source_name: str = None):\n",
    "        \"\"\"텍스트 파일 로드\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            source = source_name or filepath\n",
    "            chunks = self.rag.add_document(content, source)\n",
    "            self.sources[source] = chunks\n",
    "            \n",
    "            print(f\"✅ '{source}' 로드 완료 ({chunks} 청크)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 로드 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"대화형 Q&A 세션\"\"\"\n",
    "        print(\"\\n💬 문서 Q&A 시스템\")\n",
    "        print(\"질문을 입력하세요 ('종료' 입력시 종료)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"질문: \")\n",
    "            \n",
    "            if question.lower() in ['종료', 'exit', 'quit']:\n",
    "                print(\"👋 Q&A 세션 종료\")\n",
    "                break\n",
    "            \n",
    "            # 답변 생성\n",
    "            result = self.rag.query(question)\n",
    "            \n",
    "            print(f\"\\n답변: {result['answer']}\\n\")\n",
    "            print(f\"(참고: {result['num_sources']}개 소스 활용)\\n\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def batch_qa(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"배치 Q&A 처리\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"처리중 [{i}/{len(questions)}]: {question[:50]}...\")\n",
    "            result = self.rag.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 시스템 초기화\n",
    "doc_qa = DocumentQA()\n",
    "\n",
    "# 샘플 문서 생성\n",
    "sample_doc = \"\"\"\n",
    "RAG 시스템 사용 가이드\n",
    "\n",
    "1. 소개\n",
    "RAG(Retrieval-Augmented Generation)는 검색과 생성을 결합한 AI 시스템입니다.\n",
    "기존 LLM의 한계를 극복하고 최신 정보를 제공할 수 있습니다.\n",
    "\n",
    "2. 주요 구성 요소\n",
    "- 문서 처리: 텍스트를 작은 청크로 분할\n",
    "- 임베딩: 텍스트를 벡터로 변환\n",
    "- 벡터 DB: 임베딩을 저장하고 검색\n",
    "- LLM: 컨텍스트 기반 응답 생성\n",
    "\n",
    "3. 장점\n",
    "- 환각 현상 감소\n",
    "- 출처 제공 가능\n",
    "- 도메인 특화 가능\n",
    "- 실시간 정보 반영\n",
    "\n",
    "4. 활용 분야\n",
    "- 고객 서비스 챗봇\n",
    "- 기술 문서 Q&A\n",
    "- 법률 자문 시스템\n",
    "- 의료 정보 검색\n",
    "\"\"\"\n",
    "\n",
    "# 임시 파일로 저장\n",
    "with open('rag_guide.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "# 문서 로드\n",
    "doc_qa.load_text_file('rag_guide.txt', 'RAG 가이드')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리중 [1/3]: RAG의 주요 구성 요소는?...\n",
      "처리중 [2/3]: RAG 시스템의 장점을 설명해주세요...\n",
      "처리중 [3/3]: RAG는 어떤 분야에서 활용되나요?...\n",
      "\n",
      "📊 배치 Q&A 결과:\n",
      "\n",
      "[1] Q: RAG의 주요 구성 요소는?\n",
      "    A: <think>\n",
      "Okay, let's see. The user is asking about the main components of RAG. First, I need to check the provided context to see if there's any information related to RAG. The context given is a compa...\n",
      "\n",
      "[2] Q: RAG 시스템의 장점을 설명해주세요\n",
      "    A: <think>\n",
      "Okay, the user is asking about the advantages of a RAG system. Let me check the context provided. The context is a company regulations document that includes details about working hours, remot...\n",
      "\n",
      "[3] Q: RAG는 어떤 분야에서 활용되나요?\n",
      "    A: <think>\n",
      "Okay, let's see. The user is asking about where RAG is applied. The context provided is a company regulations document with sections on work hours, remote work, leave, education support, and m...\n"
     ]
    }
   ],
   "source": [
    "# 배치 Q&A 테스트\n",
    "test_questions = [\n",
    "    \"RAG의 주요 구성 요소는?\",\n",
    "    \"RAG 시스템의 장점을 설명해주세요\",\n",
    "    \"RAG는 어떤 분야에서 활용되나요?\"\n",
    "]\n",
    "\n",
    "results = doc_qa.batch_qa(test_questions)\n",
    "\n",
    "print(\"\\n📊 배치 Q&A 결과:\")\n",
    "for i, (q, r) in enumerate(zip(test_questions, results), 1):\n",
    "    print(f\"\\n[{i}] Q: {q}\")\n",
    "    print(f\"    A: {r['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG 시스템 평가 및 개선\n",
    "\n",
    "### 평가 메트릭\n",
    "1. **검색 품질**: Precision, Recall, MRR\n",
    "2. **생성 품질**: BLEU, ROUGE, 의미적 유사도\n",
    "3. **전체 품질**: 정확도, 관련성, 완전성\n",
    "\n",
    "### 개선 방법\n",
    "1. **청킹 최적화**: 문서 특성에 맞는 청크 크기\n",
    "2. **임베딩 모델 선택**: 도메인 특화 모델 사용\n",
    "3. **하이브리드 검색**: 의미 + 키워드 검색 결합\n",
    "4. **재순위**: Cross-encoder로 정확도 향상\n",
    "5. **프롬프트 엔지니어링**: 더 나은 컨텍스트 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실습 과제\n",
    "\n",
    "1. **기본 과제**:\n",
    "   - 자신의 문서로 RAG 시스템 구축\n",
    "   - 다양한 청크 크기 실험\n",
    "   - 검색 결과 평가\n",
    "\n",
    "2. **심화 과제**:\n",
    "   - 멀티모달 RAG (이미지 + 텍스트)\n",
    "   - 다국어 RAG 시스템\n",
    "   - 실시간 업데이트 RAG\n",
    "\n",
    "3. **프로젝트**:\n",
    "   - 기술 문서 Q&A 봇\n",
    "   - 논문 검색 및 요약 시스템\n",
    "   - 코드베이스 지식 어시스턴트\n",
    "\n",
    "## 📚 추가 학습 자료\n",
    "\n",
    "- [RAG 논문](https://arxiv.org/abs/2005.11401)\n",
    "- [ChromaDB 문서](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [LlamaIndex RAG 가이드](https://gpt-index.readthedocs.io/)\n",
    "\n",
    "## 다음 단계\n",
    "\n",
    "다음 노트북에서는 **LoRA 파인튜닝**을 통해 모델을 커스터마이징하는 방법을 배워보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
