{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RAG (Retrieval-Augmented Generation) 시스템 구축\n",
    "\n",
    "## 🎯 학습 목표\n",
    "1. RAG의 개념과 필요성 이해\n",
    "2. 임베딩과 벡터 검색 구현\n",
    "3. 문서 처리 파이프라인 구축\n",
    "4. 실전 RAG 시스템 개발\n",
    "\n",
    "## 📚 RAG란 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG 시스템 아키텍처\n",
    "\n",
    "### RAG의 핵심 개념\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              RAG Pipeline                   │\n",
    "├─────────────────────────────────────────────┤\n",
    "│   1. 문서 수집     →    2. 청킹              │\n",
    "│   (Documents)          (Chunking)           │\n",
    "│                                             │\n",
    "│   3. 임베딩 생성   →    4. 벡터 DB 저장      │\n",
    "│   (Embedding)          (Vector Store)       │\n",
    "│                                             │\n",
    "│   5. 쿼리 임베딩   →    6. 유사도 검색       │\n",
    "│   (Query Embed)        (Similarity Search)  │\n",
    "│                                             │\n",
    "│   7. 컨텍스트 생성  →   8. LLM 응답 생성     │\n",
    "│   (Context)            (Generation)         │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 왜 RAG가 필요한가?\n",
    "\n",
    "1. **최신 정보**: LLM의 학습 데이터 시점 이후 정보 제공\n",
    "2. **정확성**: 환각(Hallucination) 감소\n",
    "3. **커스터마이징**: 도메인 특화 지식 활용\n",
    "4. **출처 제공**: 답변의 근거 명시 가능\n",
    "5. **비용 효율**: 파인튜닝 없이 지식 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy 버전 충돌 해결 및 RAG 시스템 필수 패키지 설치\n",
    "# \n",
    "# 주의: NumPy 2.x는 많은 머신러닝 패키지와 호환되지 않으므로 \n",
    "# 1.x 버전으로 다운그레이드하고 호환되는 버전 조합으로 설치\n",
    "\n",
    "# 1. 기존 패키지 제거 및 NumPy 다운그레이드\n",
    "!pip uninstall numpy torch torchvision transformers sentence-transformers -y\n",
    "!pip install numpy==1.24.3\n",
    "\n",
    "# 2. 호환되는 버전 조합으로 재설치\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers==4.37.0 sentence-transformers==2.2.2\n",
    "\n",
    "# 3. 추가 필요 패키지\n",
    "!pip install chromadb langchain langchain-community ollama scikit-learn\n",
    "\n",
    "print(\"✅ 패키지 설치 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 2.3.2 정상 로드됨\n",
      "SentenceTransformer 정상 로드됨\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"numpy {np.__version__} 정상 로드됨\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"SentenceTransformer 정상 로드됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설치 확인:\n",
      "✓ NumPy: 2.3.2\n",
      "✓ PyTorch: 2.8.0\n",
      "✓ torchvision: 0.23.0\n",
      "✓ transformers: 4.56.0\n",
      "✓ sentence-transformers 정상 로드됨\n",
      "\n",
      "임베딩 모델 테스트 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559b0c3033346bea9cb06244d485dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2d3dd9426e4db98e7581c65ff6297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b22a6a754c349619bbd8516e3a91bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be3032348ea4410910c9fbe38de6459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b7f9b09a4d4abc8307bab38e3ec140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1a0c9691b944a5839f091b84d47994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801176743ff5487397b1ac030cccceb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f77edb26ad0493db8746ae31578e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85e124cfa20431e903923f2ee5ca819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbda99d66a64021afd188f1a80dabd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9ca9d310648019f1e670c85a802c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 임베딩 생성 성공: shape=(1, 384)\n",
      "\n",
      "✅ 모든 패키지가 정상적으로 설치되었습니다!\n",
      "이제 RAG 시스템을 구축할 준비가 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 패키지 버전 확인 및 호환성 검사\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def verify_installation():\n",
    "    print(\"설치 확인:\")\n",
    "    \n",
    "    try:\n",
    "        # NumPy 확인\n",
    "        import numpy\n",
    "        print(f\"✓ NumPy: {numpy.__version__}\")\n",
    "        \n",
    "        # PyTorch 확인\n",
    "        import torch\n",
    "        print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "        \n",
    "        # Torchvision 확인\n",
    "        import torchvision\n",
    "        print(f\"✓ torchvision: {torchvision.__version__}\")\n",
    "        \n",
    "        # Transformers 확인\n",
    "        import transformers\n",
    "        print(f\"✓ transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # Sentence Transformers 테스트\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        print(\"✓ sentence-transformers 정상 로드됨\")\n",
    "        \n",
    "        # 간단한 임베딩 테스트\n",
    "        print(\"\\n임베딩 모델 테스트 중...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        test_text = [\"테스트 문장\"]\n",
    "        embedding = model.encode(test_text)\n",
    "        print(f\"✓ 임베딩 생성 성공: shape={embedding.shape}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ 에러 발생: {e}\")\n",
    "        print(\"\\n해결 방법:\")\n",
    "        print(\"1. 커널을 재시작하세요 (Kernel > Restart)\")\n",
    "        print(\"2. 위의 설치 셀을 다시 실행하세요\")\n",
    "        print(\"3. 문제가 지속되면 다음 명령어를 터미널에서 실행:\")\n",
    "        print(\"   pip uninstall numpy torch torchvision transformers sentence-transformers -y\")\n",
    "        print(\"   pip install numpy==1.24.3\")\n",
    "        print(\"   pip install torch==2.1.0 torchvision==0.16.0\")\n",
    "        print(\"   pip install transformers==4.37.0 sentence-transformers==2.2.2\")\n",
    "        return False\n",
    "\n",
    "# 설치 확인 실행\n",
    "if verify_installation():\n",
    "    print(\"\\n✅ 모든 패키지가 정상적으로 설치되었습니다!\")\n",
    "    print(\"이제 RAG 시스템을 구축할 준비가 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설치 확인:\n",
      "✓ 모든 패키지 정상 로드됨\n",
      "PyTorch: 2.8.0\n",
      "torchvision: 0.23.0\n",
      "transformers: 4.56.0\n",
      "✓ SentenceTransformer 정상 작동\n"
     ]
    }
   ],
   "source": [
    "# 패키지 버전 확인 및 호환성 문제 해결\n",
    "\n",
    "def verify_installation():\n",
    "    print(\"설치 확인:\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        print(\"✓ 모든 패키지 정상 로드됨\")\n",
    "        print(f\"PyTorch: {torch.__version__}\")\n",
    "        print(f\"torchvision: {torchvision.__version__}\")\n",
    "        print(f\"transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # 간단한 테스트\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ SentenceTransformer 정상 작동\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 에러 발생: {e}\")\n",
    "\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 설치\n",
    "!pip install sentence-transformers chromadb langchain langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 패키지 로드 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"✅ 패키지 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 기초\n",
    "\n",
    "### 임베딩이란?\n",
    "텍스트를 의미를 담은 벡터(숫자 배열)로 변환하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 차원: (4, 384)\n",
      "첫 번째 텍스트의 임베딩 (처음 10개 값): [-0.06128301  0.02552284 -0.02994079 -0.02846024 -0.04995072 -0.13498838\n",
      "  0.03824809  0.03610769 -0.06114284 -0.05800613]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 텍스트 임베딩 생성\n",
    "texts = [\n",
    "    \"Python은 프로그래밍 언어입니다\",\n",
    "    \"파이썬은 코딩 언어입니다\",\n",
    "    \"고양이는 동물입니다\",\n",
    "    \"머신러닝은 AI의 한 분야입니다\"\n",
    "]\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "print(f\"임베딩 차원: {embeddings.shape}\")\n",
    "print(f\"첫 번째 텍스트의 임베딩 (처음 10개 값): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"텍스트 간 유사도 매트릭스:\")\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i < j:\n",
    "            print(f\"'{text1}' vs '{text2}': {similarities[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 벡터 데이터베이스 구축\n",
    "\n",
    "ChromaDB를 사용하여 벡터 검색 시스템을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"knowledge_base\"):\n",
    "        # 새로운 ChromaDB 클라이언트\n",
    "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        \n",
    "        # 임베딩 모델\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 컬렉션 생성/로드\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(collection_name)\n",
    "            print(f\"기존 컬렉션 '{collection_name}' 로드됨\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(collection_name)\n",
    "            print(f\"새 컬렉션 '{collection_name}' 생성됨\")\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"문서 추가\"\"\"\n",
    "        embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # ChromaDB에 추가\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        print(f\"{len(documents)}개 문서 추가됨\")\n",
    "    \n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"유사도 검색\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        return results\n",
    "\n",
    "# 사용 예시\n",
    "vector_store = VectorStore(\"rag_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식 베이스 구축\n",
    "knowledge_base = [\n",
    "    \"Python은 1991년 귀도 반 로섬이 개발한 고급 프로그래밍 언어입니다.\",\n",
    "    \"Python은 간결하고 읽기 쉬운 문법으로 유명하며, 들여쓰기로 코드 블록을 구분합니다.\",\n",
    "    \"머신러닝은 데이터에서 패턴을 학습하는 인공지능의 한 분야입니다.\",\n",
    "    \"딥러닝은 인공 신경망을 사용하는 머신러닝의 하위 분야입니다.\",\n",
    "    \"RAG는 Retrieval-Augmented Generation의 약자로, 검색 기반 생성 기법입니다.\",\n",
    "    \"LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.\",\n",
    "    \"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 시스템입니다.\",\n",
    "    \"임베딩은 텍스트를 의미를 담은 벡터로 변환하는 과정입니다.\"\n",
    "]\n",
    "\n",
    "# 메타데이터 추가\n",
    "# metadatas = [\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"ML\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"DL\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"RAG\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"LangChain\", \"category\": \"tool\"},\n",
    "#     {\"topic\": \"VectorDB\", \"category\": \"database\"},\n",
    "#     {\"topic\": \"Embedding\", \"category\": \"AI\"}\n",
    "# ]\n",
    "\n",
    "# 벡터 DB에 추가\n",
    "# vector_store.add_documents(knowledge_base, metadatas)\n",
    "vector_store.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트\n",
    "queries = [\n",
    "    \"파이썬의 특징은?\",\n",
    "    \"인공지능과 머신러닝의 관계\",\n",
    "    \"RAG 시스템이란?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    results = vector_store.search(query, n_results=2)\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        distance = results['distances'][0][i] if results['distances'] else 0\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "        print(f\"  [{i+1}] (거리: {distance:.3f})\")\n",
    "        print(f\"      {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 완전한 RAG 시스템 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"완전한 RAG 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        # LLM 초기화\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        \n",
    "        # 벡터 스토어\n",
    "        self.vector_store = VectorStore(\"rag_system\")\n",
    "        \n",
    "        # 텍스트 분할기\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def add_document(self, text: str, source: str = \"unknown\"):\n",
    "        \"\"\"문서 추가 (자동 청킹)\"\"\"\n",
    "        # 텍스트를 청크로 분할\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # 벡터 DB에 추가\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, use_thinking: bool = False):\n",
    "        \"\"\"RAG 기반 질의응답\"\"\"\n",
    "        \n",
    "        # 1. 관련 문서 검색\n",
    "        search_results = self.vector_store.search(question, n_results=top_k)\n",
    "        \n",
    "        # 2. 컨텍스트 생성\n",
    "        context_docs = search_results['documents'][0] if search_results['documents'] else []\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "        \n",
    "        # 3. 프롬프트 구성\n",
    "        prompt = f\"\"\"\n",
    "다음 컨텍스트를 참고하여 질문에 답변해주세요.\n",
    "컨텍스트에 없는 내용은 추측하지 말고 \"정보가 없습니다\"라고 답하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "        \n",
    "        # Thinking Mode 적용\n",
    "        if use_thinking:\n",
    "            prompt = f\"/think {prompt}\"\n",
    "        \n",
    "        # 4. LLM으로 응답 생성\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"sources\": context_docs,\n",
    "            \"num_sources\": len(context_docs)\n",
    "        }\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = RAGSystem()\n",
    "print(\"✅ RAG 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 추가\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    회사 규정 문서\n",
    "    \n",
    "    1. 근무 시간: 오전 9시 - 오후 6시 (점심시간 12시-1시)\n",
    "    2. 재택근무: 주 2회 가능 (월/금 권장)\n",
    "    3. 휴가: 연차 15일, 병가 10일\n",
    "    4. 교육 지원: 연간 200만원 한도\n",
    "    5. 회의: 매주 월요일 10시 팀 미팅\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    프로젝트 가이드라인\n",
    "    \n",
    "    1. 코드 리뷰: 모든 PR은 2명 이상의 리뷰 필요\n",
    "    2. 테스트: 코드 커버리지 80% 이상 유지\n",
    "    3. 문서화: 모든 공개 API는 문서화 필수\n",
    "    4. 브랜치: feature/*, bugfix/*, hotfix/* 규칙 준수\n",
    "    5. 배포: 매주 화요일, 목요일 정기 배포\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    기술 스택\n",
    "    \n",
    "    - 백엔드: Python (FastAPI), PostgreSQL\n",
    "    - 프론트엔드: React, TypeScript, TailwindCSS\n",
    "    - 인프라: AWS, Docker, Kubernetes\n",
    "    - CI/CD: GitHub Actions, ArgoCD\n",
    "    - 모니터링: Prometheus, Grafana, Sentry\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    chunks = rag.add_document(doc, source=f\"document_{i+1}\")\n",
    "    print(f\"문서 {i+1}: {chunks}개 청크로 분할\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 시스템 테스트\n",
    "questions = [\n",
    "    \"재택근무는 언제 가능한가요?\",\n",
    "    \"코드 리뷰 규칙은 무엇인가요?\",\n",
    "    \"우리 회사는 어떤 프로그래밍 언어를 사용하나요?\",\n",
    "    \"점심시간은 언제인가요?\",\n",
    "    \"CEO는 누구인가요?\"  # 컨텍스트에 없는 질문\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"❓ 질문: {question}\")\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    \n",
    "    print(f\"\\n💡 답변: {result['answer']}\")\n",
    "    print(f\"\\n📚 참고한 소스 ({result['num_sources']}개):\")\n",
    "    for i, source in enumerate(result['sources'][:2]):\n",
    "        print(f\"  [{i+1}] {source[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 고급 RAG 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"고급 RAG 기법 구현\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\"):\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5):\n",
    "        \"\"\"하이브리드 검색 (의미 + 키워드)\"\"\"\n",
    "        \n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # 1. 의미 기반 검색\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # 2. 키워드 기반 검색 (BM25 간단 구현)\n",
    "        query_words = set(query.lower().split())\n",
    "        keyword_scores = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc.lower().split())\n",
    "            overlap = len(query_words & doc_words)\n",
    "            keyword_scores.append(overlap / max(len(query_words), 1))\n",
    "        \n",
    "        keyword_scores = np.array(keyword_scores)\n",
    "        \n",
    "        # 3. 점수 결합 (가중 평균)\n",
    "        combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
    "        \n",
    "        # 4. 상위 k개 선택\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": combined_scores[idx],\n",
    "                \"semantic_score\": semantic_scores[idx],\n",
    "                \"keyword_score\": keyword_scores[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def query_expansion(self, query: str) -> List[str]:\n",
    "        \"\"\"쿼리 확장 (관련 용어 추가)\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "다음 질문과 관련된 유사 용어나 동의어를 3개 제시해주세요.\n",
    "각 용어는 쉼표로 구분해주세요.\n",
    "\n",
    "질문: {query}\n",
    "유사 용어:\n",
    "\"\"\"\n",
    "        \n",
    "        expansion = self.llm.invoke(prompt)\n",
    "        expanded_terms = [term.strip() for term in expansion.split(',')]\n",
    "        \n",
    "        return [query] + expanded_terms[:3]\n",
    "    \n",
    "    def rerank_results(self, query: str, documents: List[str]) -> List[Dict]:\n",
    "        \"\"\"재순위 지정 (Cross-encoder 스타일)\"\"\"\n",
    "        \n",
    "        reranked = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # 쿼리와 문서의 관련성을 LLM으로 평가\n",
    "            prompt = f\"\"\"\n",
    "다음 문서가 질문에 얼마나 관련이 있는지 0-10 점수로 평가해주세요.\n",
    "숫자만 답하세요.\n",
    "\n",
    "질문: {query}\n",
    "문서: {doc[:200]}\n",
    "\n",
    "점수:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                score = float(self.llm.invoke(prompt).strip())\n",
    "            except:\n",
    "                score = 5.0\n",
    "            \n",
    "            reranked.append({\n",
    "                \"document\": doc,\n",
    "                \"relevance_score\": score\n",
    "            })\n",
    "        \n",
    "        # 점수 기준 정렬\n",
    "        reranked.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"문서 추가\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        new_embeddings = self.embedder.encode(documents)\n",
    "        \n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "\n",
    "# 고급 RAG 시스템 초기화\n",
    "adv_rag = AdvancedRAG()\n",
    "\n",
    "# 문서 추가\n",
    "tech_docs = [\n",
    "    \"Python은 동적 타이핑을 지원하는 인터프리터 언어입니다.\",\n",
    "    \"JavaScript는 웹 브라우저에서 실행되는 스크립트 언어입니다.\",\n",
    "    \"Docker는 컨테이너화 기술을 제공하는 플랫폼입니다.\",\n",
    "    \"Kubernetes는 컨테이너 오케스트레이션 도구입니다.\",\n",
    "    \"Git은 분산 버전 관리 시스템입니다.\"\n",
    "]\n",
    "\n",
    "adv_rag.add_documents(tech_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이브리드 검색 테스트\n",
    "query = \"컨테이너 관리 도구\"\n",
    "\n",
    "print(\"🔍 하이브리드 검색 결과:\")\n",
    "results = adv_rag.hybrid_search(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] 종합 점수: {result['score']:.3f}\")\n",
    "    print(f\"    의미 점수: {result['semantic_score']:.3f}\")\n",
    "    print(f\"    키워드 점수: {result['keyword_score']:.3f}\")\n",
    "    print(f\"    문서: {result['document']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 확장 테스트\n",
    "original_query = \"프로그래밍 언어\"\n",
    "\n",
    "print(f\"원본 쿼리: {original_query}\")\n",
    "expanded = adv_rag.query_expansion(original_query)\n",
    "print(f\"확장된 쿼리: {expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실전 프로젝트: PDF 기반 Q&A 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQA:\n",
    "    \"\"\"문서 기반 Q&A 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag = RAGSystem()\n",
    "        self.sources = {}\n",
    "    \n",
    "    def load_text_file(self, filepath: str, source_name: str = None):\n",
    "        \"\"\"텍스트 파일 로드\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            source = source_name or filepath\n",
    "            chunks = self.rag.add_document(content, source)\n",
    "            self.sources[source] = chunks\n",
    "            \n",
    "            print(f\"✅ '{source}' 로드 완료 ({chunks} 청크)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 로드 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"대화형 Q&A 세션\"\"\"\n",
    "        print(\"\\n💬 문서 Q&A 시스템\")\n",
    "        print(\"질문을 입력하세요 ('종료' 입력시 종료)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"질문: \")\n",
    "            \n",
    "            if question.lower() in ['종료', 'exit', 'quit']:\n",
    "                print(\"👋 Q&A 세션 종료\")\n",
    "                break\n",
    "            \n",
    "            # 답변 생성\n",
    "            result = self.rag.query(question)\n",
    "            \n",
    "            print(f\"\\n답변: {result['answer']}\\n\")\n",
    "            print(f\"(참고: {result['num_sources']}개 소스 활용)\\n\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def batch_qa(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"배치 Q&A 처리\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"처리중 [{i}/{len(questions)}]: {question[:50]}...\")\n",
    "            result = self.rag.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 시스템 초기화\n",
    "doc_qa = DocumentQA()\n",
    "\n",
    "# 샘플 문서 생성\n",
    "sample_doc = \"\"\"\n",
    "RAG 시스템 사용 가이드\n",
    "\n",
    "1. 소개\n",
    "RAG(Retrieval-Augmented Generation)는 검색과 생성을 결합한 AI 시스템입니다.\n",
    "기존 LLM의 한계를 극복하고 최신 정보를 제공할 수 있습니다.\n",
    "\n",
    "2. 주요 구성 요소\n",
    "- 문서 처리: 텍스트를 작은 청크로 분할\n",
    "- 임베딩: 텍스트를 벡터로 변환\n",
    "- 벡터 DB: 임베딩을 저장하고 검색\n",
    "- LLM: 컨텍스트 기반 응답 생성\n",
    "\n",
    "3. 장점\n",
    "- 환각 현상 감소\n",
    "- 출처 제공 가능\n",
    "- 도메인 특화 가능\n",
    "- 실시간 정보 반영\n",
    "\n",
    "4. 활용 분야\n",
    "- 고객 서비스 챗봇\n",
    "- 기술 문서 Q&A\n",
    "- 법률 자문 시스템\n",
    "- 의료 정보 검색\n",
    "\"\"\"\n",
    "\n",
    "# 임시 파일로 저장\n",
    "with open('rag_guide.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "# 문서 로드\n",
    "doc_qa.load_text_file('rag_guide.txt', 'RAG 가이드')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 Q&A 테스트\n",
    "test_questions = [\n",
    "    \"RAG의 주요 구성 요소는?\",\n",
    "    \"RAG 시스템의 장점을 설명해주세요\",\n",
    "    \"RAG는 어떤 분야에서 활용되나요?\"\n",
    "]\n",
    "\n",
    "results = doc_qa.batch_qa(test_questions)\n",
    "\n",
    "print(\"\\n📊 배치 Q&A 결과:\")\n",
    "for i, (q, r) in enumerate(zip(test_questions, results), 1):\n",
    "    print(f\"\\n[{i}] Q: {q}\")\n",
    "    print(f\"    A: {r['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG 시스템 평가 및 개선\n",
    "\n",
    "### 평가 메트릭\n",
    "1. **검색 품질**: Precision, Recall, MRR\n",
    "2. **생성 품질**: BLEU, ROUGE, 의미적 유사도\n",
    "3. **전체 품질**: 정확도, 관련성, 완전성\n",
    "\n",
    "### 개선 방법\n",
    "1. **청킹 최적화**: 문서 특성에 맞는 청크 크기\n",
    "2. **임베딩 모델 선택**: 도메인 특화 모델 사용\n",
    "3. **하이브리드 검색**: 의미 + 키워드 검색 결합\n",
    "4. **재순위**: Cross-encoder로 정확도 향상\n",
    "5. **프롬프트 엔지니어링**: 더 나은 컨텍스트 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실습 과제\n",
    "\n",
    "1. **기본 과제**:\n",
    "   - 자신의 문서로 RAG 시스템 구축\n",
    "   - 다양한 청크 크기 실험\n",
    "   - 검색 결과 평가\n",
    "\n",
    "2. **심화 과제**:\n",
    "   - 멀티모달 RAG (이미지 + 텍스트)\n",
    "   - 다국어 RAG 시스템\n",
    "   - 실시간 업데이트 RAG\n",
    "\n",
    "3. **프로젝트**:\n",
    "   - 기술 문서 Q&A 봇\n",
    "   - 논문 검색 및 요약 시스템\n",
    "   - 코드베이스 지식 어시스턴트\n",
    "\n",
    "## 📚 추가 학습 자료\n",
    "\n",
    "- [RAG 논문](https://arxiv.org/abs/2005.11401)\n",
    "- [ChromaDB 문서](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [LlamaIndex RAG 가이드](https://gpt-index.readthedocs.io/)\n",
    "\n",
    "## 다음 단계\n",
    "\n",
    "다음 노트북에서는 **LoRA 파인튜닝**을 통해 모델을 커스터마이징하는 방법을 배워보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
