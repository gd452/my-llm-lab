{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "1. RAGì˜ ê°œë…ê³¼ í•„ìš”ì„± ì´í•´\n",
    "2. ì„ë² ë”©ê³¼ ë²¡í„° ê²€ìƒ‰ êµ¬í˜„\n",
    "3. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "4. ì‹¤ì „ RAG ì‹œìŠ¤í…œ ê°œë°œ\n",
    "\n",
    "## ğŸ“š RAGë€ ë¬´ì—‡ì¸ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜\n",
    "\n",
    "### RAGì˜ í•µì‹¬ ê°œë…\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              RAG Pipeline                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   1. ë¬¸ì„œ ìˆ˜ì§‘     â†’    2. ì²­í‚¹              â”‚\n",
    "â”‚   (Documents)          (Chunking)           â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   3. ì„ë² ë”© ìƒì„±   â†’    4. ë²¡í„° DB ì €ì¥      â”‚\n",
    "â”‚   (Embedding)          (Vector Store)       â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   5. ì¿¼ë¦¬ ì„ë² ë”©   â†’    6. ìœ ì‚¬ë„ ê²€ìƒ‰       â”‚\n",
    "â”‚   (Query Embed)        (Similarity Search)  â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚   7. ì»¨í…ìŠ¤íŠ¸ ìƒì„±  â†’   8. LLM ì‘ë‹µ ìƒì„±     â”‚\n",
    "â”‚   (Context)            (Generation)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ì™œ RAGê°€ í•„ìš”í•œê°€?\n",
    "\n",
    "1. **ìµœì‹  ì •ë³´**: LLMì˜ í•™ìŠµ ë°ì´í„° ì‹œì  ì´í›„ ì •ë³´ ì œê³µ\n",
    "2. **ì •í™•ì„±**: í™˜ê°(Hallucination) ê°ì†Œ\n",
    "3. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ í™œìš©\n",
    "4. **ì¶œì²˜ ì œê³µ**: ë‹µë³€ì˜ ê·¼ê±° ëª…ì‹œ ê°€ëŠ¥\n",
    "5. **ë¹„ìš© íš¨ìœ¨**: íŒŒì¸íŠœë‹ ì—†ì´ ì§€ì‹ í™•ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# condaë¡œ í˜¸í™˜ë˜ëŠ” ë²„ì „ ì„¤ì¹˜\n",
    "- conda install numpy=1.24\n",
    "- conda install pytorch torchvision torchaudio -c pytorch\n",
    "- conda install -c conda-forge transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì¹˜ í™•ì¸:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œë¨\n",
      "PyTorch: 2.2.2\n",
      "torchvision: 0.17.2\n",
      "transformers: 4.53.3\n",
      "âœ“ SentenceTransformer ì •ìƒ ì‘ë™\n"
     ]
    }
   ],
   "source": [
    "def verify_installation():\n",
    "    print(\"ì„¤ì¹˜ í™•ì¸:\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        print(\"âœ“ ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œë¨\")\n",
    "        print(f\"PyTorch: {torch.__version__}\")\n",
    "        print(f\"torchvision: {torchvision.__version__}\")\n",
    "        print(f\"transformers: {transformers.__version__}\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"âœ“ SentenceTransformer ì •ìƒ ì‘ë™\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "\n",
    "verify_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install sentence-transformers chromadb langchain langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„ë² ë”© ê¸°ì´ˆ\n",
    "\n",
    "### ì„ë² ë”©ì´ë€?\n",
    "í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë‹´ì€ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ì°¨ì›: (4, 384)\n",
      "ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): [-0.06128298  0.02552282 -0.0299408  -0.02846022 -0.04995069 -0.13498837\n",
      "  0.03824809  0.03610768 -0.06114289 -0.05800608]\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "texts = [\n",
    "    \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤\",\n",
    "    \"íŒŒì´ì¬ì€ ì½”ë”© ì–¸ì–´ì…ë‹ˆë‹¤\",\n",
    "    \"ê³ ì–‘ì´ëŠ” ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {embeddings.shape}\")\n",
    "print(f\"ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© (ì²˜ìŒ 10ê°œ ê°’): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\n",
      "'Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤' vs 'íŒŒì´ì¬ì€ ì½”ë”© ì–¸ì–´ì…ë‹ˆë‹¤': 0.450\n",
      "'Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤' vs 'ê³ ì–‘ì´ëŠ” ë™ë¬¼ì…ë‹ˆë‹¤': 0.447\n",
      "'Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤' vs 'ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤': 0.416\n",
      "'íŒŒì´ì¬ì€ ì½”ë”© ì–¸ì–´ì…ë‹ˆë‹¤' vs 'ê³ ì–‘ì´ëŠ” ë™ë¬¼ì…ë‹ˆë‹¤': 0.691\n",
      "'íŒŒì´ì¬ì€ ì½”ë”© ì–¸ì–´ì…ë‹ˆë‹¤' vs 'ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤': 0.561\n",
      "'ê³ ì–‘ì´ëŠ” ë™ë¬¼ì…ë‹ˆë‹¤' vs 'ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤': 0.565\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i < j:\n",
    "            print(f\"'{text1}' vs '{text2}': {similarities[i][j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ì»¬ë ‰ì…˜ 'rag_demo' ë¡œë“œë¨\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"knowledge_base\"):\n",
    "        # ìƒˆë¡œìš´ ChromaDB í´ë¼ì´ì–¸íŠ¸\n",
    "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        \n",
    "        # ì„ë² ë”© ëª¨ë¸\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(collection_name)\n",
    "            print(f\"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œë¨\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(collection_name)\n",
    "            print(f\"ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„±ë¨\")\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€\"\"\"\n",
    "        embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        # ChromaDBì— ì¶”ê°€\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=documents,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        print(f\"{len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\")\n",
    "    \n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"ìœ ì‚¬ë„ ê²€ìƒ‰\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=n_results\n",
    "        )\n",
    "        return results\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "vector_store = VectorStore(\"rag_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\n"
     ]
    }
   ],
   "source": [
    "# ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•\n",
    "knowledge_base = [\n",
    "    \"Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ìœ¼ë¡œ ìœ ëª…í•˜ë©°, ë“¤ì—¬ì“°ê¸°ë¡œ ì½”ë“œ ë¸”ë¡ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ê¸°ë²•ì…ë‹ˆë‹¤.\",\n",
    "    \"LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\",\n",
    "    \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\",\n",
    "    \"ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë‹´ì€ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "# metadatas = [\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"Python\", \"category\": \"programming\"},\n",
    "#     {\"topic\": \"ML\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"DL\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"RAG\", \"category\": \"AI\"},\n",
    "#     {\"topic\": \"LangChain\", \"category\": \"tool\"},\n",
    "#     {\"topic\": \"VectorDB\", \"category\": \"database\"},\n",
    "#     {\"topic\": \"Embedding\", \"category\": \"AI\"}\n",
    "# ]\n",
    "\n",
    "# ë²¡í„° DBì— ì¶”ê°€\n",
    "# vector_store.add_documents(knowledge_base, metadatas)\n",
    "vector_store.add_documents(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Query: íŒŒì´ì¬ì˜ íŠ¹ì§•ì€?\n",
      "  [1] (ê±°ë¦¬: 0.956)\n",
      "      ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤....\n",
      "  [2] (ê±°ë¦¬: 1.008)\n",
      "      ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤....\n",
      "\n",
      "ğŸ” Query: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„\n",
      "  [1] (ê±°ë¦¬: 0.343)\n",
      "      ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤....\n",
      "  [2] (ê±°ë¦¬: 0.409)\n",
      "      ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤....\n",
      "\n",
      "ğŸ” Query: RAG ì‹œìŠ¤í…œì´ë€?\n",
      "  [1] (ê±°ë¦¬: 1.132)\n",
      "      ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤....\n",
      "  [2] (ê±°ë¦¬: 1.155)\n",
      "      ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤....\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "queries = [\n",
    "    \"íŒŒì´ì¬ì˜ íŠ¹ì§•ì€?\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„\",\n",
    "    \"RAG ì‹œìŠ¤í…œì´ë€?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nğŸ” Query: {query}\")\n",
    "    results = vector_store.search(query, n_results=2)\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        distance = results['distances'][0][i] if results['distances'] else 0\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
    "        print(f\"  [{i+1}] (ê±°ë¦¬: {distance:.3f})\")\n",
    "        print(f\"      {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ì»¬ë ‰ì…˜ 'rag_system' ë¡œë“œë¨\n",
      "âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"ì™„ì „í•œ RAG ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        # LLM ì´ˆê¸°í™”\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        \n",
    "        # ë²¡í„° ìŠ¤í† ì–´\n",
    "        self.vector_store = VectorStore(\"rag_system\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def add_document(self, text: str, source: str = \"unknown\"):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€ (ìë™ ì²­í‚¹)\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # ë²¡í„° DBì— ì¶”ê°€\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, use_thinking: bool = False):\n",
    "        \"\"\"RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\"\"\"\n",
    "        \n",
    "        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        search_results = self.vector_store.search(question, n_results=top_k)\n",
    "        \n",
    "        # 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        context_docs = search_results['documents'][0] if search_results['documents'] else []\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "        \n",
    "        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        prompt = f\"\"\"\n",
    "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  \"ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    "        \n",
    "        # Thinking Mode ì ìš©\n",
    "        if use_thinking:\n",
    "            prompt = f\"/think {prompt}\"\n",
    "        \n",
    "        # 4. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"sources\": context_docs,\n",
    "            \"num_sources\": len(context_docs)\n",
    "        }\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "rag = RAGSystem()\n",
    "print(\"âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\n",
      "ë¬¸ì„œ 1: 1ê°œ ì²­í¬ë¡œ ë¶„í• \n",
      "1ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\n",
      "ë¬¸ì„œ 2: 1ê°œ ì²­í¬ë¡œ ë¶„í• \n",
      "1ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\n",
      "ë¬¸ì„œ 3: 1ê°œ ì²­í¬ë¡œ ë¶„í• \n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ì¶”ê°€\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
    "    \n",
    "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
    "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
    "    3. íœ´ê°€: ì—°ì°¨ 15ì¼, ë³‘ê°€ 10ì¼\n",
    "    4. êµìœ¡ ì§€ì›: ì—°ê°„ 200ë§Œì› í•œë„\n",
    "    5. íšŒì˜: ë§¤ì£¼ ì›”ìš”ì¼ 10ì‹œ íŒ€ ë¯¸íŒ…\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    í”„ë¡œì íŠ¸ ê°€ì´ë“œë¼ì¸\n",
    "    \n",
    "    1. ì½”ë“œ ë¦¬ë·°: ëª¨ë“  PRì€ 2ëª… ì´ìƒì˜ ë¦¬ë·° í•„ìš”\n",
    "    2. í…ŒìŠ¤íŠ¸: ì½”ë“œ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ìœ ì§€\n",
    "    3. ë¬¸ì„œí™”: ëª¨ë“  ê³µê°œ APIëŠ” ë¬¸ì„œí™” í•„ìˆ˜\n",
    "    4. ë¸Œëœì¹˜: feature/*, bugfix/*, hotfix/* ê·œì¹™ ì¤€ìˆ˜\n",
    "    5. ë°°í¬: ë§¤ì£¼ í™”ìš”ì¼, ëª©ìš”ì¼ ì •ê¸° ë°°í¬\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    ê¸°ìˆ  ìŠ¤íƒ\n",
    "    \n",
    "    - ë°±ì—”ë“œ: Python (FastAPI), PostgreSQL\n",
    "    - í”„ë¡ íŠ¸ì—”ë“œ: React, TypeScript, TailwindCSS\n",
    "    - ì¸í”„ë¼: AWS, Docker, Kubernetes\n",
    "    - CI/CD: GitHub Actions, ArgoCD\n",
    "    - ëª¨ë‹ˆí„°ë§: Prometheus, Grafana, Sentry\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    chunks = rag.add_document(doc, source=f\"document_{i+1}\")\n",
    "    print(f\"ë¬¸ì„œ {i+1}: {chunks}ê°œ ì²­í¬ë¡œ ë¶„í• \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "â“ ì§ˆë¬¸: ì¬íƒê·¼ë¬´ëŠ” ì–¸ì œ ê°€ëŠ¥í•œê°€ìš”?\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: <think>\n",
      "Okay, let's see. The user is asking about when remote work is possible based on the provided context. The context mentions that ì¬íƒê·¼ë¬´ (remote work) is allowed twice a week, specifically on Monday and Friday, and it's recommended. So the answer should state that it's possible on those days. I need to make sure not to add any information that's not in the context. The user might be looking for the exact days, so I should mention Monday and Friday. Also, since the context says \"ê¶Œì¥\" which means recommended, maybe include that it's recommended to work from home on those days. But the question is just asking when it's possible, so the main points are the days. Let me check again. The context says \"ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\" which translates to \"possible twice a week (Monday/Friday recommended)\". So the answer should be that remote work is possible on Monday and Friday, and those are the recommended days. I should present that clearly without any extra guesses.\n",
      "</think>\n",
      "\n",
      "ì¬íƒê·¼ë¬´ëŠ” ì£¼ 2íšŒ ê°€ëŠ¥í•˜ë©°, ì›”ìš”ì¼ê³¼ ê¸ˆìš”ì¼ì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ (1ê°œ):\n",
      "  [1] íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
      "\n",
      "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
      "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
      "    3. íœ´ê°€: ì—°ì°¨ 15ì¼,...\n",
      "\n",
      "============================================================\n",
      "â“ ì§ˆë¬¸: ì½”ë“œ ë¦¬ë·° ê·œì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: <think>\n",
      "Okay, let's see. The user is asking about the code review rules based on the provided context. I need to check the context first.\n",
      "\n",
      "Looking at the context, it's a company regulations document with sections on working hours, remote work, vacation, education support, and meetings. The sections listed don't mention anything about code review rules. The topics covered are all about general workplace policies, not specific development practices or code review procedures. \n",
      "\n",
      "Since the question is about code review rules and the context doesn't have any information on that, I should respond that the information isn't available. I mustn't make up any rules or assume anything beyond what's provided. The answer needs to be straightforward, just stating that the information isn't present in the given context.\n",
      "</think>\n",
      "\n",
      "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ (1ê°œ):\n",
      "  [1] íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
      "\n",
      "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
      "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
      "    3. íœ´ê°€: ì—°ì°¨ 15ì¼,...\n",
      "\n",
      "============================================================\n",
      "â“ ì§ˆë¬¸: ìš°ë¦¬ íšŒì‚¬ëŠ” ì–´ë–¤ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: <think>\n",
      "Okay, let's see. The user is asking which programming languages the company uses. I need to check the provided context to find the answer.\n",
      "\n",
      "Looking at the context, it's a company regulations document. The sections mentioned are working hours, remote work, vacation days, education support, and meetings. There's nothing about programming languages here. The context doesn't mention any specific technologies or languages used by the company. \n",
      "\n",
      "Since the question is about programming languages and the context doesn't have that information, I should respond that the information isn't available. The user might be assuming that the document includes tech stack details, but based on the given content, there's no such data. So the correct answer is to state that the information isn't provided.\n",
      "</think>\n",
      "\n",
      "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ (1ê°œ):\n",
      "  [1] íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
      "\n",
      "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
      "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
      "    3. íœ´ê°€: ì—°ì°¨ 15ì¼,...\n",
      "\n",
      "============================================================\n",
      "â“ ì§ˆë¬¸: ì ì‹¬ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: <think>\n",
      "Okay, let's see. The user is asking about the lunch break time based on the provided context. The context is a company regulations document. Let me check each section.\n",
      "\n",
      "Looking at point 1: ê·¼ë¬´ ì‹œê°„ is 9 AM to 6 PM, with ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ. So that's 12 PM to 1 PM. The other points are about remote work, vacation days, education support, and meetings. The question is straightforward, just asking for the lunch time. Since the context explicitly states the lunch break is from 12 to 1, I should just answer that. No need to guess anything else. The answer is 12ì‹œ-1ì‹œ.\n",
      "</think>\n",
      "\n",
      "ì ì‹¬ì‹œê°„ì€ ì˜¤í›„ 12ì‹œë¶€í„° ì˜¤í›„ 1ì‹œê¹Œì§€ì…ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ (1ê°œ):\n",
      "  [1] íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
      "\n",
      "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
      "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
      "    3. íœ´ê°€: ì—°ì°¨ 15ì¼,...\n",
      "\n",
      "============================================================\n",
      "â“ ì§ˆë¬¸: CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: <think>\n",
      "Okay, let's see. The user is asking about the CEO of the company. I need to check the provided context to find any mention of the CEO. The context given is a company regulations document with sections on working hours, remote work, vacation, education support, and meetings. \n",
      "\n",
      "Looking through each point:\n",
      "1. Working hours: 9 AM to 6 PM, lunch break 12-1 PM.\n",
      "2. Remote work: 2 days a week, Monday and Friday recommended.\n",
      "3. Vacation: 15 annual days, 10 sick days.\n",
      "4. Education support: up to 2 million won per year.\n",
      "5. Meetings: weekly Monday 10 AM team meeting.\n",
      "\n",
      "None of these sections mention the CEO's name or any information about the CEO. The document is about policies and procedures, not personnel details. Since the context doesn't have any information about the CEO, the correct answer should be that there's no information available.\n",
      "</think>\n",
      "\n",
      "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ (1ê°œ):\n",
      "  [1] íšŒì‚¬ ê·œì • ë¬¸ì„œ\n",
      "\n",
      "    1. ê·¼ë¬´ ì‹œê°„: ì˜¤ì „ 9ì‹œ - ì˜¤í›„ 6ì‹œ (ì ì‹¬ì‹œê°„ 12ì‹œ-1ì‹œ)\n",
      "    2. ì¬íƒê·¼ë¬´: ì£¼ 2íšŒ ê°€ëŠ¥ (ì›”/ê¸ˆ ê¶Œì¥)\n",
      "    3. íœ´ê°€: ì—°ì°¨ 15ì¼,...\n"
     ]
    }
   ],
   "source": [
    "# RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "questions = [\n",
    "    \"ì¬íƒê·¼ë¬´ëŠ” ì–¸ì œ ê°€ëŠ¥í•œê°€ìš”?\",\n",
    "    \"ì½”ë“œ ë¦¬ë·° ê·œì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ìš°ë¦¬ íšŒì‚¬ëŠ” ì–´ë–¤ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?\",\n",
    "    \"ì ì‹¬ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?\",\n",
    "    \"CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\"  # ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì§ˆë¬¸\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"â“ ì§ˆë¬¸: {question}\")\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ë‹µë³€: {result['answer']}\")\n",
    "    print(f\"\\nğŸ“š ì°¸ê³ í•œ ì†ŒìŠ¤ ({result['num_sources']}ê°œ):\")\n",
    "    for i, source in enumerate(result['sources'][:2]):\n",
    "        print(f\"  [{i+1}] {source[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê³ ê¸‰ RAG ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"ê³ ê¸‰ RAG ê¸°ë²• êµ¬í˜„\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model=\"qwen3:8b\"):\n",
    "        self.llm = Ollama(model=llm_model)\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ + í‚¤ì›Œë“œ)\"\"\"\n",
    "        \n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # 1. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        semantic_scores = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25 ê°„ë‹¨ êµ¬í˜„)\n",
    "        query_words = set(query.lower().split())\n",
    "        keyword_scores = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            doc_words = set(doc.lower().split())\n",
    "            overlap = len(query_words & doc_words)\n",
    "            keyword_scores.append(overlap / max(len(query_words), 1))\n",
    "        \n",
    "        keyword_scores = np.array(keyword_scores)\n",
    "        \n",
    "        # 3. ì ìˆ˜ ê²°í•© (ê°€ì¤‘ í‰ê· )\n",
    "        combined_scores = 0.7 * semantic_scores + 0.3 * keyword_scores\n",
    "        \n",
    "        # 4. ìƒìœ„ kê°œ ì„ íƒ\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": combined_scores[idx],\n",
    "                \"semantic_score\": semantic_scores[idx],\n",
    "                \"keyword_score\": keyword_scores[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    def query_expansion(self, query: str) -> List[str]:\n",
    "        \"\"\"ì¿¼ë¦¬ í™•ì¥ (ê´€ë ¨ ìš©ì–´ ì¶”ê°€)\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ ìš©ì–´ë‚˜ ë™ì˜ì–´ë¥¼ 3ê°œ ì œì‹œí•´ì£¼ì„¸ìš”.\n",
    "ê° ìš©ì–´ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "ìœ ì‚¬ ìš©ì–´:\n",
    "\"\"\"\n",
    "        \n",
    "        expansion = self.llm.invoke(prompt)\n",
    "        expanded_terms = [term.strip() for term in expansion.split(',')]\n",
    "        \n",
    "        return [query] + expanded_terms[:3]\n",
    "    \n",
    "    def rerank_results(self, query: str, documents: List[str]) -> List[Dict]:\n",
    "        \"\"\"ì¬ìˆœìœ„ ì§€ì • (Cross-encoder ìŠ¤íƒ€ì¼)\"\"\"\n",
    "        \n",
    "        reranked = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ LLMìœ¼ë¡œ í‰ê°€\n",
    "            prompt = f\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 0-10 ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.\n",
    "ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "ë¬¸ì„œ: {doc[:200]}\n",
    "\n",
    "ì ìˆ˜:\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                score = float(self.llm.invoke(prompt).strip())\n",
    "            except:\n",
    "                score = 5.0\n",
    "            \n",
    "            reranked.append({\n",
    "                \"document\": doc,\n",
    "                \"relevance_score\": score\n",
    "            })\n",
    "        \n",
    "        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬\n",
    "        reranked.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        \"\"\"ë¬¸ì„œ ì¶”ê°€\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        new_embeddings = self.embedder.encode(documents)\n",
    "        \n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "\n",
    "# ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "adv_rag = AdvancedRAG()\n",
    "\n",
    "# ë¬¸ì„œ ì¶”ê°€\n",
    "tech_docs = [\n",
    "    \"Pythonì€ ë™ì  íƒ€ì´í•‘ì„ ì§€ì›í•˜ëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"JavaScriptëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
    "    \"DockerëŠ” ì»¨í…Œì´ë„ˆí™” ê¸°ìˆ ì„ ì œê³µí•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.\",\n",
    "    \"KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤.\",\n",
    "    \"Gitì€ ë¶„ì‚° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "adv_rag.add_documents(tech_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼:\n",
      "\n",
      "[1] ì¢…í•© ì ìˆ˜: 0.431\n",
      "    ì˜ë¯¸ ì ìˆ˜: 0.473\n",
      "    í‚¤ì›Œë“œ ì ìˆ˜: 0.333\n",
      "    ë¬¸ì„œ: Gitì€ ë¶„ì‚° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "\n",
      "[2] ì¢…í•© ì ìˆ˜: 0.361\n",
      "    ì˜ë¯¸ ì ìˆ˜: 0.516\n",
      "    í‚¤ì›Œë“œ ì ìˆ˜: 0.000\n",
      "    ë¬¸ì„œ: JavaScriptëŠ” ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì–¸ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "[3] ì¢…í•© ì ìˆ˜: 0.355\n",
      "    ì˜ë¯¸ ì ìˆ˜: 0.364\n",
      "    í‚¤ì›Œë“œ ì ìˆ˜: 0.333\n",
      "    ë¬¸ì„œ: KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "query = \"ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ë„êµ¬\"\n",
    "\n",
    "print(\"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "results = adv_rag.hybrid_search(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] ì¢…í•© ì ìˆ˜: {result['score']:.3f}\")\n",
    "    print(f\"    ì˜ë¯¸ ì ìˆ˜: {result['semantic_score']:.3f}\")\n",
    "    print(f\"    í‚¤ì›Œë“œ ì ìˆ˜: {result['keyword_score']:.3f}\")\n",
    "    print(f\"    ë¬¸ì„œ: {result['document']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ì¿¼ë¦¬: í”„ë¡œê·¸ë˜ë° ì–¸ì–´\n",
      "í™•ì¥ëœ ì¿¼ë¦¬: ['í”„ë¡œê·¸ë˜ë° ì–¸ì–´', '<think>\\nOkay', 'the user is asking for three similar terms or synonyms related to \"programming language.\" Let me start by recalling what a programming language is. It\\'s a formal language used to write instructions that a computer can execute. So', 'synonyms would be terms that refer to the same concept but with different names.\\n\\nFirst']\n"
     ]
    }
   ],
   "source": [
    "# ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸\n",
    "original_query = \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\"\n",
    "\n",
    "print(f\"ì›ë³¸ ì¿¼ë¦¬: {original_query}\")\n",
    "expanded = adv_rag.query_expansion(original_query)\n",
    "print(f\"í™•ì¥ëœ ì¿¼ë¦¬: {expanded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ì „ í”„ë¡œì íŠ¸: PDF ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ì»¬ë ‰ì…˜ 'rag_system' ë¡œë“œë¨\n",
      "1ê°œ ë¬¸ì„œ ì¶”ê°€ë¨\n",
      "âœ… 'RAG ê°€ì´ë“œ' ë¡œë“œ ì™„ë£Œ (1 ì²­í¬)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DocumentQA:\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag = RAGSystem()\n",
    "        self.sources = {}\n",
    "    \n",
    "    def load_text_file(self, filepath: str, source_name: str = None):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            source = source_name or filepath\n",
    "            chunks = self.rag.add_document(content, source)\n",
    "            self.sources[source] = chunks\n",
    "            \n",
    "            print(f\"âœ… '{source}' ë¡œë“œ ì™„ë£Œ ({chunks} ì²­í¬)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"ëŒ€í™”í˜• Q&A ì„¸ì…˜\"\"\"\n",
    "        print(\"\\nğŸ’¬ ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ\")\n",
    "        print(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('ì¢…ë£Œ' ì…ë ¥ì‹œ ì¢…ë£Œ)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"ì§ˆë¬¸: \")\n",
    "            \n",
    "            if question.lower() in ['ì¢…ë£Œ', 'exit', 'quit']:\n",
    "                print(\"ğŸ‘‹ Q&A ì„¸ì…˜ ì¢…ë£Œ\")\n",
    "                break\n",
    "            \n",
    "            # ë‹µë³€ ìƒì„±\n",
    "            result = self.rag.query(question)\n",
    "            \n",
    "            print(f\"\\në‹µë³€: {result['answer']}\\n\")\n",
    "            print(f\"(ì°¸ê³ : {result['num_sources']}ê°œ ì†ŒìŠ¤ í™œìš©)\\n\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def batch_qa(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"ë°°ì¹˜ Q&A ì²˜ë¦¬\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"ì²˜ë¦¬ì¤‘ [{i}/{len(questions)}]: {question[:50]}...\")\n",
    "            result = self.rag.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "doc_qa = DocumentQA()\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "sample_doc = \"\"\"\n",
    "RAG ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ì´ë“œ\n",
    "\n",
    "1. ì†Œê°œ\n",
    "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
    "ê¸°ì¡´ LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
    "- ë¬¸ì„œ ì²˜ë¦¬: í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• \n",
    "- ì„ë² ë”©: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- ë²¡í„° DB: ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰\n",
    "- LLM: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±\n",
    "\n",
    "3. ì¥ì \n",
    "- í™˜ê° í˜„ìƒ ê°ì†Œ\n",
    "- ì¶œì²˜ ì œê³µ ê°€ëŠ¥\n",
    "- ë„ë©”ì¸ íŠ¹í™” ê°€ëŠ¥\n",
    "- ì‹¤ì‹œê°„ ì •ë³´ ë°˜ì˜\n",
    "\n",
    "4. í™œìš© ë¶„ì•¼\n",
    "- ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡\n",
    "- ê¸°ìˆ  ë¬¸ì„œ Q&A\n",
    "- ë²•ë¥  ìë¬¸ ì‹œìŠ¤í…œ\n",
    "- ì˜ë£Œ ì •ë³´ ê²€ìƒ‰\n",
    "\"\"\"\n",
    "\n",
    "# ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "with open('rag_guide.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "doc_qa.load_text_file('rag_guide.txt', 'RAG ê°€ì´ë“œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ì¤‘ [1/3]: RAGì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ”?...\n",
      "ì²˜ë¦¬ì¤‘ [2/3]: RAG ì‹œìŠ¤í…œì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”...\n",
      "ì²˜ë¦¬ì¤‘ [3/3]: RAGëŠ” ì–´ë–¤ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë‚˜ìš”?...\n",
      "\n",
      "ğŸ“Š ë°°ì¹˜ Q&A ê²°ê³¼:\n",
      "\n",
      "[1] Q: RAGì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ”?\n",
      "    A: <think>\n",
      "Okay, let's see. The user is asking about the main components of RAG. First, I need to check the provided context to see if there's any information related to RAG. The context given is a compa...\n",
      "\n",
      "[2] Q: RAG ì‹œìŠ¤í…œì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
      "    A: <think>\n",
      "Okay, the user is asking about the advantages of a RAG system. Let me check the context provided. The context is a company regulations document that includes details about working hours, remot...\n",
      "\n",
      "[3] Q: RAGëŠ” ì–´ë–¤ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë‚˜ìš”?\n",
      "    A: <think>\n",
      "Okay, let's see. The user is asking about where RAG is applied. The context provided is a company regulations document with sections on work hours, remote work, leave, education support, and m...\n"
     ]
    }
   ],
   "source": [
    "# ë°°ì¹˜ Q&A í…ŒìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"RAGì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ”?\",\n",
    "    \"RAG ì‹œìŠ¤í…œì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "    \"RAGëŠ” ì–´ë–¤ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "results = doc_qa.batch_qa(test_questions)\n",
    "\n",
    "print(\"\\nğŸ“Š ë°°ì¹˜ Q&A ê²°ê³¼:\")\n",
    "for i, (q, r) in enumerate(zip(test_questions, results), 1):\n",
    "    print(f\"\\n[{i}] Q: {q}\")\n",
    "    print(f\"    A: {r['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG ì‹œìŠ¤í…œ í‰ê°€ ë° ê°œì„ \n",
    "\n",
    "### í‰ê°€ ë©”íŠ¸ë¦­\n",
    "1. **ê²€ìƒ‰ í’ˆì§ˆ**: Precision, Recall, MRR\n",
    "2. **ìƒì„± í’ˆì§ˆ**: BLEU, ROUGE, ì˜ë¯¸ì  ìœ ì‚¬ë„\n",
    "3. **ì „ì²´ í’ˆì§ˆ**: ì •í™•ë„, ê´€ë ¨ì„±, ì™„ì „ì„±\n",
    "\n",
    "### ê°œì„  ë°©ë²•\n",
    "1. **ì²­í‚¹ ìµœì í™”**: ë¬¸ì„œ íŠ¹ì„±ì— ë§ëŠ” ì²­í¬ í¬ê¸°\n",
    "2. **ì„ë² ë”© ëª¨ë¸ ì„ íƒ**: ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©\n",
    "3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ì˜ë¯¸ + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©\n",
    "4. **ì¬ìˆœìœ„**: Cross-encoderë¡œ ì •í™•ë„ í–¥ìƒ\n",
    "5. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë” ë‚˜ì€ ì»¨í…ìŠ¤íŠ¸ í™œìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. **ê¸°ë³¸ ê³¼ì œ**:\n",
    "   - ìì‹ ì˜ ë¬¸ì„œë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "   - ë‹¤ì–‘í•œ ì²­í¬ í¬ê¸° ì‹¤í—˜\n",
    "   - ê²€ìƒ‰ ê²°ê³¼ í‰ê°€\n",
    "\n",
    "2. **ì‹¬í™” ê³¼ì œ**:\n",
    "   - ë©€í‹°ëª¨ë‹¬ RAG (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)\n",
    "   - ë‹¤êµ­ì–´ RAG ì‹œìŠ¤í…œ\n",
    "   - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ RAG\n",
    "\n",
    "3. **í”„ë¡œì íŠ¸**:\n",
    "   - ê¸°ìˆ  ë¬¸ì„œ Q&A ë´‡\n",
    "   - ë…¼ë¬¸ ê²€ìƒ‰ ë° ìš”ì•½ ì‹œìŠ¤í…œ\n",
    "   - ì½”ë“œë² ì´ìŠ¤ ì§€ì‹ ì–´ì‹œìŠ¤í„´íŠ¸\n",
    "\n",
    "## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "- [RAG ë…¼ë¬¸](https://arxiv.org/abs/2005.11401)\n",
    "- [ChromaDB ë¬¸ì„œ](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [LlamaIndex RAG ê°€ì´ë“œ](https://gpt-index.readthedocs.io/)\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LoRA íŒŒì¸íŠœë‹**ì„ í†µí•´ ëª¨ë¸ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
