{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. LoRA 파인튜닝 - 효율적인 LLM 커스터마이징\n",
    "\n",
    "## 🎯 학습 목표\n",
    "1. 파인튜닝과 LoRA의 개념 이해\n",
    "2. LoRA의 작동 원리와 장점 파악\n",
    "3. PEFT 라이브러리를 활용한 구현\n",
    "4. 실전 파인튜닝 프로젝트 수행\n",
    "\n",
    "## 📚 파인튜닝이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 파인튜닝 기초 개념\n",
    "\n",
    "### 파인튜닝 vs 사전학습 vs 프롬프팅\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────┐\n",
    "│           LLM 커스터마이징 방법 비교            │\n",
    "├────────────────────────────────────────────────┤\n",
    "│                                                │\n",
    "│  1. 프롬프팅 (Prompting)                      │\n",
    "│     • 비용: 💰 (낮음)                          │\n",
    "│     • 시간: ⏱️ (즉시)                          │\n",
    "│     • 효과: ⭐⭐                               │\n",
    "│                                                │\n",
    "│  2. 파인튜닝 (Fine-tuning)                    │\n",
    "│     • 비용: 💰💰💰 (높음)                      │\n",
    "│     • 시간: ⏱️⏱️⏱️ (수일)                      │\n",
    "│     • 효과: ⭐⭐⭐⭐⭐                        │\n",
    "│                                                │\n",
    "│  3. LoRA (Low-Rank Adaptation)                │\n",
    "│     • 비용: 💰💰 (중간)                        │\n",
    "│     • 시간: ⏱️⏱️ (수시간)                      │\n",
    "│     • 효과: ⭐⭐⭐⭐                          │\n",
    "└────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 전통적 파인튜닝의 문제점\n",
    "\n",
    "1. **메모리 문제**: 7B 모델 = 28GB+ GPU 메모리 필요\n",
    "2. **계산 비용**: 모든 파라미터 업데이트 → 높은 연산량\n",
    "3. **저장 공간**: 각 태스크마다 전체 모델 저장\n",
    "4. **과적합 위험**: 작은 데이터셋에서 성능 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA의 핵심 원리\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)란?\n",
    "\n",
    "**핵심 아이디어**: 큰 행렬의 변화를 작은 행렬의 곱으로 표현\n",
    "\n",
    "```\n",
    "원본 가중치 업데이트:           LoRA 방식:\n",
    "                              \n",
    "    W (d×d)                    W (고정) + ΔW\n",
    "                                         ↓\n",
    "  큰 행렬 전체                 W + B×A\n",
    "  업데이트 필요                   (d×r)(r×d)\n",
    "                                    ↓\n",
    "  d²개 파라미터                2×d×r개 파라미터\n",
    "                              (r << d일 때 효율적)\n",
    "```\n",
    "\n",
    "### 수학적 표현\n",
    "\n",
    "- **원본**: `h = Wx`\n",
    "- **LoRA**: `h = Wx + BAx = (W + BA)x`\n",
    "  - `W`: 원본 가중치 (고정)\n",
    "  - `B`: d×r 행렬 (학습)\n",
    "  - `A`: r×d 행렬 (학습)\n",
    "  - `r`: rank (일반적으로 4~64)\n",
    "\n",
    "### LoRA의 장점\n",
    "\n",
    "| 측면 | 전통적 파인튜닝 | LoRA |\n",
    "|-----|----------------|------|\n",
    "| 학습 파라미터 | 100% (7B) | 0.1% (7MB) |\n",
    "| GPU 메모리 | 28GB+ | 6GB |\n",
    "| 학습 시간 | 수일 | 수시간 |\n",
    "| 모델 저장 | 7GB × N개 | 7GB + (7MB × N개) |\n",
    "| 추론 속도 | 동일 | 동일 (병합 후) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 설치\n",
    "!pip install transformers peft accelerate bitsandbytes datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# GPU 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA 구성 상세 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정 파라미터 이해하기\n",
    "\n",
    "def explain_lora_config():\n",
    "    \"\"\"LoRA 설정 파라미터 설명\"\"\"\n",
    "    \n",
    "    config_explanation = {\n",
    "        \"r (rank)\": {\n",
    "            \"설명\": \"LoRA 행렬의 차원 (낮을수록 효율적, 높을수록 표현력↑)\",\n",
    "            \"일반값\": \"4, 8, 16, 32, 64\",\n",
    "            \"선택기준\": \"태스크 복잡도에 비례\"\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"설명\": \"LoRA 스케일링 파라미터 (learning rate와 유사)\",\n",
    "            \"일반값\": \"16, 32, 64\",\n",
    "            \"선택기준\": \"일반적으로 r × 2\"\n",
    "        },\n",
    "        \"lora_dropout\": {\n",
    "            \"설명\": \"과적합 방지를 위한 드롭아웃\",\n",
    "            \"일반값\": \"0.05 ~ 0.1\",\n",
    "            \"선택기준\": \"데이터셋 크기에 반비례\"\n",
    "        },\n",
    "        \"target_modules\": {\n",
    "            \"설명\": \"LoRA를 적용할 레이어\",\n",
    "            \"일반값\": \"['q_proj', 'v_proj'] 또는 ['q_proj', 'k_proj', 'v_proj', 'o_proj']\",\n",
    "            \"선택기준\": \"더 많은 모듈 = 더 높은 성능, 더 많은 메모리\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for param, info in config_explanation.items():\n",
    "        print(f\"\\n📌 {param}\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"  • {key}: {value}\")\n",
    "\n",
    "explain_lora_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 간단한 LoRA 예제 (시뮬레이션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 원리를 보여주는 간단한 시뮬레이션\n",
    "\n",
    "class SimpleLoRA(nn.Module):\n",
    "    \"\"\"LoRA 원리 시연용 간단한 구현\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 원본 가중치 (고정)\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.W.weight.requires_grad = False  # 고정!\n",
    "        \n",
    "        # LoRA 파라미터 (학습 가능)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        \n",
    "        # 초기화\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        self.rank = rank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 원본 출력 + LoRA 출력\n",
    "        original = self.W(x)\n",
    "        lora = self.lora_B(self.lora_A(x))\n",
    "        return original + lora\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"파라미터 수 계산\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"trainable\": trainable,\n",
    "            \"fixed\": total - trainable,\n",
    "            \"reduction\": f\"{(1 - trainable/total)*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "# 예제: 768차원 임베딩 (BERT 크기)\n",
    "d = 768\n",
    "ranks = [4, 8, 16, 32, 64]\n",
    "\n",
    "print(\"LoRA Rank별 파라미터 수 비교\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for r in ranks:\n",
    "    lora_layer = SimpleLoRA(d, d, rank=r)\n",
    "    stats = lora_layer.count_parameters()\n",
    "    \n",
    "    print(f\"\\nRank={r:2d}:\")\n",
    "    print(f\"  • 원본 파라미터: {stats['fixed']:,}\")\n",
    "    print(f\"  • LoRA 파라미터: {stats['trainable']:,}\")\n",
    "    print(f\"  • 절감률: {stats['reduction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 실전: 작은 모델로 LoRA 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLoRATrainer:\n",
    "    \"\"\"간단한 LoRA 파인튜닝 예제\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):  # 작은 모델 사용\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def setup_model_and_tokenizer(self):\n",
    "        \"\"\"모델과 토크나이저 설정\"\"\"\n",
    "        \n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "        \n",
    "        # 토크나이저\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 모델 (8-bit 양자화 옵션)\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                load_in_8bit=True,  # 메모리 절약\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ Model loaded: {self.model.__class__.__name__}\")\n",
    "        \n",
    "    def apply_lora(self, r=8, alpha=32, dropout=0.1):\n",
    "        \"\"\"LoRA 적용\"\"\"\n",
    "        \n",
    "        # LoRA 설정\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"c_attn\", \"c_proj\"]  # GPT2 레이어\n",
    "        )\n",
    "        \n",
    "        # PEFT 모델 생성\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # 학습 가능 파라미터 출력\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "    def prepare_dataset(self, texts, max_length=128):\n",
    "        \"\"\"데이터셋 준비\"\"\"\n",
    "        \n",
    "        def tokenize(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "        \n",
    "        # Dataset 생성\n",
    "        dataset = Dataset.from_dict({\"text\": texts})\n",
    "        tokenized = dataset.map(tokenize, batched=True)\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "# 사용 예제 (실제 실행은 GPU 필요)\n",
    "trainer = MiniLoRATrainer(\"gpt2\")\n",
    "\n",
    "print(\"📝 LoRA 파인튜닝 설정 예제\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "# 1. 모델 로드\n",
    "trainer.setup_model_and_tokenizer()\n",
    "\n",
    "# 2. LoRA 적용\n",
    "trainer.apply_lora(r=8, alpha=16, dropout=0.1)\n",
    "\n",
    "# 3. 데이터 준비\n",
    "texts = [\"Example text 1\", \"Example text 2\"]\n",
    "dataset = trainer.prepare_dataset(texts)\n",
    "\n",
    "# 4. 학습 (실제로는 Trainer 사용)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터셋 준비 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBuilder:\n",
    "    \"\"\"파인튜닝용 데이터셋 생성\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_instruction_dataset():\n",
    "        \"\"\"인스트럭션 튜닝 데이터셋\"\"\"\n",
    "        \n",
    "        instructions = [\n",
    "            {\n",
    "                \"instruction\": \"Python에서 리스트를 역순으로 정렬하는 방법을 알려주세요.\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"리스트를 역순으로 정렬하는 방법:\\n1. list.reverse() - 원본 수정\\n2. reversed(list) - 이터레이터 반환\\n3. list[::-1] - 새 리스트 반환\\n4. sorted(list, reverse=True) - 정렬 후 역순\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"딕셔너리와 리스트의 차이점을 설명하세요.\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"딕셔너리 vs 리스트:\\n• 리스트: 순서가 있는 컬렉션, 인덱스로 접근\\n• 딕셔너리: 키-값 쌍, 키로 접근, 순서 보장(3.7+)\\n• 리스트는 []사용, 딕셔너리는 {} 사용\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"주어진 코드의 시간복잡도를 분석하세요.\",\n",
    "                \"input\": \"for i in range(n):\\n    for j in range(n):\\n        print(i, j)\",\n",
    "                \"output\": \"시간복잡도: O(n²)\\n이유: 중첩된 반복문이 각각 n번 실행되므로 총 n×n = n² 번 실행됩니다.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Alpaca 형식으로 변환\n",
    "        formatted_data = []\n",
    "        for item in instructions:\n",
    "            if item[\"input\"]:\n",
    "                text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Input:\\n{item['input']}\\n\\n### Response:\\n{item['output']}\"\n",
    "            else:\n",
    "                text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n{item['output']}\"\n",
    "            formatted_data.append(text)\n",
    "        \n",
    "        return formatted_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_chat_dataset():\n",
    "        \"\"\"대화형 데이터셋\"\"\"\n",
    "        \n",
    "        conversations = [\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"머신러닝과 딥러닝의 차이는?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"머신러닝은 데이터에서 패턴을 학습하는 AI의 한 분야이고, 딥러닝은 인공신경망을 사용하는 머신러닝의 하위 분야입니다. 딥러닝은 더 복잡한 패턴을 학습할 수 있지만 더 많은 데이터와 연산이 필요합니다.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"REST API란?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"REST API는 웹 서비스 설계 아키텍처입니다. HTTP 메서드(GET, POST, PUT, DELETE)를 사용하여 리소스를 조작하고, 무상태성과 클라이언트-서버 구조를 특징으로 합니다.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # ChatML 형식으로 변환\n",
    "        formatted_data = []\n",
    "        for conv in conversations:\n",
    "            text = \"\"\n",
    "            for msg in conv[\"messages\"]:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    text += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "                else:\n",
    "                    text += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "            formatted_data.append(text)\n",
    "        \n",
    "        return formatted_data\n",
    "\n",
    "# 데이터셋 생성 예제\n",
    "builder = DatasetBuilder()\n",
    "\n",
    "print(\"📚 인스트럭션 데이터셋 예제:\")\n",
    "instruction_data = builder.create_instruction_dataset()\n",
    "print(instruction_data[0][:200] + \"...\")\n",
    "\n",
    "print(\"\\n💬 대화형 데이터셋 예제:\")\n",
    "chat_data = builder.create_chat_dataset()\n",
    "print(chat_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 모니터링과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_training_metrics():\n",
    "    \"\"\"학습 메트릭 시각화 (시뮬레이션)\"\"\"\n",
    "    \n",
    "    # 가상의 학습 데이터\n",
    "    epochs = list(range(1, 11))\n",
    "    train_loss = [2.5, 2.1, 1.8, 1.5, 1.3, 1.1, 0.9, 0.8, 0.7, 0.65]\n",
    "    val_loss = [2.6, 2.2, 2.0, 1.8, 1.7, 1.6, 1.5, 1.5, 1.5, 1.5]\n",
    "    learning_rate = [3e-4, 3e-4, 2.5e-4, 2e-4, 1.5e-4, 1e-4, 7e-5, 5e-5, 3e-5, 1e-5]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Loss 그래프\n",
    "    axes[0].plot(epochs, train_loss, 'b-', label='Train Loss')\n",
    "    axes[0].plot(epochs, val_loss, 'r--', label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training vs Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate 그래프\n",
    "    axes[1].plot(epochs, learning_rate, 'g-')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 메모리 사용량\n",
    "    memory_full = [28, 28, 28, 28, 28, 28, 28, 28, 28, 28]\n",
    "    memory_lora = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
    "    \n",
    "    x = [0.5, 1.5]\n",
    "    axes[2].bar(x[0], np.mean(memory_full), width=0.4, label='Full Fine-tuning', color='red', alpha=0.7)\n",
    "    axes[2].bar(x[1], np.mean(memory_lora), width=0.4, label='LoRA', color='blue', alpha=0.7)\n",
    "    axes[2].set_ylabel('GPU Memory (GB)')\n",
    "    axes[2].set_title('Memory Usage Comparison')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(['Full', 'LoRA'])\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 시각화 실행\n",
    "visualize_training_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LoRA 병합과 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRADeployment:\n",
    "    \"\"\"LoRA 모델 병합 및 배포\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_and_save():\n",
    "        \"\"\"LoRA 가중치를 원본 모델에 병합\"\"\"\n",
    "        \n",
    "        print(\"🔄 LoRA 병합 프로세스\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        steps = [\n",
    "            \"1. LoRA 어댑터 로드\",\n",
    "            \"2. 원본 모델 로드\",\n",
    "            \"3. 가중치 병합: W_merged = W_original + B × A\",\n",
    "            \"4. 병합된 모델 저장\",\n",
    "            \"5. 추론 최적화\"\n",
    "        ]\n",
    "        \n",
    "        for step in steps:\n",
    "            print(f\"  {step}\")\n",
    "        \n",
    "        print(\"\\n💾 저장 옵션:\")\n",
    "        print(\"  • 어댑터만 저장: ~10MB (원본 모델 필요)\")\n",
    "        print(\"  • 병합 모델 저장: 전체 크기 (독립 실행 가능)\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def deployment_strategies():\n",
    "        \"\"\"배포 전략\"\"\"\n",
    "        \n",
    "        strategies = {\n",
    "            \"1. 다중 어댑터\": {\n",
    "                \"설명\": \"하나의 베이스 모델 + 여러 LoRA 어댑터\",\n",
    "                \"장점\": \"메모리 효율적, 빠른 전환\",\n",
    "                \"사용사례\": \"다국어 지원, 도메인별 커스터마이징\"\n",
    "            },\n",
    "            \"2. 병합 모델\": {\n",
    "                \"설명\": \"LoRA를 원본에 병합한 단일 모델\",\n",
    "                \"장점\": \"추론 속도 최적화\",\n",
    "                \"사용사례\": \"프로덕션 배포\"\n",
    "            },\n",
    "            \"3. 동적 로딩\": {\n",
    "                \"설명\": \"필요시 LoRA 어댑터 로드/언로드\",\n",
    "                \"장점\": \"유연성, 메모리 관리\",\n",
    "                \"사용사례\": \"멀티테넌트 서비스\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\n🚀 LoRA 배포 전략\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for name, info in strategies.items():\n",
    "            print(f\"\\n{name}\")\n",
    "            for key, value in info.items():\n",
    "                print(f\"  • {key}: {value}\")\n",
    "\n",
    "# 실행\n",
    "deployment = LoRADeployment()\n",
    "deployment.merge_and_save()\n",
    "deployment.deployment_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 실전 프로젝트: 도메인 특화 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainSpecificBot:\n",
    "    \"\"\"도메인 특화 챗봇 파인튜닝 프로젝트\"\"\"\n",
    "    \n",
    "    def __init__(self, domain=\"customer_service\"):\n",
    "        self.domain = domain\n",
    "        self.domains = {\n",
    "            \"customer_service\": {\n",
    "                \"description\": \"고객 서비스 챗봇\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: 환불 정책은 어떻게 되나요?\\nA: 구매 후 14일 이내 미사용 제품에 한해 전액 환불 가능합니다.\",\n",
    "                    \"Q: 배송은 얼마나 걸리나요?\\nA: 일반 배송은 2-3일, 특급 배송은 당일 또는 익일 도착합니다.\",\n",
    "                    \"Q: 회원 등급 혜택은?\\nA: 실버(5%), 골드(10%), 플래티넘(15%) 할인 혜택이 있습니다.\"\n",
    "                ]\n",
    "            },\n",
    "            \"medical\": {\n",
    "                \"description\": \"의료 상담 어시스턴트\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: 두통이 자주 있어요\\nA: 충분한 수분 섭취와 규칙적인 수면이 중요합니다. 증상이 지속되면 의사 상담을 권합니다.\",\n",
    "                    \"Q: 건강검진 주기는?\\nA: 20-30대는 2년마다, 40대 이상은 매년 권장됩니다.\"\n",
    "                ]\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"description\": \"법률 자문 어시스턴트\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: 계약서 검토 포인트는?\\nA: 계약 조건, 책임 범위, 해지 조항, 분쟁 해결 방법을 확인하세요.\",\n",
    "                    \"Q: 임대차 보호법은?\\nA: 임차인 보호를 위해 계약갱신청구권, 전월세상한제 등이 있습니다.\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def prepare_training_data(self, num_samples=100):\n",
    "        \"\"\"학습 데이터 준비\"\"\"\n",
    "        \n",
    "        domain_info = self.domains[self.domain]\n",
    "        print(f\"📊 {domain_info['description']} 데이터 준비\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\n샘플 데이터 ({len(domain_info['sample_data'])}개):\")\n",
    "        for i, sample in enumerate(domain_info['sample_data'][:2], 1):\n",
    "            print(f\"\\n[샘플 {i}]\")\n",
    "            print(sample)\n",
    "        \n",
    "        print(f\"\\n✅ {num_samples}개 학습 데이터 생성 완료 (시뮬레이션)\")\n",
    "        \n",
    "        return domain_info['sample_data']\n",
    "    \n",
    "    def configure_lora_for_domain(self):\n",
    "        \"\"\"도메인별 LoRA 설정 추천\"\"\"\n",
    "        \n",
    "        configs = {\n",
    "            \"customer_service\": {\n",
    "                \"r\": 8,\n",
    "                \"alpha\": 16,\n",
    "                \"dropout\": 0.05,\n",
    "                \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "                \"learning_rate\": 2e-4,\n",
    "                \"epochs\": 3\n",
    "            },\n",
    "            \"medical\": {\n",
    "                \"r\": 16,\n",
    "                \"alpha\": 32,\n",
    "                \"dropout\": 0.1,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"epochs\": 5\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"r\": 32,\n",
    "                \"alpha\": 64,\n",
    "                \"dropout\": 0.1,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                \"learning_rate\": 5e-5,\n",
    "                \"epochs\": 10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = configs[self.domain]\n",
    "        \n",
    "        print(f\"\\n⚙️ {self.domain} 도메인 LoRA 설정\")\n",
    "        print(\"=\"*50)\n",
    "        for key, value in config.items():\n",
    "            print(f\"  • {key}: {value}\")\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def evaluate_performance(self):\n",
    "        \"\"\"성능 평가 메트릭\"\"\"\n",
    "        \n",
    "        print(\"\\n📈 성능 평가 결과 (시뮬레이션)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        metrics = {\n",
    "            \"정확도\": \"92.3%\",\n",
    "            \"F1 Score\": \"0.89\",\n",
    "            \"응답 시간\": \"145ms\",\n",
    "            \"메모리 사용\": \"2.3GB\",\n",
    "            \"사용자 만족도\": \"4.6/5.0\"\n",
    "        }\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  • {metric}: {value}\")\n",
    "\n",
    "# 실행 예제\n",
    "for domain in [\"customer_service\", \"medical\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🎯 {domain.upper()} 도메인 파인튜닝\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bot = DomainSpecificBot(domain)\n",
    "    bot.prepare_training_data()\n",
    "    bot.configure_lora_for_domain()\n",
    "    bot.evaluate_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. LoRA vs 다른 PEFT 방법들\n",
    "\n",
    "### PEFT (Parameter-Efficient Fine-Tuning) 방법 비교\n",
    "\n",
    "| 방법 | 원리 | 장점 | 단점 | 사용 시나리오 |\n",
    "|-----|------|------|------|-------------|\n",
    "| **LoRA** | 저차원 행렬 분해 | 효율적, 병합 가능 | Rank 선택 필요 | 범용적 |\n",
    "| **QLoRA** | LoRA + 4bit 양자화 | 메모리 극도 절약 | 약간의 성능 손실 | GPU 메모리 제한 |\n",
    "| **Prefix Tuning** | 프롬프트 임베딩 학습 | 매우 적은 파라미터 | 긴 시퀀스 제한 | 특정 태스크 |\n",
    "| **P-Tuning** | 연속 프롬프트 | 간단한 구현 | 제한적 표현력 | 분류 태스크 |\n",
    "| **Adapter** | 작은 모듈 삽입 | 모듈식 설계 | 추론 속도 저하 | 다중 태스크 |\n",
    "\n",
    "### QLoRA 특별 설명\n",
    "\n",
    "```python\n",
    "# QLoRA = Quantization + LoRA\n",
    "# 메모리 사용량: 7B 모델 → 4GB GPU에서 실행 가능!\n",
    "\n",
    "qlora_config = {\n",
    "    \"load_in_4bit\": True,  # 4-bit 양자화\n",
    "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "    \"bnb_4bit_use_double_quant\": True,  # 이중 양자화\n",
    "    \"lora_r\": 64,  # 더 높은 rank로 보상\n",
    "    \"lora_alpha\": 16\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 트러블슈팅 가이드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshooting_guide():\n",
    "    \"\"\"일반적인 문제와 해결책\"\"\"\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            \"문제\": \"CUDA Out of Memory\",\n",
    "            \"원인\": \"GPU 메모리 부족\",\n",
    "            \"해결책\": [\n",
    "                \"배치 크기 감소 (batch_size=1)\",\n",
    "                \"gradient_accumulation_steps 증가\",\n",
    "                \"LoRA rank 감소 (r=4)\",\n",
    "                \"QLoRA 사용 (4-bit 양자화)\",\n",
    "                \"더 작은 모델 사용\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"문제\": \"학습이 수렴하지 않음\",\n",
    "            \"원인\": \"하이퍼파라미터 부적절\",\n",
    "            \"해결책\": [\n",
    "                \"Learning rate 조정 (1e-5 ~ 5e-4)\",\n",
    "                \"LoRA alpha 조정\",\n",
    "                \"더 많은 target_modules 추가\",\n",
    "                \"데이터 품질 확인\",\n",
    "                \"더 긴 학습 (epochs 증가)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"문제\": \"추론 속도가 느림\",\n",
    "            \"원인\": \"LoRA 어댑터 오버헤드\",\n",
    "            \"해결책\": [\n",
    "                \"LoRA 가중치 병합\",\n",
    "                \"모델 양자화\",\n",
    "                \"배치 추론 사용\",\n",
    "                \"GPU 추론 최적화\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"문제\": \"과적합 발생\",\n",
    "            \"원인\": \"데이터 부족 또는 모델 과대\",\n",
    "            \"해결책\": [\n",
    "                \"Dropout 증가 (0.1 ~ 0.3)\",\n",
    "                \"LoRA rank 감소\",\n",
    "                \"데이터 증강\",\n",
    "                \"Early stopping 사용\",\n",
    "                \"정규화 강화\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🔧 LoRA 파인튜닝 트러블슈팅 가이드\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for issue in issues:\n",
    "        print(f\"\\n❌ {issue['문제']}\")\n",
    "        print(f\"   원인: {issue['원인']}\")\n",
    "        print(\"   해결책:\")\n",
    "        for solution in issue['해결책']:\n",
    "            print(f\"     • {solution}\")\n",
    "\n",
    "troubleshooting_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실습 과제\n",
    "\n",
    "### 기본 과제\n",
    "1. 자신만의 데이터셋 10개 샘플 준비\n",
    "2. 다양한 LoRA rank 실험 (4, 8, 16)\n",
    "3. 학습 전후 성능 비교\n",
    "\n",
    "### 심화 과제\n",
    "1. QLoRA로 7B 모델 파인튜닝\n",
    "2. 다중 LoRA 어댑터 관리 시스템\n",
    "3. 자동 하이퍼파라미터 튜닝\n",
    "\n",
    "### 프로젝트\n",
    "1. **도메인 전문가 봇**: 특정 분야 Q&A 챗봇\n",
    "2. **코드 생성기**: 프로그래밍 언어별 특화\n",
    "3. **번역 모델**: 특정 언어쌍 최적화\n",
    "\n",
    "## 📚 추가 학습 자료\n",
    "\n",
    "- [LoRA 원논문](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA 논문](https://arxiv.org/abs/2305.14314)\n",
    "- [PEFT 라이브러리](https://github.com/huggingface/peft)\n",
    "- [HuggingFace PEFT 튜토리얼](https://huggingface.co/docs/peft)\n",
    "\n",
    "## 핵심 정리\n",
    "\n",
    "✅ **LoRA는 효율적인 파인튜닝의 게임체인저**\n",
    "- 1% 파라미터로 95% 성능 달성\n",
    "- GPU 메모리 80% 절약\n",
    "- 여러 태스크 어댑터 쉽게 전환\n",
    "\n",
    "✅ **성공적인 LoRA 파인튜닝 팁**\n",
    "1. 작은 rank부터 시작 (r=8)\n",
    "2. 고품질 데이터 > 대량 데이터\n",
    "3. 도메인에 맞는 하이퍼파라미터\n",
    "4. 충분한 평가와 테스트\n",
    "\n",
    "## 다음 단계\n",
    "\n",
    "다음 노트북에서는 **자율 에이전트(Autonomous Agents)**를 구축하여 복잡한 작업을 자동으로 수행하는 시스템을 만들어보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
