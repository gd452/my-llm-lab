{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. LoRA νμΈνλ‹ - ν¨μ¨μ μΈ LLM μ»¤μ¤ν„°λ§μ΄μ§•\n",
    "\n",
    "## π― ν•™μµ λ©ν‘\n",
    "1. νμΈνλ‹κ³Ό LoRAμ κ°λ… μ΄ν•΄\n",
    "2. LoRAμ μ‘λ™ μ›λ¦¬μ™€ μ¥μ  νμ•…\n",
    "3. PEFT λΌμ΄λΈλ¬λ¦¬λ¥Ό ν™μ©ν• κµ¬ν„\n",
    "4. μ‹¤μ „ νμΈνλ‹ ν”„λ΅μ νΈ μν–‰\n",
    "\n",
    "## π“ νμΈνλ‹μ΄λ€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. νμΈνλ‹ κΈ°μ΄ κ°λ…\n",
    "\n",
    "### νμΈνλ‹ vs μ‚¬μ „ν•™μµ vs ν”„λ΅¬ν”„ν…\n",
    "\n",
    "```\n",
    "β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”\n",
    "β”‚           LLM μ»¤μ¤ν„°λ§μ΄μ§• λ°©λ²• λΉ„κµ            β”‚\n",
    "β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤\n",
    "β”‚                                                β”‚\n",
    "β”‚  1. ν”„λ΅¬ν”„ν… (Prompting)                      β”‚\n",
    "β”‚     β€Ά λΉ„μ©: π’° (λ‚®μ)                          β”‚\n",
    "β”‚     β€Ά μ‹κ°„: β±οΈ (μ¦‰μ‹)                          β”‚\n",
    "β”‚     β€Ά ν¨κ³Ό: β­β­                               β”‚\n",
    "β”‚                                                β”‚\n",
    "β”‚  2. νμΈνλ‹ (Fine-tuning)                    β”‚\n",
    "β”‚     β€Ά λΉ„μ©: π’°π’°π’° (λ†’μ)                      β”‚\n",
    "β”‚     β€Ά μ‹κ°„: β±οΈβ±οΈβ±οΈ (μμΌ)                      β”‚\n",
    "β”‚     β€Ά ν¨κ³Ό: β­β­β­β­β­                        β”‚\n",
    "β”‚                                                β”‚\n",
    "β”‚  3. LoRA (Low-Rank Adaptation)                β”‚\n",
    "β”‚     β€Ά λΉ„μ©: π’°π’° (μ¤‘κ°„)                        β”‚\n",
    "β”‚     β€Ά μ‹κ°„: β±οΈβ±οΈ (μμ‹κ°„)                      β”‚\n",
    "β”‚     β€Ά ν¨κ³Ό: β­β­β­β­                          β”‚\n",
    "β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”\n",
    "```\n",
    "\n",
    "### μ „ν†µμ  νμΈνλ‹μ λ¬Έμ μ \n",
    "\n",
    "1. **λ©”λ¨λ¦¬ λ¬Έμ **: 7B λ¨λΈ = 28GB+ GPU λ©”λ¨λ¦¬ ν•„μ”\n",
    "2. **κ³„μ‚° λΉ„μ©**: λ¨λ“  νλΌλ―Έν„° μ—…λ°μ΄νΈ β†’ λ†’μ€ μ—°μ‚°λ‰\n",
    "3. **μ €μ¥ κ³µκ°„**: κ° νƒμ¤ν¬λ§λ‹¤ μ „μ²΄ λ¨λΈ μ €μ¥\n",
    "4. **κ³Όμ ν•© μ„ν—**: μ‘μ€ λ°μ΄ν„°μ…‹μ—μ„ μ„±λ¥ μ €ν•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRAμ ν•µμ‹¬ μ›λ¦¬\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)λ€?\n",
    "\n",
    "**ν•µμ‹¬ μ•„μ΄λ””μ–΄**: ν° ν–‰λ ¬μ λ³€ν™”λ¥Ό μ‘μ€ ν–‰λ ¬μ κ³±μΌλ΅ ν‘ν„\n",
    "\n",
    "```\n",
    "μ›λ³Έ κ°€μ¤‘μΉ μ—…λ°μ΄νΈ:           LoRA λ°©μ‹:\n",
    "                              \n",
    "    W (dΓ—d)                    W (κ³ μ •) + Ξ”W\n",
    "                                         β†“\n",
    "  ν° ν–‰λ ¬ μ „μ²΄                 W + BΓ—A\n",
    "  μ—…λ°μ΄νΈ ν•„μ”                   (dΓ—r)(rΓ—d)\n",
    "                                    β†“\n",
    "  dΒ²κ° νλΌλ―Έν„°                2Γ—dΓ—rκ° νλΌλ―Έν„°\n",
    "                              (r << dμΌ λ• ν¨μ¨μ )\n",
    "```\n",
    "\n",
    "### μν•™μ  ν‘ν„\n",
    "\n",
    "- **μ›λ³Έ**: `h = Wx`\n",
    "- **LoRA**: `h = Wx + BAx = (W + BA)x`\n",
    "  - `W`: μ›λ³Έ κ°€μ¤‘μΉ (κ³ μ •)\n",
    "  - `B`: dΓ—r ν–‰λ ¬ (ν•™μµ)\n",
    "  - `A`: rΓ—d ν–‰λ ¬ (ν•™μµ)\n",
    "  - `r`: rank (μΌλ°μ μΌλ΅ 4~64)\n",
    "\n",
    "### LoRAμ μ¥μ \n",
    "\n",
    "| μΈ΅λ©΄ | μ „ν†µμ  νμΈνλ‹ | LoRA |\n",
    "|-----|----------------|------|\n",
    "| ν•™μµ νλΌλ―Έν„° | 100% (7B) | 0.1% (7MB) |\n",
    "| GPU λ©”λ¨λ¦¬ | 28GB+ | 6GB |\n",
    "| ν•™μµ μ‹κ°„ | μμΌ | μμ‹κ°„ |\n",
    "| λ¨λΈ μ €μ¥ | 7GB Γ— Nκ° | 7GB + (7MB Γ— Nκ°) |\n",
    "| μ¶”λ΅  μ†λ„ | λ™μΌ | λ™μΌ (λ³‘ν•© ν›„) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ν™κ²½ μ„¤μ •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ν•„μ” ν¨ν‚¤μ§€ μ„¤μΉ\n",
    "!pip install transformers peft accelerate bitsandbytes datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# GPU ν™•μΈ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA κµ¬μ„± μƒμ„Έ μ„¤λ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA μ„¤μ • νλΌλ―Έν„° μ΄ν•΄ν•κΈ°\n",
    "\n",
    "def explain_lora_config():\n",
    "    \"\"\"LoRA μ„¤μ • νλΌλ―Έν„° μ„¤λ…\"\"\"\n",
    "    \n",
    "    config_explanation = {\n",
    "        \"r (rank)\": {\n",
    "            \"μ„¤λ…\": \"LoRA ν–‰λ ¬μ μ°¨μ› (λ‚®μ„μλ΅ ν¨μ¨μ , λ†’μ„μλ΅ ν‘ν„λ ¥β†‘)\",\n",
    "            \"μΌλ°κ°’\": \"4, 8, 16, 32, 64\",\n",
    "            \"μ„ νƒκΈ°μ¤€\": \"νƒμ¤ν¬ λ³µμ΅λ„μ— λΉ„λ΅€\"\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"μ„¤λ…\": \"LoRA μ¤μΌ€μΌλ§ νλΌλ―Έν„° (learning rateμ™€ μ μ‚¬)\",\n",
    "            \"μΌλ°κ°’\": \"16, 32, 64\",\n",
    "            \"μ„ νƒκΈ°μ¤€\": \"μΌλ°μ μΌλ΅ r Γ— 2\"\n",
    "        },\n",
    "        \"lora_dropout\": {\n",
    "            \"μ„¤λ…\": \"κ³Όμ ν•© λ°©μ§€λ¥Ό μ„ν• λ“λ΅­μ•„μ›ƒ\",\n",
    "            \"μΌλ°κ°’\": \"0.05 ~ 0.1\",\n",
    "            \"μ„ νƒκΈ°μ¤€\": \"λ°μ΄ν„°μ…‹ ν¬κΈ°μ— λ°λΉ„λ΅€\"\n",
    "        },\n",
    "        \"target_modules\": {\n",
    "            \"μ„¤λ…\": \"LoRAλ¥Ό μ μ©ν•  λ μ΄μ–΄\",\n",
    "            \"μΌλ°κ°’\": \"['q_proj', 'v_proj'] λλ” ['q_proj', 'k_proj', 'v_proj', 'o_proj']\",\n",
    "            \"μ„ νƒκΈ°μ¤€\": \"λ” λ§μ€ λ¨λ“ = λ” λ†’μ€ μ„±λ¥, λ” λ§μ€ λ©”λ¨λ¦¬\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for param, info in config_explanation.items():\n",
    "        print(f\"\\nπ“ {param}\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"  β€Ά {key}: {value}\")\n",
    "\n",
    "explain_lora_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. κ°„λ‹¨ν• LoRA μμ  (μ‹λ®¬λ μ΄μ…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA μ›λ¦¬λ¥Ό λ³΄μ—¬μ£Όλ” κ°„λ‹¨ν• μ‹λ®¬λ μ΄μ…\n",
    "\n",
    "class SimpleLoRA(nn.Module):\n",
    "    \"\"\"LoRA μ›λ¦¬ μ‹μ—°μ© κ°„λ‹¨ν• κµ¬ν„\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # μ›λ³Έ κ°€μ¤‘μΉ (κ³ μ •)\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.W.weight.requires_grad = False  # κ³ μ •!\n",
    "        \n",
    "        # LoRA νλΌλ―Έν„° (ν•™μµ κ°€λ¥)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        \n",
    "        # μ΄κΈ°ν™”\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        self.rank = rank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # μ›λ³Έ μ¶λ ¥ + LoRA μ¶λ ¥\n",
    "        original = self.W(x)\n",
    "        lora = self.lora_B(self.lora_A(x))\n",
    "        return original + lora\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"νλΌλ―Έν„° μ κ³„μ‚°\"\"\"\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"trainable\": trainable,\n",
    "            \"fixed\": total - trainable,\n",
    "            \"reduction\": f\"{(1 - trainable/total)*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "# μμ : 768μ°¨μ› μ„λ² λ”© (BERT ν¬κΈ°)\n",
    "d = 768\n",
    "ranks = [4, 8, 16, 32, 64]\n",
    "\n",
    "print(\"LoRA Rankλ³„ νλΌλ―Έν„° μ λΉ„κµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for r in ranks:\n",
    "    lora_layer = SimpleLoRA(d, d, rank=r)\n",
    "    stats = lora_layer.count_parameters()\n",
    "    \n",
    "    print(f\"\\nRank={r:2d}:\")\n",
    "    print(f\"  β€Ά μ›λ³Έ νλΌλ―Έν„°: {stats['fixed']:,}\")\n",
    "    print(f\"  β€Ά LoRA νλΌλ―Έν„°: {stats['trainable']:,}\")\n",
    "    print(f\"  β€Ά μ κ°λ¥ : {stats['reduction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. μ‹¤μ „: μ‘μ€ λ¨λΈλ΅ LoRA νμΈνλ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLoRATrainer:\n",
    "    \"\"\"κ°„λ‹¨ν• LoRA νμΈνλ‹ μμ \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):  # μ‘μ€ λ¨λΈ μ‚¬μ©\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def setup_model_and_tokenizer(self):\n",
    "        \"\"\"λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € μ„¤μ •\"\"\"\n",
    "        \n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "        \n",
    "        # ν† ν¬λ‚μ΄μ €\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # λ¨λΈ (8-bit μ–‘μν™” μµμ…)\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                load_in_8bit=True,  # λ©”λ¨λ¦¬ μ μ•½\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(f\"β… Model loaded: {self.model.__class__.__name__}\")\n",
    "        \n",
    "    def apply_lora(self, r=8, alpha=32, dropout=0.1):\n",
    "        \"\"\"LoRA μ μ©\"\"\"\n",
    "        \n",
    "        # LoRA μ„¤μ •\n",
    "        lora_config = LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"c_attn\", \"c_proj\"]  # GPT2 λ μ΄μ–΄\n",
    "        )\n",
    "        \n",
    "        # PEFT λ¨λΈ μƒμ„±\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # ν•™μµ κ°€λ¥ νλΌλ―Έν„° μ¶λ ¥\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "    def prepare_dataset(self, texts, max_length=128):\n",
    "        \"\"\"λ°μ΄ν„°μ…‹ μ¤€λΉ„\"\"\"\n",
    "        \n",
    "        def tokenize(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "        \n",
    "        # Dataset μƒμ„±\n",
    "        dataset = Dataset.from_dict({\"text\": texts})\n",
    "        tokenized = dataset.map(tokenize, batched=True)\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "# μ‚¬μ© μμ  (μ‹¤μ  μ‹¤ν–‰μ€ GPU ν•„μ”)\n",
    "trainer = MiniLoRATrainer(\"gpt2\")\n",
    "\n",
    "print(\"π“ LoRA νμΈνλ‹ μ„¤μ • μμ \")\n",
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "# 1. λ¨λΈ λ΅λ“\n",
    "trainer.setup_model_and_tokenizer()\n",
    "\n",
    "# 2. LoRA μ μ©\n",
    "trainer.apply_lora(r=8, alpha=16, dropout=0.1)\n",
    "\n",
    "# 3. λ°μ΄ν„° μ¤€λΉ„\n",
    "texts = [\"Example text 1\", \"Example text 2\"]\n",
    "dataset = trainer.prepare_dataset(texts)\n",
    "\n",
    "# 4. ν•™μµ (μ‹¤μ λ΅λ” Trainer μ‚¬μ©)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. λ°μ΄ν„°μ…‹ μ¤€λΉ„ μ „λµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBuilder:\n",
    "    \"\"\"νμΈνλ‹μ© λ°μ΄ν„°μ…‹ μƒμ„±\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_instruction_dataset():\n",
    "        \"\"\"μΈμ¤νΈλ­μ… νλ‹ λ°μ΄ν„°μ…‹\"\"\"\n",
    "        \n",
    "        instructions = [\n",
    "            {\n",
    "                \"instruction\": \"Pythonμ—μ„ λ¦¬μ¤νΈλ¥Ό μ—­μμΌλ΅ μ •λ ¬ν•λ” λ°©λ²•μ„ μ•λ ¤μ£Όμ„Έμ”.\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"λ¦¬μ¤νΈλ¥Ό μ—­μμΌλ΅ μ •λ ¬ν•λ” λ°©λ²•:\\n1. list.reverse() - μ›λ³Έ μμ •\\n2. reversed(list) - μ΄ν„°λ μ΄ν„° λ°ν™\\n3. list[::-1] - μƒ λ¦¬μ¤νΈ λ°ν™\\n4. sorted(list, reverse=True) - μ •λ ¬ ν›„ μ—­μ\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"λ”•μ…”λ„λ¦¬μ™€ λ¦¬μ¤νΈμ μ°¨μ΄μ μ„ μ„¤λ…ν•μ„Έμ”.\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"λ”•μ…”λ„λ¦¬ vs λ¦¬μ¤νΈ:\\nβ€Ά λ¦¬μ¤νΈ: μμ„κ°€ μλ” μ»¬λ ‰μ…, μΈλ±μ¤λ΅ μ ‘κ·Ό\\nβ€Ά λ”•μ…”λ„λ¦¬: ν‚¤-κ°’ μ, ν‚¤λ΅ μ ‘κ·Ό, μμ„ λ³΄μ¥(3.7+)\\nβ€Ά λ¦¬μ¤νΈλ” []μ‚¬μ©, λ”•μ…”λ„λ¦¬λ” {} μ‚¬μ©\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"μ£Όμ–΄μ§„ μ½”λ“μ μ‹κ°„λ³µμ΅λ„λ¥Ό λ¶„μ„ν•μ„Έμ”.\",\n",
    "                \"input\": \"for i in range(n):\\n    for j in range(n):\\n        print(i, j)\",\n",
    "                \"output\": \"μ‹κ°„λ³µμ΅λ„: O(nΒ²)\\nμ΄μ : μ¤‘μ²©λ λ°λ³µλ¬Έμ΄ κ°κ° nλ² μ‹¤ν–‰λλ―€λ΅ μ΄ nΓ—n = nΒ² λ² μ‹¤ν–‰λ©λ‹λ‹¤.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Alpaca ν•μ‹μΌλ΅ λ³€ν™\n",
    "        formatted_data = []\n",
    "        for item in instructions:\n",
    "            if item[\"input\"]:\n",
    "                text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Input:\\n{item['input']}\\n\\n### Response:\\n{item['output']}\"\n",
    "            else:\n",
    "                text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n{item['output']}\"\n",
    "            formatted_data.append(text)\n",
    "        \n",
    "        return formatted_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_chat_dataset():\n",
    "        \"\"\"λ€ν™”ν• λ°μ΄ν„°μ…‹\"\"\"\n",
    "        \n",
    "        conversations = [\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"λ¨Έμ‹ λ¬λ‹κ³Ό λ”¥λ¬λ‹μ μ°¨μ΄λ”?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"λ¨Έμ‹ λ¬λ‹μ€ λ°μ΄ν„°μ—μ„ ν¨ν„΄μ„ ν•™μµν•λ” AIμ ν• λ¶„μ•Όμ΄κ³ , λ”¥λ¬λ‹μ€ μΈκ³µμ‹ κ²½λ§μ„ μ‚¬μ©ν•λ” λ¨Έμ‹ λ¬λ‹μ ν•μ„ λ¶„μ•Όμ…λ‹λ‹¤. λ”¥λ¬λ‹μ€ λ” λ³µμ΅ν• ν¨ν„΄μ„ ν•™μµν•  μ μμ§€λ§ λ” λ§μ€ λ°μ΄ν„°μ™€ μ—°μ‚°μ΄ ν•„μ”ν•©λ‹λ‹¤.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"REST APIλ€?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"REST APIλ” μ›Ή μ„λΉ„μ¤ μ„¤κ³„ μ•„ν‚¤ν…μ²μ…λ‹λ‹¤. HTTP λ©”μ„λ“(GET, POST, PUT, DELETE)λ¥Ό μ‚¬μ©ν•μ—¬ λ¦¬μ†μ¤λ¥Ό μ΅°μ‘ν•κ³ , λ¬΄μƒνƒμ„±κ³Ό ν΄λΌμ΄μ–ΈνΈ-μ„λ²„ κµ¬μ΅°λ¥Ό νΉμ§•μΌλ΅ ν•©λ‹λ‹¤.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # ChatML ν•μ‹μΌλ΅ λ³€ν™\n",
    "        formatted_data = []\n",
    "        for conv in conversations:\n",
    "            text = \"\"\n",
    "            for msg in conv[\"messages\"]:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    text += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "                else:\n",
    "                    text += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "            formatted_data.append(text)\n",
    "        \n",
    "        return formatted_data\n",
    "\n",
    "# λ°μ΄ν„°μ…‹ μƒμ„± μμ \n",
    "builder = DatasetBuilder()\n",
    "\n",
    "print(\"π“ μΈμ¤νΈλ­μ… λ°μ΄ν„°μ…‹ μμ :\")\n",
    "instruction_data = builder.create_instruction_dataset()\n",
    "print(instruction_data[0][:200] + \"...\")\n",
    "\n",
    "print(\"\\nπ’¬ λ€ν™”ν• λ°μ΄ν„°μ…‹ μμ :\")\n",
    "chat_data = builder.create_chat_dataset()\n",
    "print(chat_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ν•™μµ λ¨λ‹ν„°λ§κ³Ό ν‰κ°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_training_metrics():\n",
    "    \"\"\"ν•™μµ λ©”νΈλ¦­ μ‹κ°ν™” (μ‹λ®¬λ μ΄μ…)\"\"\"\n",
    "    \n",
    "    # κ°€μƒμ ν•™μµ λ°μ΄ν„°\n",
    "    epochs = list(range(1, 11))\n",
    "    train_loss = [2.5, 2.1, 1.8, 1.5, 1.3, 1.1, 0.9, 0.8, 0.7, 0.65]\n",
    "    val_loss = [2.6, 2.2, 2.0, 1.8, 1.7, 1.6, 1.5, 1.5, 1.5, 1.5]\n",
    "    learning_rate = [3e-4, 3e-4, 2.5e-4, 2e-4, 1.5e-4, 1e-4, 7e-5, 5e-5, 3e-5, 1e-5]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Loss κ·Έλν”„\n",
    "    axes[0].plot(epochs, train_loss, 'b-', label='Train Loss')\n",
    "    axes[0].plot(epochs, val_loss, 'r--', label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training vs Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate κ·Έλν”„\n",
    "    axes[1].plot(epochs, learning_rate, 'g-')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # λ©”λ¨λ¦¬ μ‚¬μ©λ‰\n",
    "    memory_full = [28, 28, 28, 28, 28, 28, 28, 28, 28, 28]\n",
    "    memory_lora = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
    "    \n",
    "    x = [0.5, 1.5]\n",
    "    axes[2].bar(x[0], np.mean(memory_full), width=0.4, label='Full Fine-tuning', color='red', alpha=0.7)\n",
    "    axes[2].bar(x[1], np.mean(memory_lora), width=0.4, label='LoRA', color='blue', alpha=0.7)\n",
    "    axes[2].set_ylabel('GPU Memory (GB)')\n",
    "    axes[2].set_title('Memory Usage Comparison')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(['Full', 'LoRA'])\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# μ‹κ°ν™” μ‹¤ν–‰\n",
    "visualize_training_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LoRA λ³‘ν•©κ³Ό λ°°ν¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRADeployment:\n",
    "    \"\"\"LoRA λ¨λΈ λ³‘ν•© λ° λ°°ν¬\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_and_save():\n",
    "        \"\"\"LoRA κ°€μ¤‘μΉλ¥Ό μ›λ³Έ λ¨λΈμ— λ³‘ν•©\"\"\"\n",
    "        \n",
    "        print(\"π”„ LoRA λ³‘ν•© ν”„λ΅μ„Έμ¤\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        steps = [\n",
    "            \"1. LoRA μ–΄λ‘ν„° λ΅λ“\",\n",
    "            \"2. μ›λ³Έ λ¨λΈ λ΅λ“\",\n",
    "            \"3. κ°€μ¤‘μΉ λ³‘ν•©: W_merged = W_original + B Γ— A\",\n",
    "            \"4. λ³‘ν•©λ λ¨λΈ μ €μ¥\",\n",
    "            \"5. μ¶”λ΅  μµμ ν™”\"\n",
    "        ]\n",
    "        \n",
    "        for step in steps:\n",
    "            print(f\"  {step}\")\n",
    "        \n",
    "        print(\"\\nπ’Ύ μ €μ¥ μµμ…:\")\n",
    "        print(\"  β€Ά μ–΄λ‘ν„°λ§ μ €μ¥: ~10MB (μ›λ³Έ λ¨λΈ ν•„μ”)\")\n",
    "        print(\"  β€Ά λ³‘ν•© λ¨λΈ μ €μ¥: μ „μ²΄ ν¬κΈ° (λ…λ¦½ μ‹¤ν–‰ κ°€λ¥)\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def deployment_strategies():\n",
    "        \"\"\"λ°°ν¬ μ „λµ\"\"\"\n",
    "        \n",
    "        strategies = {\n",
    "            \"1. λ‹¤μ¤‘ μ–΄λ‘ν„°\": {\n",
    "                \"μ„¤λ…\": \"ν•λ‚μ λ² μ΄μ¤ λ¨λΈ + μ—¬λ¬ LoRA μ–΄λ‘ν„°\",\n",
    "                \"μ¥μ \": \"λ©”λ¨λ¦¬ ν¨μ¨μ , λΉ λ¥Έ μ „ν™\",\n",
    "                \"μ‚¬μ©μ‚¬λ΅€\": \"λ‹¤κµ­μ–΄ μ§€μ›, λ„λ©”μΈλ³„ μ»¤μ¤ν„°λ§μ΄μ§•\"\n",
    "            },\n",
    "            \"2. λ³‘ν•© λ¨λΈ\": {\n",
    "                \"μ„¤λ…\": \"LoRAλ¥Ό μ›λ³Έμ— λ³‘ν•©ν• λ‹¨μΌ λ¨λΈ\",\n",
    "                \"μ¥μ \": \"μ¶”λ΅  μ†λ„ μµμ ν™”\",\n",
    "                \"μ‚¬μ©μ‚¬λ΅€\": \"ν”„λ΅λ•μ… λ°°ν¬\"\n",
    "            },\n",
    "            \"3. λ™μ  λ΅λ”©\": {\n",
    "                \"μ„¤λ…\": \"ν•„μ”μ‹ LoRA μ–΄λ‘ν„° λ΅λ“/μ–Έλ΅λ“\",\n",
    "                \"μ¥μ \": \"μ μ—°μ„±, λ©”λ¨λ¦¬ κ΄€λ¦¬\",\n",
    "                \"μ‚¬μ©μ‚¬λ΅€\": \"λ©€ν‹°ν…λ„νΈ μ„λΉ„μ¤\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\nπ€ LoRA λ°°ν¬ μ „λµ\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for name, info in strategies.items():\n",
    "            print(f\"\\n{name}\")\n",
    "            for key, value in info.items():\n",
    "                print(f\"  β€Ά {key}: {value}\")\n",
    "\n",
    "# μ‹¤ν–‰\n",
    "deployment = LoRADeployment()\n",
    "deployment.merge_and_save()\n",
    "deployment.deployment_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. μ‹¤μ „ ν”„λ΅μ νΈ: λ„λ©”μΈ νΉν™” μ±—λ΄‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainSpecificBot:\n",
    "    \"\"\"λ„λ©”μΈ νΉν™” μ±—λ΄‡ νμΈνλ‹ ν”„λ΅μ νΈ\"\"\"\n",
    "    \n",
    "    def __init__(self, domain=\"customer_service\"):\n",
    "        self.domain = domain\n",
    "        self.domains = {\n",
    "            \"customer_service\": {\n",
    "                \"description\": \"κ³ κ° μ„λΉ„μ¤ μ±—λ΄‡\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: ν™λ¶ μ •μ±…μ€ μ–΄λ–»κ² λλ‚μ”?\\nA: κµ¬λ§¤ ν›„ 14μΌ μ΄λ‚΄ λ―Έμ‚¬μ© μ ν’μ— ν•ν•΄ μ „μ•΅ ν™λ¶ κ°€λ¥ν•©λ‹λ‹¤.\",\n",
    "                    \"Q: λ°°μ†΅μ€ μ–Όλ§λ‚ κ±Έλ¦¬λ‚μ”?\\nA: μΌλ° λ°°μ†΅μ€ 2-3μΌ, νΉκΈ‰ λ°°μ†΅μ€ λ‹ΉμΌ λλ” μµμΌ λ„μ°©ν•©λ‹λ‹¤.\",\n",
    "                    \"Q: νμ› λ“±κΈ‰ ννƒμ€?\\nA: μ‹¤λ²„(5%), κ³¨λ“(10%), ν”λν‹°λ„(15%) ν• μΈ ννƒμ΄ μμµλ‹λ‹¤.\"\n",
    "                ]\n",
    "            },\n",
    "            \"medical\": {\n",
    "                \"description\": \"μλ£ μƒλ‹΄ μ–΄μ‹μ¤ν„΄νΈ\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: λ‘ν†µμ΄ μμ£Ό μμ–΄μ”\\nA: μ¶©λ¶„ν• μλ¶„ μ„­μ·¨μ™€ κ·μΉ™μ μΈ μλ©΄μ΄ μ¤‘μ”ν•©λ‹λ‹¤. μ¦μƒμ΄ μ§€μ†λλ©΄ μμ‚¬ μƒλ‹΄μ„ κ¶ν•©λ‹λ‹¤.\",\n",
    "                    \"Q: κ±΄κ°•κ²€μ§„ μ£ΌκΈ°λ”?\\nA: 20-30λ€λ” 2λ…„λ§λ‹¤, 40λ€ μ΄μƒμ€ λ§¤λ…„ κ¶μ¥λ©λ‹λ‹¤.\"\n",
    "                ]\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"description\": \"λ²•λ¥  μλ¬Έ μ–΄μ‹μ¤ν„΄νΈ\",\n",
    "                \"sample_data\": [\n",
    "                    \"Q: κ³„μ•½μ„ κ²€ν†  ν¬μΈνΈλ”?\\nA: κ³„μ•½ μ΅°κ±΄, μ±…μ„ λ²”μ„, ν•΄μ§€ μ΅°ν•­, λ¶„μ ν•΄κ²° λ°©λ²•μ„ ν™•μΈν•μ„Έμ”.\",\n",
    "                    \"Q: μ„λ€μ°¨ λ³΄νΈλ²•μ€?\\nA: μ„μ°¨μΈ λ³΄νΈλ¥Ό μ„ν•΄ κ³„μ•½κ°±μ‹ μ²­κµ¬κ¶, μ „μ›”μ„Έμƒν•μ  λ“±μ΄ μμµλ‹λ‹¤.\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def prepare_training_data(self, num_samples=100):\n",
    "        \"\"\"ν•™μµ λ°μ΄ν„° μ¤€λΉ„\"\"\"\n",
    "        \n",
    "        domain_info = self.domains[self.domain]\n",
    "        print(f\"π“ {domain_info['description']} λ°μ΄ν„° μ¤€λΉ„\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nμƒν” λ°μ΄ν„° ({len(domain_info['sample_data'])}κ°):\")\n",
    "        for i, sample in enumerate(domain_info['sample_data'][:2], 1):\n",
    "            print(f\"\\n[μƒν” {i}]\")\n",
    "            print(sample)\n",
    "        \n",
    "        print(f\"\\nβ… {num_samples}κ° ν•™μµ λ°μ΄ν„° μƒμ„± μ™„λ£ (μ‹λ®¬λ μ΄μ…)\")\n",
    "        \n",
    "        return domain_info['sample_data']\n",
    "    \n",
    "    def configure_lora_for_domain(self):\n",
    "        \"\"\"λ„λ©”μΈλ³„ LoRA μ„¤μ • μ¶”μ²\"\"\"\n",
    "        \n",
    "        configs = {\n",
    "            \"customer_service\": {\n",
    "                \"r\": 8,\n",
    "                \"alpha\": 16,\n",
    "                \"dropout\": 0.05,\n",
    "                \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "                \"learning_rate\": 2e-4,\n",
    "                \"epochs\": 3\n",
    "            },\n",
    "            \"medical\": {\n",
    "                \"r\": 16,\n",
    "                \"alpha\": 32,\n",
    "                \"dropout\": 0.1,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"epochs\": 5\n",
    "            },\n",
    "            \"legal\": {\n",
    "                \"r\": 32,\n",
    "                \"alpha\": 64,\n",
    "                \"dropout\": 0.1,\n",
    "                \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                \"learning_rate\": 5e-5,\n",
    "                \"epochs\": 10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = configs[self.domain]\n",
    "        \n",
    "        print(f\"\\nβ™οΈ {self.domain} λ„λ©”μΈ LoRA μ„¤μ •\")\n",
    "        print(\"=\"*50)\n",
    "        for key, value in config.items():\n",
    "            print(f\"  β€Ά {key}: {value}\")\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def evaluate_performance(self):\n",
    "        \"\"\"μ„±λ¥ ν‰κ°€ λ©”νΈλ¦­\"\"\"\n",
    "        \n",
    "        print(\"\\nπ“ μ„±λ¥ ν‰κ°€ κ²°κ³Ό (μ‹λ®¬λ μ΄μ…)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        metrics = {\n",
    "            \"μ •ν™•λ„\": \"92.3%\",\n",
    "            \"F1 Score\": \"0.89\",\n",
    "            \"μ‘λ‹µ μ‹κ°„\": \"145ms\",\n",
    "            \"λ©”λ¨λ¦¬ μ‚¬μ©\": \"2.3GB\",\n",
    "            \"μ‚¬μ©μ λ§μ΅±λ„\": \"4.6/5.0\"\n",
    "        }\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  β€Ά {metric}: {value}\")\n",
    "\n",
    "# μ‹¤ν–‰ μμ \n",
    "for domain in [\"customer_service\", \"medical\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"π― {domain.upper()} λ„λ©”μΈ νμΈνλ‹\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bot = DomainSpecificBot(domain)\n",
    "    bot.prepare_training_data()\n",
    "    bot.configure_lora_for_domain()\n",
    "    bot.evaluate_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. LoRA vs λ‹¤λ¥Έ PEFT λ°©λ²•λ“¤\n",
    "\n",
    "### PEFT (Parameter-Efficient Fine-Tuning) λ°©λ²• λΉ„κµ\n",
    "\n",
    "| λ°©λ²• | μ›λ¦¬ | μ¥μ  | λ‹¨μ  | μ‚¬μ© μ‹λ‚λ¦¬μ¤ |\n",
    "|-----|------|------|------|-------------|\n",
    "| **LoRA** | μ €μ°¨μ› ν–‰λ ¬ λ¶„ν•΄ | ν¨μ¨μ , λ³‘ν•© κ°€λ¥ | Rank μ„ νƒ ν•„μ” | λ²”μ©μ  |\n",
    "| **QLoRA** | LoRA + 4bit μ–‘μν™” | λ©”λ¨λ¦¬ κ·Ήλ„ μ μ•½ | μ•½κ°„μ μ„±λ¥ μ†μ‹¤ | GPU λ©”λ¨λ¦¬ μ ν• |\n",
    "| **Prefix Tuning** | ν”„λ΅¬ν”„νΈ μ„λ² λ”© ν•™μµ | λ§¤μ° μ μ€ νλΌλ―Έν„° | κΈ΄ μ‹ν€€μ¤ μ ν• | νΉμ • νƒμ¤ν¬ |\n",
    "| **P-Tuning** | μ—°μ† ν”„λ΅¬ν”„νΈ | κ°„λ‹¨ν• κµ¬ν„ | μ ν•μ  ν‘ν„λ ¥ | λ¶„λ¥ νƒμ¤ν¬ |\n",
    "| **Adapter** | μ‘μ€ λ¨λ“ μ‚½μ… | λ¨λ“μ‹ μ„¤κ³„ | μ¶”λ΅  μ†λ„ μ €ν• | λ‹¤μ¤‘ νƒμ¤ν¬ |\n",
    "\n",
    "### QLoRA νΉλ³„ μ„¤λ…\n",
    "\n",
    "```python\n",
    "# QLoRA = Quantization + LoRA\n",
    "# λ©”λ¨λ¦¬ μ‚¬μ©λ‰: 7B λ¨λΈ β†’ 4GB GPUμ—μ„ μ‹¤ν–‰ κ°€λ¥!\n",
    "\n",
    "qlora_config = {\n",
    "    \"load_in_4bit\": True,  # 4-bit μ–‘μν™”\n",
    "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "    \"bnb_4bit_use_double_quant\": True,  # μ΄μ¤‘ μ–‘μν™”\n",
    "    \"lora_r\": 64,  # λ” λ†’μ€ rankλ΅ λ³΄μƒ\n",
    "    \"lora_alpha\": 16\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. νΈλ¬λΈ”μν… κ°€μ΄λ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshooting_guide():\n",
    "    \"\"\"μΌλ°μ μΈ λ¬Έμ μ™€ ν•΄κ²°μ±…\"\"\"\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            \"λ¬Έμ \": \"CUDA Out of Memory\",\n",
    "            \"μ›μΈ\": \"GPU λ©”λ¨λ¦¬ λ¶€μ΅±\",\n",
    "            \"ν•΄κ²°μ±…\": [\n",
    "                \"λ°°μΉ ν¬κΈ° κ°μ† (batch_size=1)\",\n",
    "                \"gradient_accumulation_steps μ¦κ°€\",\n",
    "                \"LoRA rank κ°μ† (r=4)\",\n",
    "                \"QLoRA μ‚¬μ© (4-bit μ–‘μν™”)\",\n",
    "                \"λ” μ‘μ€ λ¨λΈ μ‚¬μ©\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"λ¬Έμ \": \"ν•™μµμ΄ μλ ΄ν•μ§€ μ•μ\",\n",
    "            \"μ›μΈ\": \"ν•μ΄νΌνλΌλ―Έν„° λ¶€μ μ \",\n",
    "            \"ν•΄κ²°μ±…\": [\n",
    "                \"Learning rate μ΅°μ • (1e-5 ~ 5e-4)\",\n",
    "                \"LoRA alpha μ΅°μ •\",\n",
    "                \"λ” λ§μ€ target_modules μ¶”κ°€\",\n",
    "                \"λ°μ΄ν„° ν’μ§ ν™•μΈ\",\n",
    "                \"λ” κΈ΄ ν•™μµ (epochs μ¦κ°€)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"λ¬Έμ \": \"μ¶”λ΅  μ†λ„κ°€ λλ¦Ό\",\n",
    "            \"μ›μΈ\": \"LoRA μ–΄λ‘ν„° μ¤λ²„ν—¤λ“\",\n",
    "            \"ν•΄κ²°μ±…\": [\n",
    "                \"LoRA κ°€μ¤‘μΉ λ³‘ν•©\",\n",
    "                \"λ¨λΈ μ–‘μν™”\",\n",
    "                \"λ°°μΉ μ¶”λ΅  μ‚¬μ©\",\n",
    "                \"GPU μ¶”λ΅  μµμ ν™”\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"λ¬Έμ \": \"κ³Όμ ν•© λ°μƒ\",\n",
    "            \"μ›μΈ\": \"λ°μ΄ν„° λ¶€μ΅± λλ” λ¨λΈ κ³Όλ€\",\n",
    "            \"ν•΄κ²°μ±…\": [\n",
    "                \"Dropout μ¦κ°€ (0.1 ~ 0.3)\",\n",
    "                \"LoRA rank κ°μ†\",\n",
    "                \"λ°μ΄ν„° μ¦κ°•\",\n",
    "                \"Early stopping μ‚¬μ©\",\n",
    "                \"μ •κ·ν™” κ°•ν™”\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"π”§ LoRA νμΈνλ‹ νΈλ¬λΈ”μν… κ°€μ΄λ“\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for issue in issues:\n",
    "        print(f\"\\nβ {issue['λ¬Έμ ']}\")\n",
    "        print(f\"   μ›μΈ: {issue['μ›μΈ']}\")\n",
    "        print(\"   ν•΄κ²°μ±…:\")\n",
    "        for solution in issue['ν•΄κ²°μ±…']:\n",
    "            print(f\"     β€Ά {solution}\")\n",
    "\n",
    "troubleshooting_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## π― μ‹¤μµ κ³Όμ \n",
    "\n",
    "### κΈ°λ³Έ κ³Όμ \n",
    "1. μμ‹ λ§μ λ°μ΄ν„°μ…‹ 10κ° μƒν” μ¤€λΉ„\n",
    "2. λ‹¤μ–‘ν• LoRA rank μ‹¤ν— (4, 8, 16)\n",
    "3. ν•™μµ μ „ν›„ μ„±λ¥ λΉ„κµ\n",
    "\n",
    "### μ‹¬ν™” κ³Όμ \n",
    "1. QLoRAλ΅ 7B λ¨λΈ νμΈνλ‹\n",
    "2. λ‹¤μ¤‘ LoRA μ–΄λ‘ν„° κ΄€λ¦¬ μ‹μ¤ν…\n",
    "3. μλ™ ν•μ΄νΌνλΌλ―Έν„° νλ‹\n",
    "\n",
    "### ν”„λ΅μ νΈ\n",
    "1. **λ„λ©”μΈ μ „λ¬Έκ°€ λ΄‡**: νΉμ • λ¶„μ•Ό Q&A μ±—λ΄‡\n",
    "2. **μ½”λ“ μƒμ„±κΈ°**: ν”„λ΅κ·Έλλ° μ–Έμ–΄λ³„ νΉν™”\n",
    "3. **λ²μ—­ λ¨λΈ**: νΉμ • μ–Έμ–΄μ μµμ ν™”\n",
    "\n",
    "## π“ μ¶”κ°€ ν•™μµ μλ£\n",
    "\n",
    "- [LoRA μ›λ…Όλ¬Έ](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA λ…Όλ¬Έ](https://arxiv.org/abs/2305.14314)\n",
    "- [PEFT λΌμ΄λΈλ¬λ¦¬](https://github.com/huggingface/peft)\n",
    "- [HuggingFace PEFT νν† λ¦¬μ–Ό](https://huggingface.co/docs/peft)\n",
    "\n",
    "## ν•µμ‹¬ μ •λ¦¬\n",
    "\n",
    "β… **LoRAλ” ν¨μ¨μ μΈ νμΈνλ‹μ κ²μ„μ²΄μΈμ €**\n",
    "- 1% νλΌλ―Έν„°λ΅ 95% μ„±λ¥ λ‹¬μ„±\n",
    "- GPU λ©”λ¨λ¦¬ 80% μ μ•½\n",
    "- μ—¬λ¬ νƒμ¤ν¬ μ–΄λ‘ν„° μ‰½κ² μ „ν™\n",
    "\n",
    "β… **μ„±κ³µμ μΈ LoRA νμΈνλ‹ ν**\n",
    "1. μ‘μ€ rankλ¶€ν„° μ‹μ‘ (r=8)\n",
    "2. κ³ ν’μ§ λ°μ΄ν„° > λ€λ‰ λ°μ΄ν„°\n",
    "3. λ„λ©”μΈμ— λ§λ” ν•μ΄νΌνλΌλ―Έν„°\n",
    "4. μ¶©λ¶„ν• ν‰κ°€μ™€ ν…μ¤νΈ\n",
    "\n",
    "## λ‹¤μ λ‹¨κ³„\n",
    "\n",
    "λ‹¤μ λ…ΈνΈλ¶μ—μ„λ” **μμ¨ μ—μ΄μ „νΈ(Autonomous Agents)**λ¥Ό κµ¬μ¶•ν•μ—¬ λ³µμ΅ν• μ‘μ—…μ„ μλ™μΌλ΅ μν–‰ν•λ” μ‹μ¤ν…μ„ λ§λ“¤μ–΄λ³΄κ² μµλ‹λ‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
