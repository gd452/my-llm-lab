{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Ollama + Qwen3 ê¸°ì´ˆ\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "1. Ollamaì˜ ê°œë…ê³¼ ì‘ë™ ì›ë¦¬ ì´í•´\n",
    "2. Qwen3 ëª¨ë¸ì˜ íŠ¹ì§•ê³¼ ì¥ì  íŒŒì•…\n",
    "3. ë¡œì»¬ LLM ì‹¤í–‰ ë° í™œìš©ë²• ìŠµë“\n",
    "4. Thinking Modeë¥¼ í†µí•œ ì‹¬ì¸µ ì¶”ë¡  í™œìš©\n",
    "\n",
    "## ğŸ“š í•µì‹¬ ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ollamaë€?\n",
    "\n",
    "**Ollama**ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ë¡œì»¬ì—ì„œ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” íŠ¹ì§•:\n",
    "- ğŸ–¥ï¸ **ë¡œì»¬ ì‹¤í–‰**: ì¸í„°ë„· ì—°ê²° ì—†ì´ ê°œì¸ ì»´í“¨í„°ì—ì„œ ì‹¤í–‰\n",
    "- ğŸ”’ **í”„ë¼ì´ë²„ì‹œ**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŒ\n",
    "- ğŸ’° **ë¬´ë£Œ**: API ë¹„ìš© ì—†ì´ ë¬´ì œí•œ ì‚¬ìš©\n",
    "- ğŸš€ **ê°„í¸í•œ ì„¤ì •**: í•œ ì¤„ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰\n",
    "\n",
    "### ì‘ë™ ì›ë¦¬:\n",
    "```\n",
    "ì‚¬ìš©ì â†’ Ollama CLI â†’ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ â†’ ë¡œì»¬ ì„œë²„ ì‹¤í–‰ â†’ REST API ì œê³µ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Qwen3 ëª¨ë¸ ì†Œê°œ\n",
    "\n",
    "**Qwen3**ëŠ” Alibabaì—ì„œ 2025ë…„ 4ì›”ì— ì¶œì‹œí•œ ìµœì‹  ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "### Qwen3 vs Qwen2 ë¹„êµ:\n",
    "\n",
    "| íŠ¹ì§• | Qwen2 | Qwen3 |\n",
    "|------|-------|-------|\n",
    "| ì¶”ë¡  ëŠ¥ë ¥ | ì¢‹ìŒ | ë§¤ìš° ë›°ì–´ë‚¨ (QwQ ìˆ˜ì¤€) |\n",
    "| ëª¨ë¸ íš¨ìœ¨ì„± | ì¼ë°˜ | ë›°ì–´ë‚¨ (4Bê°€ 72B ì„±ëŠ¥) |\n",
    "| Thinking Mode | âŒ | âœ… /think, /no_think |\n",
    "| MoE ì§€ì› | ì œí•œì  | ì™„ì „ ì§€ì› |\n",
    "| ì½”ë”© ëŠ¥ë ¥ | ì¢‹ìŒ | ë§¤ìš° ë›°ì–´ë‚¨ |\n",
    "\n",
    "### Thinking Modeë€?\n",
    "- ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ëŠ” ëª¨ë“œ\n",
    "- `/think`ë¡œ í™œì„±í™”, `/no_think`ë¡œ ë¹„í™œì„±í™”\n",
    "- Chain of Thought (CoT) ì¶”ë¡ ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama ì„¤ì¹˜ í™•ì¸\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ í•œ ë²ˆë§Œ)\n",
    "# í¬ê¸°ë³„ ì„ íƒ:\n",
    "# - qwen3:4b (ì‘ì§€ë§Œ ê°•ë ¥)\n",
    "# - qwen3:8b (ê· í˜•ì¡íŒ ì„ íƒ) \n",
    "# - qwen3:30b-a3b (MoE, ê³ ì„±ëŠ¥)\n",
    "\n",
    "!ollama pull qwen3:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ê¸°ë³¸ ì‚¬ìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def chat_with_qwen(prompt, model=\"qwen3:8b\", thinking_mode=False):\n",
    "    \"\"\"Qwen3ì™€ ëŒ€í™”í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # Thinking Mode ì ìš©\n",
    "    if thinking_mode:\n",
    "        prompt = f\"/think {prompt}\"\n",
    "    \n",
    "    # API í˜¸ì¶œ\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['response']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for three strengths of Python. Let me start by recalling what I know about Python's advantages.\n",
      "\n",
      "First, Python is known for its simplicity and readability. The syntax is straightforward, which makes it easy for beginners to learn and for experienced developers to write code that's easy to maintain. That's a solid point.\n",
      "\n",
      "Second, Python has a vast standard library and a rich ecosystem of third-party packages. This means you can find libraries for almost any task, from web development to data analysis, which saves time and effort. For example, libraries like NumPy, Pandas, and Django are really popular.\n",
      "\n",
      "Third, Python is versatile and can be used across different domains. Whether it's web development, scientific computing, automation, or machine learning, Python has the tools and frameworks to handle it. This versatility makes it a go-to language for many developers.\n",
      "\n",
      "Wait, are there other strengths? Maybe things like dynamic typing or integration with other languages? But the user asked for three, so I should stick to the most commonly cited ones. Let me make sure the three points I mentioned are accurate and cover the main advantages. Also, I should explain each point clearly and concisely, maybe with examples to illustrate them. That should help the user understand why Python is popular.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì˜ ì£¼ìš” ì¥ì ì„ 3ê°€ì§€ ì •ë¦¬í•´ë“œë¦´ê²Œìš”:\n",
      "\n",
      "### 1. **ê°„ê²°í•˜ê³  ê°€ë…ì„±ì´ ë†’ì€ ë¬¸ë²•**\n",
      "   - íŒŒì´ì¬ì€ `print(\"Hello\")`ì²˜ëŸ¼ ì§§ê³  ì§ê´€ì ì¸ ë¬¸ë²•ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´ **ê°œë°œ ìƒì‚°ì„±**ì„ ë†’ì…ë‹ˆë‹¤.\n",
      "   - ë“¤ì—¬ì“°ê¸°(Indentation)ë¡œ ì½”ë“œ ë¸”ë¡ì„ êµ¬ë¶„í•˜ë¯€ë¡œ **ê³µê°„ ë‚­ë¹„ ì—†ì´ êµ¬ì¡°í™”ëœ ì½”ë“œ**ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - ì˜ˆ:  \n",
      "     ```python\n",
      "     def greet():\n",
      "         print(\"Hello, World!\")\n",
      "     ```\n",
      "\n",
      "### 2. **ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ìƒíƒœê³„**\n",
      "   - **í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬**ì™€ **ì‚¬ìš©ì ë¼ì´ë¸ŒëŸ¬ë¦¬**ê°€ í’ë¶€í•´ ë°ì´í„° ë¶„ì„(`pandas`, `numpy`), ë¨¸ì‹ ëŸ¬ë‹(`scikit-learn`, `tensorflow`), ì›¹ ê°œë°œ(`Django`, `Flask`) ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "   - ì˜ˆ:  \n",
      "     ```python\n",
      "     import pandas as pd\n",
      "     df = pd.read_csv(\"data.csv\")\n",
      "     ```\n",
      "\n",
      "### 3. **ë‹¤ì¤‘ ë¶„ì•¼ì—ì„œì˜ ìœ ì—°ì„±**\n",
      "   - **ì›¹ ê°œë°œ**, **ë°ì´í„° ê³¼í•™**, **AI/ë¨¸ì‹ ëŸ¬ë‹**, **ìë™í™”**, **ê²Œì„ ê°œë°œ** ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "   - **ì¸í„°í”„ë¦¬í„° ì–¸ì–´**ë¡œ **ì¦‰ì‹œ ì‹¤í–‰**ì´ ê°€ëŠ¥í•´ ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ì™€ ê°œë°œì´ ìš©ì´í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ì¥ì  ë•ë¶„ì— íŒŒì´ì¬ì€ **ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€** ë‹¤ì–‘í•œ ê°œë°œìë“¤ì˜ ì„ í˜¸ ì–¸ì–´ë¡œ ìë¦¬ ì¡ê³  ìˆìŠµë‹ˆë‹¤! ğŸ\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ ì§ˆë¬¸\n",
    "response = chat_with_qwen(\"íŒŒì´ì¬ì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thinking Mode í™œìš©\n",
    "\n",
    "ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ë¬¸ì œì—ì„œ Thinking Modeë¥¼ í™œìš©í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë°˜ ëª¨ë“œ vs Thinking Mode ë¹„êµ\n",
    "problem = \"ë†ë¶€ê°€ ê°•ì„ ê±´ë„ˆì•¼ í•˜ëŠ”ë°, ëŠ‘ëŒ€, ì–‘, ì–‘ë°°ì¶”ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë³´íŠ¸ëŠ” ë†ë¶€ì™€ í•œ ê°€ì§€ë§Œ ì‹¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŠ‘ëŒ€ì™€ ì–‘ì„ í˜¼ì ë‘ë©´ ëŠ‘ëŒ€ê°€ ì–‘ì„ ë¨¹ê³ , ì–‘ê³¼ ì–‘ë°°ì¶”ë¥¼ í˜¼ì ë‘ë©´ ì–‘ì´ ì–‘ë°°ì¶”ë¥¼ ë¨¹ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ê°•ì„ ê±´ë„ ìˆ˜ ìˆì„ê¹Œìš”?\"\n",
    "\n",
    "print(\"=== ì¼ë°˜ ëª¨ë“œ ===\")\n",
    "normal_response = chat_with_qwen(problem, thinking_mode=False)\n",
    "print(normal_response[:500] + \"...\\n\")\n",
    "\n",
    "print(\"=== Thinking Mode ===\")\n",
    "thinking_response = chat_with_qwen(problem, thinking_mode=True)\n",
    "print(thinking_response[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\n",
    "\n",
    "ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ë³´ëŠ” ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(prompt, model=\"qwen3:8b\"):\n",
    "    \"\"\"ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°\"\"\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": True\n",
    "        },\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line)\n",
    "            token = data.get('response', '')\n",
    "            print(token, end='', flush=True)\n",
    "            full_response += token\n",
    "            \n",
    "            if data.get('done', False):\n",
    "                break\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì´ì•¼ê¸° ìƒì„±\n",
    "story = stream_chat(\"ì¸ê³µì§€ëŠ¥ì´ ì£¼ì¸ê³µì¸ ì§§ì€ SF ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš” (100ì ì´ë‚´)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ì „ í™œìš© ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenAssistant:\n",
    "    \"\"\"ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Qwen3 ì–´ì‹œìŠ¤í„´íŠ¸\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"qwen3:8b\"):\n",
    "        self.model = model\n",
    "        self.base_url = \"http://localhost:11434\"\n",
    "    \n",
    "    def code_review(self, code):\n",
    "        \"\"\"ì½”ë“œ ë¦¬ë·°\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¤ìŒ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        ```python\n",
    "        {code}\n",
    "        ```\n",
    "        \n",
    "        1. ì ì¬ì  ë²„ê·¸\n",
    "        2. ì„±ëŠ¥ ê°œì„ ì \n",
    "        3. ì½”ë“œ ìŠ¤íƒ€ì¼ ì œì•ˆ\n",
    "        \"\"\"\n",
    "        return chat_with_qwen(prompt, self.model)\n",
    "    \n",
    "    def explain_error(self, error_message):\n",
    "        \"\"\"ì—ëŸ¬ ë©”ì‹œì§€ ì„¤ëª…\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¤ìŒ ì—ëŸ¬ë¥¼ ì„¤ëª…í•˜ê³  í•´ê²° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”:\n",
    "        \n",
    "        {error_message}\n",
    "        \"\"\"\n",
    "        return chat_with_qwen(prompt, self.model)\n",
    "    \n",
    "    def generate_docstring(self, function_code):\n",
    "        \"\"\"í•¨ìˆ˜ ë¬¸ì„œí™” ìë™ ìƒì„±\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¤ìŒ í•¨ìˆ˜ì— ëŒ€í•œ docstringì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "        \n",
    "        {function_code}\n",
    "        \n",
    "        Google ìŠ¤íƒ€ì¼ docstringìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "        \"\"\"\n",
    "        return chat_with_qwen(prompt, self.model)\n",
    "    \n",
    "    def sql_from_text(self, description, schema=None):\n",
    "        \"\"\"ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜\"\"\"\n",
    "        prompt = f\"ìì—°ì–´: {description}\\n\"\n",
    "        if schema:\n",
    "            prompt += f\"ìŠ¤í‚¤ë§ˆ: {schema}\\n\"\n",
    "        prompt += \"SQL ì¿¼ë¦¬:\"\n",
    "        return chat_with_qwen(prompt, self.model, thinking_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–´ì‹œìŠ¤í„´íŠ¸ ì‚¬ìš© ì˜ˆì œ\n",
    "assistant = QwenAssistant()\n",
    "\n",
    "# 1. ì½”ë“œ ë¦¬ë·°\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    sum = 0\n",
    "    for i in range(len(numbers)):\n",
    "        sum = sum + numbers[i]\n",
    "    average = sum / len(numbers)\n",
    "    return average\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== ì½”ë“œ ë¦¬ë·° ===\")\n",
    "review = assistant.code_review(sample_code)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SQL ìƒì„±\n",
    "print(\"\\n=== SQL ìƒì„± ===\")\n",
    "sql = assistant.sql_from_text(\n",
    "    \"ì§€ë‚œ ë‹¬ ë§¤ì¶œì´ 100ë§Œì› ì´ìƒì¸ ê³ ê°ë“¤ì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ì„ ì°¾ì•„ì¤˜\",\n",
    "    schema=\"customers(id, name, email), orders(id, customer_id, amount, date)\"\n",
    ")\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ìµœì í™” íŒ\n",
    "\n",
    "### ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "```bash\n",
    "# GPU ë©”ëª¨ë¦¬ ì œí•œ\n",
    "export OLLAMA_MAX_LOADED_MODELS=1\n",
    "export OLLAMA_NUM_GPU=1\n",
    "\n",
    "# CPU ìŠ¤ë ˆë“œ ì„¤ì •\n",
    "export OLLAMA_NUM_THREAD=8\n",
    "```\n",
    "\n",
    "### ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ\n",
    "- **qwen3:4b**: ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš° (ì±„íŒ…ë´‡, ì‹¤ì‹œê°„ ì²˜ë¦¬)\n",
    "- **qwen3:8b**: ê· í˜•ì¡íŒ ì„ íƒ (ì¼ë°˜ì ì¸ ì‘ì—…)\n",
    "- **qwen3:30b-a3b**: ë³µì¡í•œ ì¶”ë¡ , ì½”ë“œ ìƒì„± (MoE ì•„í‚¤í…ì²˜)\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ìµœì í™”\n",
    "1. ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œ\n",
    "2. ì˜ˆì œ ì œê³µ (Few-shot learning)\n",
    "3. ì¶œë ¥ í˜•ì‹ ëª…ì‹œ\n",
    "4. Thinking ModeëŠ” ë³µì¡í•œ ë¬¸ì œì—ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. **ê¸°ë³¸ ê³¼ì œ**: ìì‹ ë§Œì˜ í”„ë¡¬í”„íŠ¸ 3ê°œë¥¼ ì‘ì„±í•˜ê³  ì¼ë°˜ ëª¨ë“œì™€ Thinking Mode ë¹„êµ\n",
    "2. **ì‹¬í™” ê³¼ì œ**: QwenAssistant í´ë˜ìŠ¤ì— ìƒˆë¡œìš´ ë©”ì„œë“œ 3ê°œ ì¶”ê°€\n",
    "3. **í”„ë¡œì íŠ¸**: íŠ¹ì • ë„ë©”ì¸(ì˜ˆ: ìš”ë¦¬, ìš´ë™, ê³µë¶€)ì— íŠ¹í™”ëœ ì–´ì‹œìŠ¤í„´íŠ¸ ë§Œë“¤ê¸°\n",
    "\n",
    "## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "- [Ollama ê³µì‹ ë¬¸ì„œ](https://github.com/ollama/ollama)\n",
    "- [Qwen3 ë…¼ë¬¸](https://qwenlm.github.io/blog/qwen3/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LangChain**ì„ í™œìš©í•˜ì—¬ ë” ë³µì¡í•œ ì²´ì¸ê³¼ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
