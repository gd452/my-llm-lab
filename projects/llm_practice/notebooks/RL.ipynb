{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f51308",
   "metadata": {},
   "source": [
    "### 아이디어 요약:\n",
    "\t•\t전반전(First half) = “알고리즘이 왕” 모드\n",
    "\t→ 환경은 단순, 탭룰러 Q-learning 같은 전통 RL 알고리즘만으로 벤치마크를 오르는 게임\n",
    "\t•\t후반전(Second half) = “평가/문제정의가 왕” 모드\n",
    "\t→ 추론(Reasoning)을 하나의 ‘행동(Action)’으로 넣어서, 사전지식(priors)·계획(planning)·테스트시점 계산(test-time compute)을 활용\n",
    "\n",
    "아래 예제는 그리드 미로 환경에서 </br>\n",
    "\t1. 전반전: 순수 Q-러닝으로 목표 도달 </br>\n",
    "\t2. 후반전: THINK(추론) → ACT(행동) 루프를 도입해 “열쇠→문→목표” 순서를 스스로 계획해서 해결 </br>\n",
    "을 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013641f",
   "metadata": {},
   "source": [
    "1) 공용 환경: 아주 작은 GridWorld\n",
    "\t-\t.: 빈칸, #: 벽, K: 열쇠, D: 문(열쇠 없으면 못 지나감), G: 목표, S: 시작\n",
    "\t-\t행동: 위/아래/왼/오 + (후반전에서만) THINK\n",
    "\t-\t보상: 한 스텝당 −0.01, 열쇠 획득 시 +0.1, 목표 도달 시 +1.0, 스텝 제한 초과 시 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc586443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridworld_rl.py\n",
    "\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "UP, DOWN, LEFT, RIGHT, THINK = range(5)\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "ACTION_NAMES = {UP:\"UP\", DOWN:\"DOWN\", LEFT:\"LEFT\", RIGHT:\"RIGHT\", THINK:\"THINK\"}\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid: List[str], start: Tuple[int,int], goal: Tuple[int,int],\n",
    "                 key: Optional[Tuple[int,int]]=None, door: Optional[Tuple[int,int]]=None, max_steps: int=200):\n",
    "        self.grid = [list(row) for row in grid]\n",
    "        self.h = len(grid)\n",
    "        self.w = len(grid[0])\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.key = key\n",
    "        self.door = door\n",
    "        self.has_key = False\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x, self.y = self.start\n",
    "        self.has_key = False\n",
    "        self.steps = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        return (self.x, self.y, int(self.has_key))\n",
    "    \n",
    "    def in_bounds(self, x,y):\n",
    "        return 0<=x<self.w and 0<=y<self.h\n",
    "    \n",
    "    def is_wall(self, x,y):\n",
    "        return self.grid[y][x] == '#'\n",
    "    \n",
    "    def step(self, action: int):\n",
    "        reward = -0.01\n",
    "        done = False\n",
    "        info = {}\n",
    "        self.steps += 1\n",
    "        if action == THINK:\n",
    "            # THINK는 외부 세계를 바꾸지 않음(후반전에서 내부 계획에 쓰입니다)\n",
    "            return self._get_state(), -0.005, False, info\n",
    "        \n",
    "        dx, dy = 0,0\n",
    "        if action == UP: dy = -1\n",
    "        elif action == DOWN: dy = 1\n",
    "        elif action == LEFT: dx = -1\n",
    "        elif action == RIGHT: dx = 1\n",
    "        \n",
    "        nx, ny = self.x + dx, self.y + dy\n",
    "        if self.in_bounds(nx, ny) and not self.is_wall(nx, ny):\n",
    "            # 문은 열쇠 없으면 통과 불가\n",
    "            if self.door is not None and (nx, ny) == self.door and not self.has_key:\n",
    "                reward -= 0.02  # 문에 막힘\n",
    "            else:\n",
    "                self.x, self.y = nx, ny\n",
    "        \n",
    "        # 열쇠 획득\n",
    "        if self.key is not None and (self.x, self.y) == self.key and not self.has_key:\n",
    "            self.has_key = True\n",
    "            reward += 0.1\n",
    "        \n",
    "        # 목표 도달(문이 있을 경우 열쇠 소지 필요)\n",
    "        if (self.x, self.y) == self.goal and (self.door is None or self.has_key):\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "        \n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        out = []\n",
    "        for y in range(self.h):\n",
    "            row=\"\"\n",
    "            for x in range(self.w):\n",
    "                if (x,y) == (self.x,self.y):\n",
    "                    row+=\"A\"\n",
    "                elif (x,y) == self.goal:\n",
    "                    row+=\"G\"\n",
    "                elif self.key is not None and (x,y) == self.key and not self.has_key:\n",
    "                    row+=\"K\"\n",
    "                elif self.door is not None and (x,y) == self.door:\n",
    "                    row+=\"D\"\n",
    "                else:\n",
    "                    row+=self.grid[y][x]\n",
    "            out.append(row)\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "def make_simple_maze():\n",
    "    grid = [\n",
    "        \".....\",\n",
    "        \".###.\",\n",
    "        \".#..#\",\n",
    "        \".#.#.\",\n",
    "        \".....\",\n",
    "    ]\n",
    "    start=(0,0); goal=(4,4)\n",
    "    return GridWorld(grid, start, goal, None, None, max_steps=200)\n",
    "\n",
    "def make_door_maze():\n",
    "    # 열쇠를 반드시 먼저 주워야 문을 넘어 목표로 갈 수 있는 구조(조금 더 어렵게)\n",
    "    grid = [\n",
    "        \"#########\",\n",
    "        \"#S....#G#\",\n",
    "        \"#.##.#..#\",\n",
    "        \"#.#D.#..#\",\n",
    "        \"#.#..#..#\",\n",
    "        \"#.####..#\",\n",
    "        \"#..K....#\",\n",
    "        \"#########\",\n",
    "    ]\n",
    "    start=goal=key=door=None\n",
    "    gridL = list(grid)\n",
    "    for y,row in enumerate(gridL):\n",
    "        for x,ch in enumerate(row):\n",
    "            if ch=='S': start=(x,y); gridL[y]=gridL[y].replace('S','.')\n",
    "            if ch=='G': goal=(x,y); gridL[y]=gridL[y].replace('G','.')\n",
    "            if ch=='K': key=(x,y);  gridL[y]=gridL[y].replace('K','.')\n",
    "            if ch=='D': door=(x,y); gridL[y]=gridL[y].replace('D','.')\n",
    "    return GridWorld(gridL, start, goal, key, door, max_steps=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866ddba",
   "metadata": {},
   "source": [
    "2) 전반전: “알고리즘 중심” — 탭룰러 Q-러닝\n",
    "\n",
    "핵심 포인트\n",
    "- 상태(state) = (x, y, has_key) \n",
    "- 행동(action) = UP, DOWN, LEFT, RIGHT \n",
    "- 정책: ε-탐욕(epsilon-greedy)\n",
    "- 업데이트: Q[s,a] ← Q[s,a] + α*(r + γ*max_a' Q[s',a'] − Q[s,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8226a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Simple maze (전반전: 순수 Q-learning) ==\n",
      "[Q] ep 50: success 0.98, avg steps 23.6\n",
      "[Q] ep 100: success 0.99, avg steps 17.1\n",
      "[Q] ep 150: success 0.99, avg steps 14.7\n",
      "[Q] ep 200: success 0.99, avg steps 13.9\n",
      "\n",
      "== Door maze (전반전: 순수 Q-learning) ==\n",
      "[Q] ep 50: success 0.96, avg steps 74.7\n",
      "[Q] ep 100: success 0.98, avg steps 47.7\n",
      "[Q] ep 150: success 0.99, avg steps 38.4\n",
      "[Q] ep 200: success 0.99, avg steps 33.9\n"
     ]
    }
   ],
   "source": [
    "# 이어서 같은 파일에 붙여넣기\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.2, gamma=0.99, epsilon=0.2):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.actions = actions\n",
    "        self.alpha=alpha\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "    \n",
    "    def _key(self, state, action): return (state, action)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        vals = [(self.Q[self._key(state,a)], a) for a in self.actions]\n",
    "        return max(vals)[1]\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        key = self._key(state, action)\n",
    "        max_next = max(self.Q[(next_state,a)] for a in self.actions)\n",
    "        self.Q[key] += self.alpha * (reward + self.gamma*max_next - self.Q[key])\n",
    "\n",
    "def train_qlearning(env_fn, episodes=200, seed=0):\n",
    "    random.seed(seed)\n",
    "    env=env_fn()\n",
    "    agent=QLearningAgent(actions=ACTIONS)\n",
    "    successes=0; total_steps=0\n",
    "    for ep in range(episodes):\n",
    "        s=env.reset()\n",
    "        done=False; steps=0\n",
    "        while not done:\n",
    "            a = agent.choose_action(s)\n",
    "            ns, r, done, _ = env.step(a)\n",
    "            agent.update(s,a,r,ns)\n",
    "            s = ns; steps+=1\n",
    "            if steps>env.max_steps: break\n",
    "        total_steps += steps\n",
    "        if (env.x,env.y)==env.goal and (env.door is None or env.has_key):\n",
    "            successes+=1\n",
    "        if (ep+1)%50==0:\n",
    "            print(f\"[Q] ep {ep+1}: success {successes/(ep+1):.2f}, avg steps {total_steps/(ep+1):.1f}\")\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"== Simple maze (전반전: 순수 Q-learning) ==\")\n",
    "    _ = train_qlearning(make_simple_maze, episodes=200, seed=42)\n",
    "\n",
    "    print(\"\\n== Door maze (전반전: 순수 Q-learning) ==\")\n",
    "    _ = train_qlearning(make_door_maze, episodes=200, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d21f8",
   "metadata": {},
   "source": [
    "3) 후반전: “추론을 행동으로” — ReAct 스타일 미니 에이전트\n",
    "\n",
    "핵심 포인트\n",
    "\t•\tTHINK 라는 내부 행동을 추가: 외부 세계는 안 바꾸고, 계획(plan) 을 업데이트\n",
    "\t•\t사전지식(priors)의 역할을 계획/추론으로 형상화\n",
    "\t•\t여기서는 가르치는 목적상 BFS 계획자를 사용(“열쇠→문→목표” 순서로 경로 계획)\n",
    "\t•\t현실의 LLM-기반 에이전트에서는 언어 priors + 추론이 이 자리를 차지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71091275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Door maze (후반전: ReAct - THINK + ACT) ==\n",
      "[ReAct] ep 1: steps=16, reward=0.94, success=True\n",
      "[ReAct] ep 2: steps=16, reward=0.94, success=True\n",
      "[ReAct] ep 3: steps=16, reward=0.94, success=True\n"
     ]
    }
   ],
   "source": [
    "# 같은 파일 계속\n",
    "\n",
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    아주 단순화한 'Reason + Act' 에이전트\n",
    "    - THINK: 현재 상태에서 '열쇠(필요시) -> 목표' 로 가는 계획(Path)을 BFS로 계산\n",
    "    - ACT: 계획에 따라 한 스텝씩 이동\n",
    "    \"\"\"\n",
    "    def __init__(self, use_memory: bool=False):\n",
    "        self.plan: List[int] = []\n",
    "        self.use_memory = use_memory\n",
    "        self.memory = {}  # (맵 서명) -> plan\n",
    "    \n",
    "    def _env_id(self, env: GridWorld):\n",
    "        return tuple(tuple(row) for row in env.grid), env.start, env.goal, env.key, env.door\n",
    "    \n",
    "    def _neighbors(self, env: GridWorld, x,y, has_key):\n",
    "        for a,(dx,dy) in {UP:(0,-1), DOWN:(0,1), LEFT:(-1,0), RIGHT:(1,0)}.items():\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if not env.in_bounds(nx,ny) or env.is_wall(nx,ny):\n",
    "                continue\n",
    "            if env.door and (nx,ny)==env.door and not has_key:\n",
    "                continue\n",
    "            yield a, nx, ny\n",
    "    \n",
    "    def _bfs(self, env: GridWorld, start: Tuple[int,int], goal: Tuple[int,int], has_key: bool):\n",
    "        q=deque([ (start, []) ])\n",
    "        seen=set([start])\n",
    "        while q:\n",
    "            (x,y), path = q.popleft()\n",
    "            if (x,y)==goal:\n",
    "                return path\n",
    "            for a,nx,ny in self._neighbors(env,x,y,has_key):\n",
    "                if (nx,ny) in seen: continue\n",
    "                seen.add((nx,ny))\n",
    "                q.append(((nx,ny), path+[a]))\n",
    "        return None\n",
    "    \n",
    "    def think(self, env: GridWorld):\n",
    "        # (가르침용 단순화) 지금 위치에서 '열쇠(필요시) -> 목표'로 가는 전체 경로를 만든다\n",
    "        plan=[]\n",
    "        x,y = env.x, env.y\n",
    "        has_key = env.has_key\n",
    "        if env.key and not has_key:\n",
    "            p1 = self._bfs(env, (x,y), env.key, has_key)\n",
    "            if p1 is None: self.plan=[]; return\n",
    "            plan += p1\n",
    "            has_key = True\n",
    "            x,y = env.key\n",
    "        p2 = self._bfs(env, (x,y), env.goal, has_key)\n",
    "        if p2 is None: self.plan=[]; return\n",
    "        plan += p2\n",
    "        self.plan = plan\n",
    "        if self.use_memory:\n",
    "            self.memory[self._env_id(env)] = list(plan)\n",
    "    \n",
    "    def act(self, env: GridWorld):\n",
    "        if not self.plan:\n",
    "            return THINK\n",
    "        a = self.plan.pop(0)\n",
    "        return a\n",
    "\n",
    "def run_react(env_fn, episodes=3, seed=0, use_memory=False):\n",
    "    random.seed(seed)\n",
    "    env=env_fn()\n",
    "    agent=ReActAgent(use_memory=use_memory)\n",
    "    for ep in range(episodes):\n",
    "        env.reset()\n",
    "        steps=0; done=False; total_reward=0.0\n",
    "        agent.think(env)  # 처음에 한 번 생각\n",
    "        while not done and steps<env.max_steps:\n",
    "            a = agent.act(env)  # 계획이 없으면 THINK가 나오고, 있으면 한 칸 이동\n",
    "            ns,r,done,_ = env.step(a)\n",
    "            total_reward += r\n",
    "            # 열쇠를 집거나 계획이 소진되면 다시 THINK로 재계획\n",
    "            if a==THINK or not agent.plan or (env.key and (env.x,env.y)==env.key):\n",
    "                agent.think(env)\n",
    "            steps+=1\n",
    "        print(f\"[ReAct] ep {ep+1}: steps={steps}, reward={total_reward:.2f}, success={done and (env.x,env.y)==env.goal}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n== Door maze (후반전: ReAct - THINK + ACT) ==\")\n",
    "    run_react(make_door_maze, episodes=3, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3d661",
   "metadata": {},
   "source": [
    "실행하면 보통:\n",
    "\t•\tReAct 에이전트는 첫 에피소드부터 바로 성공하고, 스텝 수도 짧습니다(예: ~16스텝).\n",
    "\t•\tTHINK는 외부 세계를 안 바꾸는 내부 행동이고, 그 결과로 계획(plan) 이 갱신됩니다.\n",
    "→ 이게 글에서 말하는 “추론을 행동으로(reasoning as an action)”의 핵심 아이디어예요.\n",
    "\n",
    "후반전 관점: “이미 잘 되는 레시피(프리트레이닝+스케일+추론)를 전제로, 과제를 어떻게 정의/평가하느냐가 핵심이며, 추론/메모리/상호작용을 평가 안에 넣어 실제 유용성(utility)을 끌어올린다.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb250dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
