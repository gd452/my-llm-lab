# Ollama + Qwen ì„¤ì • ê°€ì´ë“œ

## 1. Ollama ì„¤ì¹˜

### macOS
```bash
# Homebrewë¡œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows
```powershell
# WSL2 ì‚¬ìš© ê¶Œìž¥
# ë˜ëŠ” https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
```

## 2. Qwen ëª¨ë¸ ì„¤ì¹˜

```bash
# ëª¨ë¸ í¬ê¸°ë³„ ì„ íƒ
ollama pull qwen2:0.5b    # 500MB - í…ŒìŠ¤íŠ¸ìš©
ollama pull qwen2:1.5b    # 1.5GB - ì¼ë°˜ ìž‘ì—…
ollama pull qwen2:7b      # 4GB - ê³ í’ˆì§ˆ (ì¶”ì²œ)

# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list
```

## 3. ì²« ì‹¤í–‰

```bash
# ëŒ€í™”í˜• ì‹¤í–‰
ollama run qwen2:7b

# API ì„œë²„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
ollama serve

# API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2:7b",
  "prompt": "Hello, how are you?",
  "stream": false
}'
```

## 4. Python ì—°ë™

```python
# requestsë¡œ ê°„ë‹¨ížˆ ì‚¬ìš©
import requests
import json

def chat_with_qwen(prompt):
    response = requests.post('http://localhost:11434/api/generate',
        json={
            "model": "qwen2:7b",
            "prompt": prompt,
            "stream": False
        })
    return response.json()['response']

# í…ŒìŠ¤íŠ¸
result = chat_with_qwen("What is the capital of France?")
print(result)
```

## 5. ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```bash
# GPU ë©”ëª¨ë¦¬ ì œí•œ (NVIDIA)
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_GPU=1

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
export OLLAMA_NUM_THREAD=8

# ëª¨ë¸ ìºì‹œ ìœ„ì¹˜ ë³€ê²½
export OLLAMA_MODELS=/path/to/models
```

## 6. ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# Modelfile ìƒì„±
cat > Modelfile << EOF
FROM qwen2:7b

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM """
You are a helpful coding assistant. 
Always provide clear explanations and working code examples.
"""

# íŒŒë¼ë¯¸í„° ì¡°ì •
PARAMETER temperature 0.7
PARAMETER top_k 40
PARAMETER top_p 0.9
EOF

# ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±
ollama create my-qwen -f Modelfile

# ì‹¤í–‰
ollama run my-qwen
```

## 7. ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë” ìž‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull qwen2:0.5b

# ë˜ëŠ” ì–‘ìží™” ëª¨ë¸
ollama pull qwen2:7b-q4_0  # 4-bit ì–‘ìží™”
```

### ì†ë„ ê°œì„ 
```bash
# GPU ì‚¬ìš© í™•ì¸
ollama run qwen2:7b --verbose

# CPU ì „ìš© ëª¨ë“œ
OLLAMA_NUM_GPU=0 ollama run qwen2:7b
```

### í¬íŠ¸ ë³€ê²½
```bash
# ê¸°ë³¸ 11434 í¬íŠ¸ ë³€ê²½
OLLAMA_HOST=0.0.0.0:8080 ollama serve
```

## ðŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Ollama ì„¤ì¹˜ ì™„ë£Œ
- [ ] Qwen2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] ì²« ëŒ€í™” í…ŒìŠ¤íŠ¸
- [ ] Python API ì—°ë™
- [ ] ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

ì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ `basic_chat.py`ë¡œ ì§„í–‰í•˜ì„¸ìš”!