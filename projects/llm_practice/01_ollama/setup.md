# Ollama + Qwen3 ì„¤ì • ê°€ì´ë“œ

## 1. Ollama ì„¤ì¹˜

### macOS
```bash
# Homebrewë¡œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ
curl -fsSL https://ollama.ai/install.sh | sh
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows
```powershell
# WSL2 ì‚¬ìš© ê¶Œìž¥
# ë˜ëŠ” https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
```

## 2. Qwen ëª¨ë¸ ì„¤ì¹˜

### Qwen3 (ìµœì‹  - ì¶”ì²œ)
```bash
# Qwen3 ëª¨ë¸ (2025ë…„ 4ì›” ì¶œì‹œ)
ollama pull qwen3:4b       # 4B - Qwen2.5-72B ìˆ˜ì¤€ ì„±ëŠ¥
ollama pull qwen3:8b       # 8B - ë²”ìš© ìž‘ì—…
ollama pull qwen3:30b-a3b  # 30B MoE - ê³ ì„±ëŠ¥ (3B í™œì„±í™”)

# íŠ¹ë³„ ê¸°ëŠ¥: Thinking Mode ì§€ì›
# /thinkì™€ /no_thinkë¡œ ë™ì  ì „í™˜ ê°€ëŠ¥
```

### Qwen2.5 (ì•ˆì •ì )
```bash
# Qwen2.5 ëª¨ë¸ (ì½”ë”© íŠ¹í™”)
ollama pull qwen2.5-coder:7b   # ì½”ë”© ì „ë¬¸
ollama pull qwen2.5:7b         # ë²”ìš©
ollama pull qwen2.5:14b        # ê³ ì„±ëŠ¥

# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list
```

## 3. ì²« ì‹¤í–‰

```bash
# ëŒ€í™”í˜• ì‹¤í–‰
ollama run qwen3:8b

# API ì„œë²„ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
ollama serve

# API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:8b",
  "prompt": "Hello, how are you?",
  "stream": false
}'
```

## 4. Python ì—°ë™

```python
# requestsë¡œ ê°„ë‹¨ížˆ ì‚¬ìš©
import requests
import json

def chat_with_qwen(prompt, thinking_mode=False):
    model_prompt = prompt
    if thinking_mode:
        model_prompt = f"/think {prompt}"
    
    response = requests.post('http://localhost:11434/api/generate',
        json={
            "model": "qwen3:8b",
            "prompt": model_prompt,
            "stream": False
        })
    return response.json()['response']

# í…ŒìŠ¤íŠ¸
result = chat_with_qwen("What is the capital of France?")
print(result)
```

## 5. ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```bash
# GPU ë©”ëª¨ë¦¬ ì œí•œ (NVIDIA)
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_GPU=1

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
export OLLAMA_NUM_THREAD=8

# ëª¨ë¸ ìºì‹œ ìœ„ì¹˜ ë³€ê²½
export OLLAMA_MODELS=/path/to/models
```

## 6. ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# Modelfile ìƒì„±
cat > Modelfile << EOF
FROM qwen3:8b

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM """
You are a helpful coding assistant. 
Always provide clear explanations and working code examples.
"""

# íŒŒë¼ë¯¸í„° ì¡°ì •
PARAMETER temperature 0.7
PARAMETER top_k 40
PARAMETER top_p 0.9
EOF

# ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±
ollama create my-qwen -f Modelfile

# ì‹¤í–‰
ollama run my-qwen
```

## 7. ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë” ìž‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull qwen3:4b       # ìž‘ì§€ë§Œ ê°•ë ¥í•¨

# ë˜ëŠ” ì–‘ìží™” ëª¨ë¸
ollama pull qwen3:8b-q4_0  # 4-bit ì–‘ìží™”
```

### ì†ë„ ê°œì„ 
```bash
# GPU ì‚¬ìš© í™•ì¸
ollama run qwen3:8b --verbose

# CPU ì „ìš© ëª¨ë“œ
OLLAMA_NUM_GPU=0 ollama run qwen3:8b
```

### í¬íŠ¸ ë³€ê²½
```bash
# ê¸°ë³¸ 11434 í¬íŠ¸ ë³€ê²½
OLLAMA_HOST=0.0.0.0:8080 ollama serve
```

## ðŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Ollama ì„¤ì¹˜ ì™„ë£Œ
- [ ] Qwen3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] ì²« ëŒ€í™” í…ŒìŠ¤íŠ¸
- [ ] Python API ì—°ë™
- [ ] ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

ì¤€ë¹„ê°€ ì™„ë£Œë˜ë©´ `basic_chat.py`ë¡œ ì§„í–‰í•˜ì„¸ìš”!