{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Text Generation - GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **GPTê°€ ì–´ë–»ê²Œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ”ì§€** ë‹¨ê³„ë³„ë¡œ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "- Autoregressive generation (ìê¸°íšŒê·€ ìƒì„±)\n",
    "- Temperature sampling\n",
    "- Top-k, Top-p sampling\n",
    "- Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoregressive Generation ì´í•´\n",
    "\n",
    "**í•µì‹¬**: í•œ ë²ˆì— í•œ í† í°ì”© ìƒì„±, ì´ì „ ì¶œë ¥ì´ ë‹¤ìŒ ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ vocabulary\n",
    "vocab = ['<PAD>', 'The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'dog', 'ran']\n",
    "vocab_size = len(vocab)\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}  # string to int\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}  # int to string\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "for i, word in enumerate(vocab):\n",
    "    print(f\"  {i}: {word}\")\n",
    "\n",
    "# ì‹œì‘ í† í°\n",
    "context = [stoi['The']]  # \"The\"ë¡œ ì‹œì‘\n",
    "print(f\"\\nì‹œì‘ context: {[itos[i] for i in context]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë‹¤ìŒ í† í° ì˜ˆì¸¡ ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ìƒì˜ ëª¨ë¸ ì¶œë ¥ (logits)\n",
    "# ì‹¤ì œë¡œëŠ” GPTê°€ ì˜ˆì¸¡í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •\n",
    "def mock_gpt_prediction(context):\n",
    "    \"\"\"ì£¼ì–´ì§„ contextì—ì„œ ë‹¤ìŒ í† í° í™•ë¥  ë°˜í™˜ (ê°€ìƒ)\"\"\"\n",
    "    # ì‹¤ì œë¡œëŠ” model(context)ë¡œ ê³„ì‚°\n",
    "    logits = torch.randn(vocab_size)\n",
    "    \n",
    "    # ë¬¸ë²•ì ìœ¼ë¡œ ê°€ëŠ¥í•œ íŒ¨í„´ ê°•ì¡° (ì˜ˆì‹œ)\n",
    "    if context[-1] == stoi['The']:\n",
    "        logits[stoi['cat']] += 3\n",
    "        logits[stoi['dog']] += 2\n",
    "    elif context[-1] == stoi['cat']:\n",
    "        logits[stoi['sat']] += 3\n",
    "        logits[stoi['ran']] += 1\n",
    "    elif context[-1] == stoi['sat']:\n",
    "        logits[stoi['on']] += 4\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "logits = mock_gpt_prediction(context)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# í™•ë¥  ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(vocab_size), probs.numpy())\n",
    "plt.xlabel('Token ID')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Next Token Probabilities after \"The\"')\n",
    "plt.xticks(range(vocab_size), vocab, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 3 í™•ë¥ \n",
    "top_probs, top_indices = torch.topk(probs, 3)\n",
    "print(\"\\nTop 3 predictions:\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    print(f\"  {itos[idx.item()]:10} {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Greedy Decoding vs Sampling\n",
    "\n",
    "**Greedy**: í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ\n",
    "**Sampling**: í™•ë¥  ë¶„í¬ì—ì„œ ëœë¤ ìƒ˜í”Œë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(start_context, num_tokens=5):\n",
    "    \"\"\"Greedy decoding: í•­ìƒ ìµœê³  í™•ë¥  ì„ íƒ\"\"\"\n",
    "    context = start_context.copy()\n",
    "    \n",
    "    for _ in range(num_tokens):\n",
    "        logits = mock_gpt_prediction(context)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probs).item()\n",
    "        context.append(next_token)\n",
    "    \n",
    "    return context\n",
    "\n",
    "def generate_sampling(start_context, num_tokens=5):\n",
    "    \"\"\"Random sampling: í™•ë¥ ì— ë”°ë¼ ìƒ˜í”Œë§\"\"\"\n",
    "    context = start_context.copy()\n",
    "    \n",
    "    for _ in range(num_tokens):\n",
    "        logits = mock_gpt_prediction(context)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1).item()\n",
    "        context.append(next_token)\n",
    "    \n",
    "    return context\n",
    "\n",
    "# ë¹„êµ\n",
    "print(\"Greedy Decoding (ê²°ì •ì ):\")\n",
    "for i in range(3):\n",
    "    result = generate_greedy([stoi['The']], 4)\n",
    "    print(f\"  {i+1}: {' '.join([itos[t] for t in result])}\")\n",
    "\n",
    "print(\"\\nRandom Sampling (ë‹¤ì–‘í•¨):\")\n",
    "for i in range(3):\n",
    "    result = generate_sampling([stoi['The']], 4)\n",
    "    print(f\"  {i+1}: {' '.join([itos[t] for t in result])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temperature Sampling\n",
    "\n",
    "Temperatureë¡œ í™•ë¥  ë¶„í¬ì˜ \"sharpness\" ì¡°ì ˆ:\n",
    "- **Low temperature (< 1.0)**: ë” í™•ì‹¤í•œ ì„ íƒ\n",
    "- **High temperature (> 1.0)**: ë” ë‹¤ì–‘í•œ ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits, temperature):\n",
    "    \"\"\"Temperatureë¥¼ ì ìš©í•œ logits\"\"\"\n",
    "    return logits / temperature\n",
    "\n",
    "# ë‹¤ì–‘í•œ temperature ë¹„êµ\n",
    "logits = mock_gpt_prediction([stoi['The']])\n",
    "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    scaled_logits = apply_temperature(logits, temp)\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    axes[i].bar(range(vocab_size), probs.numpy())\n",
    "    axes[i].set_title(f'Temperature = {temp}')\n",
    "    axes[i].set_xlabel('Token')\n",
    "    axes[i].set_ylabel('Probability')\n",
    "    axes[i].set_xticks(range(vocab_size))\n",
    "    axes[i].set_xticklabels(vocab, rotation=45, ha='right')\n",
    "    \n",
    "    # Entropy ê³„ì‚° (ë‹¤ì–‘ì„± ì²™ë„)\n",
    "    entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "    axes[i].text(0.5, 0.95, f'Entropy: {entropy:.2f}', \n",
    "                transform=axes[i].transAxes, ha='center')\n",
    "\n",
    "plt.suptitle('Effect of Temperature on Probability Distribution', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temperature íš¨ê³¼:\")\n",
    "print(\"- ë‚®ì€ T (0.5): í™•ë¥  ì°¨ì´ ì¦í­, ë³´ìˆ˜ì  ì„ íƒ\")\n",
    "print(\"- ë†’ì€ T (2.0): í™•ë¥  í‰ì¤€í™”, ì°½ì˜ì  ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top-k Sampling\n",
    "\n",
    "ìƒìœ„ kê°œ í† í° ì¤‘ì—ì„œë§Œ ìƒ˜í”Œë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=3, temperature=1.0):\n",
    "    \"\"\"Top-k í† í° ì¤‘ì—ì„œë§Œ ìƒ˜í”Œë§\"\"\"\n",
    "    # Temperature ì ìš©\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Top-k í•„í„°ë§\n",
    "    top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "    \n",
    "    # ë‚˜ë¨¸ì§€ëŠ” -infë¡œ ì„¤ì •\n",
    "    filtered_logits = torch.full_like(logits, float('-inf'))\n",
    "    filtered_logits[top_k_indices] = top_k_values\n",
    "    \n",
    "    # Softmax & sampling\n",
    "    probs = F.softmax(filtered_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1).item()\n",
    "    \n",
    "    return next_token, probs\n",
    "\n",
    "# Top-k íš¨ê³¼ ì‹œê°í™”\n",
    "logits = mock_gpt_prediction([stoi['The']])\n",
    "k_values = [1, 3, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    _, probs = top_k_sampling(logits.clone(), k=k)\n",
    "    \n",
    "    axes[i].bar(range(vocab_size), probs.numpy())\n",
    "    axes[i].set_title(f'Top-{k} Sampling')\n",
    "    axes[i].set_xlabel('Token')\n",
    "    axes[i].set_ylabel('Probability')\n",
    "    axes[i].set_xticks(range(vocab_size))\n",
    "    axes[i].set_xticklabels(vocab, rotation=45, ha='right')\n",
    "    \n",
    "    # ì‹¤ì œ ì„ íƒ ê°€ëŠ¥í•œ í† í° ìˆ˜\n",
    "    num_possible = (probs > 0).sum().item()\n",
    "    axes[i].text(0.5, 0.95, f'Possible: {num_possible}', \n",
    "                transform=axes[i].transAxes, ha='center')\n",
    "\n",
    "plt.suptitle('Top-k Sampling Effect', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top-k íš¨ê³¼:\")\n",
    "print(\"- k=1: Greedy decodingê³¼ ë™ì¼\")\n",
    "print(\"- k=3: ìƒìœ„ 3ê°œ ì¤‘ ì„ íƒ (ì•ˆì „í•˜ë©´ì„œë„ ë‹¤ì–‘)\")\n",
    "print(\"- k=10: ê±°ì˜ ëª¨ë“  í† í° ê°€ëŠ¥ (ì›ë³¸ê³¼ ìœ ì‚¬)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top-p (Nucleus) Sampling\n",
    "\n",
    "ëˆ„ì  í™•ë¥ ì´ pë¥¼ ë„˜ì„ ë•Œê¹Œì§€ì˜ í† í°ë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"ëˆ„ì  í™•ë¥  pê¹Œì§€ì˜ í† í°ë§Œ ì‚¬ìš©\"\"\"\n",
    "    # Temperature & softmax\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # í™•ë¥  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # pë¥¼ ë„˜ëŠ” ì§€ì  ì°¾ê¸°\n",
    "    cutoff_idx = torch.where(cumulative_probs > p)[0]\n",
    "    if len(cutoff_idx) > 0:\n",
    "        cutoff_idx = cutoff_idx[0].item()\n",
    "    else:\n",
    "        cutoff_idx = len(sorted_probs) - 1\n",
    "    \n",
    "    # í•„í„°ë§\n",
    "    filtered_logits = torch.full_like(logits, float('-inf'))\n",
    "    selected_indices = sorted_indices[:cutoff_idx + 1]\n",
    "    filtered_logits[selected_indices] = logits[selected_indices]\n",
    "    \n",
    "    # ì¬ì •ê·œí™” & ìƒ˜í”Œë§\n",
    "    final_probs = F.softmax(filtered_logits, dim=-1)\n",
    "    next_token = torch.multinomial(final_probs, 1).item()\n",
    "    \n",
    "    return next_token, final_probs, cutoff_idx + 1\n",
    "\n",
    "# Top-p íš¨ê³¼ ì‹œê°í™”\n",
    "p_values = [0.5, 0.7, 0.9, 0.99]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "\n",
    "for i, p in enumerate(p_values):\n",
    "    _, probs, num_tokens = top_p_sampling(logits.clone(), p=p)\n",
    "    \n",
    "    axes[i].bar(range(vocab_size), probs.numpy())\n",
    "    axes[i].set_title(f'Top-p (p={p})')\n",
    "    axes[i].set_xlabel('Token')\n",
    "    axes[i].set_ylabel('Probability')\n",
    "    axes[i].set_xticks(range(vocab_size))\n",
    "    axes[i].set_xticklabels(vocab, rotation=45, ha='right')\n",
    "    axes[i].text(0.5, 0.95, f'Using {num_tokens} tokens', \n",
    "                transform=axes[i].transAxes, ha='center')\n",
    "\n",
    "plt.suptitle('Top-p (Nucleus) Sampling', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top-p ì¥ì :\")\n",
    "print(\"- ë™ì  ì„ íƒ: í™•ë¥  ë¶„í¬ì— ë”°ë¼ kê°€ ìë™ ì¡°ì ˆ\")\n",
    "print(\"- ë” ìì—°ìŠ¤ëŸ¬ìš´ ìƒ˜í”Œë§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì „ì²´ ìƒì„± íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(start_tokens, max_new_tokens=10, \n",
    "             temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    ì™„ì „í•œ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    context = start_tokens.copy()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # ëª¨ë¸ ì˜ˆì¸¡ (ê°€ìƒ)\n",
    "        logits = mock_gpt_prediction(context)\n",
    "        \n",
    "        # Temperature ì ìš©\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Top-k í•„í„°ë§\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[-1]] = float('-inf')\n",
    "        \n",
    "        # Top-p í•„í„°ë§\n",
    "        if top_p is not None:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(\n",
    "                F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[0] = False  # ìµœì†Œ 1ê°œëŠ” ìœ ì§€\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # í™•ë¥  ê³„ì‚° & ìƒ˜í”Œë§\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        context.append(next_token)\n",
    "        \n",
    "        # ì¢…ë£Œ ì¡°ê±´ (ì˜ˆ: ë§ˆì¹¨í‘œ)\n",
    "        if next_token == stoi['.']:\n",
    "            break\n",
    "    \n",
    "    return context\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ìƒì„±\n",
    "settings = [\n",
    "    {\"name\": \"Greedy\", \"temp\": 0.01, \"top_k\": 1},\n",
    "    {\"name\": \"Conservative\", \"temp\": 0.7, \"top_k\": 5},\n",
    "    {\"name\": \"Balanced\", \"temp\": 1.0, \"top_p\": 0.9},\n",
    "    {\"name\": \"Creative\", \"temp\": 1.3, \"top_p\": 0.95},\n",
    "]\n",
    "\n",
    "print(\"ë‹¤ì–‘í•œ ìƒì„± ì „ëµ:\\n\")\n",
    "for setting in settings:\n",
    "    print(f\"{setting['name']:15}:\", end=\" \")\n",
    "    \n",
    "    tokens = generate(\n",
    "        [stoi['The']], \n",
    "        max_new_tokens=6,\n",
    "        temperature=setting.get('temp', 1.0),\n",
    "        top_k=setting.get('top_k'),\n",
    "        top_p=setting.get('top_p')\n",
    "    )\n",
    "    \n",
    "    text = ' '.join([itos[t] for t in tokens])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repetition Penalty\n",
    "\n",
    "ë°˜ë³µì„ ì¤„ì´ê¸° ìœ„í•œ í˜ë„í‹°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, generated_tokens, penalty=1.2):\n",
    "    \"\"\"ì´ë¯¸ ìƒì„±ëœ í† í°ì— í˜ë„í‹° ì ìš©\"\"\"\n",
    "    for token in set(generated_tokens):\n",
    "        # ì´ë¯¸ ë‚˜ì˜¨ í† í°ì˜ logitì„ ê°ì†Œ\n",
    "        logits[token] = logits[token] / penalty\n",
    "    return logits\n",
    "\n",
    "# ë°˜ë³µ í˜ë„í‹° íš¨ê³¼ ì‹œì—°\n",
    "context = [stoi['The'], stoi['cat'], stoi['sat']]\n",
    "logits = torch.tensor([1.0] * vocab_size)  # ê· ë“±í•œ logits\n",
    "\n",
    "# í˜ë„í‹° ì—†ìŒ\n",
    "probs_no_penalty = F.softmax(logits, dim=-1)\n",
    "\n",
    "# í˜ë„í‹° ì ìš©\n",
    "logits_with_penalty = apply_repetition_penalty(logits.clone(), context, penalty=2.0)\n",
    "probs_with_penalty = F.softmax(logits_with_penalty, dim=-1)\n",
    "\n",
    "# ë¹„êµ ì‹œê°í™”\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.bar(range(vocab_size), probs_no_penalty.numpy())\n",
    "ax1.set_title('Without Repetition Penalty')\n",
    "ax1.set_xlabel('Token')\n",
    "ax1.set_xticks(range(vocab_size))\n",
    "ax1.set_xticklabels(vocab, rotation=45)\n",
    "\n",
    "ax2.bar(range(vocab_size), probs_with_penalty.numpy())\n",
    "ax2.set_title('With Repetition Penalty (penalty=2.0)')\n",
    "ax2.set_xlabel('Token')\n",
    "ax2.set_xticks(range(vocab_size))\n",
    "ax2.set_xticklabels(vocab, rotation=45)\n",
    "\n",
    "# ì´ë¯¸ ë‚˜ì˜¨ í† í° í‘œì‹œ\n",
    "for idx in context:\n",
    "    ax2.bar(idx, probs_with_penalty[idx].numpy(), color='red', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ë¹¨ê°„ìƒ‰ ë§‰ëŒ€ = ì´ë¯¸ ìƒì„±ëœ í† í° (í˜ë„í‹° ì ìš©)\")\n",
    "print(\"â†’ ë°˜ë³µ í™•ë¥  ê°ì†Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "1. **Autoregressive = ìˆœì°¨ì  ìƒì„±**\n",
    "   - í•œ ë²ˆì— í•œ í† í°\n",
    "   - ì´ì „ ì¶œë ¥ì´ ë‹¤ìŒ ì…ë ¥\n",
    "\n",
    "2. **Temperature = ì°½ì˜ì„± ì¡°ì ˆ**\n",
    "   - Low: ì•ˆì „í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥\n",
    "   - High: ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•¨\n",
    "\n",
    "3. **Top-k/Top-p = í’ˆì§ˆ ë³´ì¥**\n",
    "   - ë‚®ì€ í™•ë¥  í† í° ì œê±°\n",
    "   - ë¬´ì˜ë¯¸í•œ ì¶œë ¥ ë°©ì§€\n",
    "\n",
    "4. **Trade-offs**\n",
    "   - Quality vs Diversity\n",
    "   - Safety vs Creativity\n",
    "   - Speed vs Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ì „ íŒ\n",
    "\n",
    "### ìš©ë„ë³„ ì¶”ì²œ ì„¤ì •\n",
    "\n",
    "**ì½”ë“œ ìƒì„±**\n",
    "```python\n",
    "temperature=0.2, top_k=10\n",
    "# ì •í™•ì„± ì¤‘ìš”, ì°½ì˜ì„± ë¶ˆí•„ìš”\n",
    "```\n",
    "\n",
    "**ì°½ì‘ ê¸€ì“°ê¸°**\n",
    "```python\n",
    "temperature=1.0, top_p=0.9\n",
    "# ë‹¤ì–‘ì„±ê³¼ ì°½ì˜ì„± ì¤‘ìš”\n",
    "```\n",
    "\n",
    "**ëŒ€í™”/ì±—ë´‡**\n",
    "```python\n",
    "temperature=0.7, top_k=40\n",
    "# ê· í˜•ì¡íŒ ì‘ë‹µ\n",
    "```\n",
    "\n",
    "**ìš”ì•½/ë²ˆì—­**\n",
    "```python\n",
    "temperature=0.3, top_k=5\n",
    "# ì •í™•ì„±ê³¼ ì¼ê´€ì„± ì¤‘ìš”\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}