{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Building GPT Block - Transformer의 기본 단위\n",
    "\n",
    "이 노트북에서는 GPT의 기본 빌딩 블록인 **Transformer Block**을 구성합니다.\n",
    "\n",
    "## 구성 요소\n",
    "1. Multi-Head Attention\n",
    "2. Feed-Forward Network\n",
    "3. Layer Normalization\n",
    "4. Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer Normalization\n",
    "\n",
    "**왜 필요한가?**\n",
    "- 깊은 네트워크에서 gradient 안정화\n",
    "- 학습 속도 향상\n",
    "- Internal covariate shift 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 이해하기\n",
    "B, T, C = 2, 4, 8  # batch, sequence, embedding\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# 수동으로 Layer Norm 계산\n",
    "mean = x.mean(dim=-1, keepdim=True)  # 각 토큰의 평균\n",
    "std = x.std(dim=-1, keepdim=True)    # 각 토큰의 표준편차\n",
    "x_norm_manual = (x - mean) / (std + 1e-5)\n",
    "\n",
    "print(f\"원본 통계:\")\n",
    "print(f\"  평균: {x[0, 0].mean():.4f}\")\n",
    "print(f\"  표준편차: {x[0, 0].std():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized 통계:\")\n",
    "print(f\"  평균: {x_norm_manual[0, 0].mean():.4f} (≈0)\")\n",
    "print(f\"  표준편차: {x_norm_manual[0, 0].std():.4f} (≈1)\")\n",
    "\n",
    "# PyTorch Layer Norm\n",
    "layer_norm = nn.LayerNorm(C)\n",
    "x_norm_pytorch = layer_norm(x)\n",
    "\n",
    "print(f\"\\nPyTorch LayerNorm도 동일한 결과:\")\n",
    "print(f\"  평균: {x_norm_pytorch[0, 0].mean():.4f}\")\n",
    "print(f\"  표준편차: {x_norm_pytorch[0, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network (FFN)\n",
    "\n",
    "**구조**: Linear → ReLU → Linear\n",
    "\n",
    "**왜 필요한가?**\n",
    "- Attention은 토큰 간 관계만 학습\n",
    "- FFN은 각 위치에서 독립적으로 특징 변환\n",
    "- 보통 4배 확장 후 축소 (bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # 확장\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # 축소\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 테스트\n",
    "n_embd = 64\n",
    "ffn = FeedForward(n_embd)\n",
    "x = torch.randn(1, 10, n_embd)  # (batch, seq_len, embedding)\n",
    "out = ffn(x)\n",
    "\n",
    "print(f\"입력: {x.shape}\")\n",
    "print(f\"출력: {out.shape}\")\n",
    "print(f\"\\n중간 차원: {n_embd} → {4*n_embd} → {n_embd}\")\n",
    "print(\"FFN은 각 위치에서 독립적으로 작동\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Connection\n",
    "\n",
    "**x + f(x)** 형태로 입력을 출력에 더함\n",
    "\n",
    "**왜 중요한가?**\n",
    "- Gradient vanishing 문제 해결\n",
    "- 깊은 네트워크 학습 가능\n",
    "- 정보 손실 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection 시각화\n",
    "x = torch.randn(1, 4, 8)\n",
    "transform = nn.Linear(8, 8)\n",
    "\n",
    "# Without residual\n",
    "y_no_residual = transform(x)\n",
    "\n",
    "# With residual\n",
    "y_with_residual = x + transform(x)\n",
    "\n",
    "# 비교\n",
    "print(\"Gradient flow:\")\n",
    "print(\"- Without residual: gradient는 transform을 통과해야 함\")\n",
    "print(\"- With residual: gradient가 직접 전달되는 경로 존재 (+)\")\n",
    "print(\"\\n정보 보존:\")\n",
    "print(f\"- 원본 정보 norm: {x.norm():.4f}\")\n",
    "print(f\"- Without residual norm: {y_no_residual.norm():.4f}\")\n",
    "print(f\"- With residual norm: {y_with_residual.norm():.4f} (원본 정보 보존)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 완전한 Transformer Block 구성\n",
    "\n",
    "구성:\n",
    "1. Multi-Head Attention + Residual + LayerNorm\n",
    "2. Feed-Forward + Residual + LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 간단한 Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Attention\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # Output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, n_embd, block_size, dropout) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전한 Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention + Residual + LayerNorm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # FFN + Residual + LayerNorm  \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 테스트\n",
    "block = TransformerBlock(n_embd=64, n_head=4, block_size=32)\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq, embd)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Transformer Block:\")\n",
    "print(f\"  입력: {x.shape}\")\n",
    "print(f\"  출력: {out.shape}\")\n",
    "print(f\"\\n구성:\")\n",
    "print(f\"  - 4개의 attention head\")\n",
    "print(f\"  - 각 head size: 16\")\n",
    "print(f\"  - FFN 확장: 64 → 256 → 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 여러 Block 쌓기 - Deep GPT\n",
    "\n",
    "GPT는 여러 Transformer Block을 쌓아서 만듦:\n",
    "- GPT-1: 12 blocks\n",
    "- GPT-2: 48 blocks\n",
    "- GPT-3: 96 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 GPT 모델\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        # Token + Position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# 작은 모델 생성\n",
    "model = MiniGPT(\n",
    "    vocab_size=100,\n",
    "    n_embd=64,\n",
    "    block_size=32,\n",
    "    n_head=4,\n",
    "    n_layer=4  # 4개 block\n",
    ")\n",
    "\n",
    "# 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"MiniGPT 모델:\")\n",
    "print(f\"  총 파라미터: {total_params:,}\")\n",
    "print(f\"  Transformer blocks: 4\")\n",
    "print(f\"  Attention heads per block: 4\")\n",
    "print(f\"  Embedding dimension: 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Block의 정보 흐름 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 block을 통과하며 표현이 어떻게 변하는지\n",
    "def visualize_block_progression(model, input_ids):\n",
    "    with torch.no_grad():\n",
    "        # Initial embeddings\n",
    "        B, T = input_ids.shape\n",
    "        tok_emb = model.token_embedding(input_ids)\n",
    "        pos_emb = model.position_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        representations = [x]\n",
    "        \n",
    "        # Track through each block\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            x = block(x)\n",
    "            representations.append(x)\n",
    "    \n",
    "    return representations\n",
    "\n",
    "# 테스트 입력\n",
    "input_ids = torch.randint(0, 100, (1, 8))  # 8 tokens\n",
    "reps = visualize_block_progression(model, input_ids)\n",
    "\n",
    "# 각 레이어의 representation norm\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Norm progression\n",
    "norms = [rep[0].norm(dim=-1).mean().item() for rep in reps]\n",
    "axes[0].plot(norms, 'o-')\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Average Norm')\n",
    "axes[0].set_title('Representation Norm through Layers')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance progression\n",
    "vars = [rep[0].var(dim=-1).mean().item() for rep in reps]\n",
    "axes[1].plot(vars, 'o-', color='orange')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Average Variance')\n",
    "axes[1].set_title('Representation Variance through Layers')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"관찰:\")\n",
    "print(\"- Residual connection 덕분에 norm이 안정적\")\n",
    "print(\"- Layer Norm이 variance를 조절\")\n",
    "print(\"- 깊은 네트워크에서도 gradient 전달 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-LN vs Post-LN Architecture\n",
    "\n",
    "GPT는 **Pre-LN** 사용: LayerNorm을 attention/FFN 전에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostLNBlock(nn.Module):\n",
    "    \"\"\"Post-LN: 원래 Transformer 논문 스타일\"\"\"\n",
    "    def forward(self, x):\n",
    "        # x → Attention → Add → LN\n",
    "        x = self.ln1(x + self.attention(x))\n",
    "        # x → FFN → Add → LN\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class PreLNBlock(nn.Module):\n",
    "    \"\"\"Pre-LN: GPT 스타일 (더 안정적)\"\"\"\n",
    "    def forward(self, x):\n",
    "        # x → LN → Attention → Add\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        # x → LN → FFN → Add\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "print(\"Pre-LN 장점 (GPT 사용):\")\n",
    "print(\"- 더 안정적인 학습\")\n",
    "print(\"- 깊은 모델에서 성능 향상\")\n",
    "print(\"- Gradient flow 개선\")\n",
    "print(\"\\nPost-LN 장점:\")\n",
    "print(\"- 원래 논문과 일치\")\n",
    "print(\"- 작은 모델에서는 비슷한 성능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 핵심 인사이트\n",
    "\n",
    "1. **Transformer Block = Attention + FFN**\n",
    "   - Attention: 토큰 간 관계 학습\n",
    "   - FFN: 각 토큰의 특징 변환\n",
    "\n",
    "2. **Residual Connection이 필수**\n",
    "   - 깊은 네트워크 학습 가능\n",
    "   - 정보 손실 방지\n",
    "\n",
    "3. **Layer Normalization 위치 중요**\n",
    "   - Pre-LN이 더 안정적 (GPT)\n",
    "   - 깊은 모델일수록 중요\n",
    "\n",
    "4. **Block 쌓기 = 표현력 증가**\n",
    "   - 각 block이 점진적 개선\n",
    "   - 더 복잡한 패턴 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 다음 단계\n",
    "\n",
    "이제 Transformer Block을 이해했으니:\n",
    "- **03_text_generation.ipynb**: 실제 텍스트 생성 과정\n",
    "- **04_training_loop.ipynb**: 모델 학습 방법"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}