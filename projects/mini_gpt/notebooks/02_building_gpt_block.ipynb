{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Building GPT Block - Transformerì˜ ê¸°ë³¸ ë‹¨ìœ„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” GPTì˜ ê¸°ë³¸ ë¹Œë”© ë¸”ë¡ì¸ **Transformer Block**ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "## êµ¬ì„± ìš”ì†Œ\n",
    "1. Multi-Head Attention\n",
    "2. Feed-Forward Network\n",
    "3. Layer Normalization\n",
    "4. Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer Normalization\n",
    "\n",
    "**ì™œ í•„ìš”í•œê°€?**\n",
    "- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ gradient ì•ˆì •í™”\n",
    "- í•™ìŠµ ì†ë„ í–¥ìƒ\n",
    "- Internal covariate shift ê°ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm ì´í•´í•˜ê¸°\n",
    "B, T, C = 2, 4, 8  # batch, sequence, embedding\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# ìˆ˜ë™ìœ¼ë¡œ Layer Norm ê³„ì‚°\n",
    "mean = x.mean(dim=-1, keepdim=True)  # ê° í† í°ì˜ í‰ê· \n",
    "std = x.std(dim=-1, keepdim=True)    # ê° í† í°ì˜ í‘œì¤€í¸ì°¨\n",
    "x_norm_manual = (x - mean) / (std + 1e-5)\n",
    "\n",
    "print(f\"ì›ë³¸ í†µê³„:\")\n",
    "print(f\"  í‰ê· : {x[0, 0].mean():.4f}\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {x[0, 0].std():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized í†µê³„:\")\n",
    "print(f\"  í‰ê· : {x_norm_manual[0, 0].mean():.4f} (â‰ˆ0)\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {x_norm_manual[0, 0].std():.4f} (â‰ˆ1)\")\n",
    "\n",
    "# PyTorch Layer Norm\n",
    "layer_norm = nn.LayerNorm(C)\n",
    "x_norm_pytorch = layer_norm(x)\n",
    "\n",
    "print(f\"\\nPyTorch LayerNormë„ ë™ì¼í•œ ê²°ê³¼:\")\n",
    "print(f\"  í‰ê· : {x_norm_pytorch[0, 0].mean():.4f}\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {x_norm_pytorch[0, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network (FFN)\n",
    "\n",
    "**êµ¬ì¡°**: Linear â†’ ReLU â†’ Linear\n",
    "\n",
    "**ì™œ í•„ìš”í•œê°€?**\n",
    "- Attentionì€ í† í° ê°„ ê´€ê³„ë§Œ í•™ìŠµ\n",
    "- FFNì€ ê° ìœ„ì¹˜ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ íŠ¹ì§• ë³€í™˜\n",
    "- ë³´í†µ 4ë°° í™•ì¥ í›„ ì¶•ì†Œ (bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # í™•ì¥\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # ì¶•ì†Œ\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "n_embd = 64\n",
    "ffn = FeedForward(n_embd)\n",
    "x = torch.randn(1, 10, n_embd)  # (batch, seq_len, embedding)\n",
    "out = ffn(x)\n",
    "\n",
    "print(f\"ì…ë ¥: {x.shape}\")\n",
    "print(f\"ì¶œë ¥: {out.shape}\")\n",
    "print(f\"\\nì¤‘ê°„ ì°¨ì›: {n_embd} â†’ {4*n_embd} â†’ {n_embd}\")\n",
    "print(\"FFNì€ ê° ìœ„ì¹˜ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Connection\n",
    "\n",
    "**x + f(x)** í˜•íƒœë¡œ ì…ë ¥ì„ ì¶œë ¥ì— ë”í•¨\n",
    "\n",
    "**ì™œ ì¤‘ìš”í•œê°€?**\n",
    "- Gradient vanishing ë¬¸ì œ í•´ê²°\n",
    "- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥\n",
    "- ì •ë³´ ì†ì‹¤ ë°©ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection ì‹œê°í™”\n",
    "x = torch.randn(1, 4, 8)\n",
    "transform = nn.Linear(8, 8)\n",
    "\n",
    "# Without residual\n",
    "y_no_residual = transform(x)\n",
    "\n",
    "# With residual\n",
    "y_with_residual = x + transform(x)\n",
    "\n",
    "# ë¹„êµ\n",
    "print(\"Gradient flow:\")\n",
    "print(\"- Without residual: gradientëŠ” transformì„ í†µê³¼í•´ì•¼ í•¨\")\n",
    "print(\"- With residual: gradientê°€ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ê²½ë¡œ ì¡´ì¬ (+)\")\n",
    "print(\"\\nì •ë³´ ë³´ì¡´:\")\n",
    "print(f\"- ì›ë³¸ ì •ë³´ norm: {x.norm():.4f}\")\n",
    "print(f\"- Without residual norm: {y_no_residual.norm():.4f}\")\n",
    "print(f\"- With residual norm: {y_with_residual.norm():.4f} (ì›ë³¸ ì •ë³´ ë³´ì¡´)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì™„ì „í•œ Transformer Block êµ¬ì„±\n",
    "\n",
    "êµ¬ì„±:\n",
    "1. Multi-Head Attention + Residual + LayerNorm\n",
    "2. Feed-Forward + Residual + LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¼ì € ê°„ë‹¨í•œ Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Attention\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # Output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, n_embd, block_size, dropout) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì™„ì „í•œ Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        \n",
    "        # Layer Norms\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention + Residual + LayerNorm\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # FFN + Residual + LayerNorm  \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "block = TransformerBlock(n_embd=64, n_head=4, block_size=32)\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq, embd)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Transformer Block:\")\n",
    "print(f\"  ì…ë ¥: {x.shape}\")\n",
    "print(f\"  ì¶œë ¥: {out.shape}\")\n",
    "print(f\"\\nêµ¬ì„±:\")\n",
    "print(f\"  - 4ê°œì˜ attention head\")\n",
    "print(f\"  - ê° head size: 16\")\n",
    "print(f\"  - FFN í™•ì¥: 64 â†’ 256 â†’ 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì—¬ëŸ¬ Block ìŒ“ê¸° - Deep GPT\n",
    "\n",
    "GPTëŠ” ì—¬ëŸ¬ Transformer Blockì„ ìŒ“ì•„ì„œ ë§Œë“¦:\n",
    "- GPT-1: 12 blocks\n",
    "- GPT-2: 48 blocks\n",
    "- GPT-3: 96 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ GPT ëª¨ë¸\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        # Token + Position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# ì‘ì€ ëª¨ë¸ ìƒì„±\n",
    "model = MiniGPT(\n",
    "    vocab_size=100,\n",
    "    n_embd=64,\n",
    "    block_size=32,\n",
    "    n_head=4,\n",
    "    n_layer=4  # 4ê°œ block\n",
    ")\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"MiniGPT ëª¨ë¸:\")\n",
    "print(f\"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print(f\"  Transformer blocks: 4\")\n",
    "print(f\"  Attention heads per block: 4\")\n",
    "print(f\"  Embedding dimension: 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Blockì˜ ì •ë³´ íë¦„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° blockì„ í†µê³¼í•˜ë©° í‘œí˜„ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€\n",
    "def visualize_block_progression(model, input_ids):\n",
    "    with torch.no_grad():\n",
    "        # Initial embeddings\n",
    "        B, T = input_ids.shape\n",
    "        tok_emb = model.token_embedding(input_ids)\n",
    "        pos_emb = model.position_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        representations = [x]\n",
    "        \n",
    "        # Track through each block\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            x = block(x)\n",
    "            representations.append(x)\n",
    "    \n",
    "    return representations\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "input_ids = torch.randint(0, 100, (1, 8))  # 8 tokens\n",
    "reps = visualize_block_progression(model, input_ids)\n",
    "\n",
    "# ê° ë ˆì´ì–´ì˜ representation norm\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Norm progression\n",
    "norms = [rep[0].norm(dim=-1).mean().item() for rep in reps]\n",
    "axes[0].plot(norms, 'o-')\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Average Norm')\n",
    "axes[0].set_title('Representation Norm through Layers')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance progression\n",
    "vars = [rep[0].var(dim=-1).mean().item() for rep in reps]\n",
    "axes[1].plot(vars, 'o-', color='orange')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Average Variance')\n",
    "axes[1].set_title('Representation Variance through Layers')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ê´€ì°°:\")\n",
    "print(\"- Residual connection ë•ë¶„ì— normì´ ì•ˆì •ì \")\n",
    "print(\"- Layer Normì´ varianceë¥¼ ì¡°ì ˆ\")\n",
    "print(\"- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ gradient ì „ë‹¬ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-LN vs Post-LN Architecture\n",
    "\n",
    "GPTëŠ” **Pre-LN** ì‚¬ìš©: LayerNormì„ attention/FFN ì „ì— ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostLNBlock(nn.Module):\n",
    "    \"\"\"Post-LN: ì›ë˜ Transformer ë…¼ë¬¸ ìŠ¤íƒ€ì¼\"\"\"\n",
    "    def forward(self, x):\n",
    "        # x â†’ Attention â†’ Add â†’ LN\n",
    "        x = self.ln1(x + self.attention(x))\n",
    "        # x â†’ FFN â†’ Add â†’ LN\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class PreLNBlock(nn.Module):\n",
    "    \"\"\"Pre-LN: GPT ìŠ¤íƒ€ì¼ (ë” ì•ˆì •ì )\"\"\"\n",
    "    def forward(self, x):\n",
    "        # x â†’ LN â†’ Attention â†’ Add\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        # x â†’ LN â†’ FFN â†’ Add\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "print(\"Pre-LN ì¥ì  (GPT ì‚¬ìš©):\")\n",
    "print(\"- ë” ì•ˆì •ì ì¸ í•™ìŠµ\")\n",
    "print(\"- ê¹Šì€ ëª¨ë¸ì—ì„œ ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(\"- Gradient flow ê°œì„ \")\n",
    "print(\"\\nPost-LN ì¥ì :\")\n",
    "print(\"- ì›ë˜ ë…¼ë¬¸ê³¼ ì¼ì¹˜\")\n",
    "print(\"- ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ë¹„ìŠ·í•œ ì„±ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "1. **Transformer Block = Attention + FFN**\n",
    "   - Attention: í† í° ê°„ ê´€ê³„ í•™ìŠµ\n",
    "   - FFN: ê° í† í°ì˜ íŠ¹ì§• ë³€í™˜\n",
    "\n",
    "2. **Residual Connectionì´ í•„ìˆ˜**\n",
    "   - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥\n",
    "   - ì •ë³´ ì†ì‹¤ ë°©ì§€\n",
    "\n",
    "3. **Layer Normalization ìœ„ì¹˜ ì¤‘ìš”**\n",
    "   - Pre-LNì´ ë” ì•ˆì •ì  (GPT)\n",
    "   - ê¹Šì€ ëª¨ë¸ì¼ìˆ˜ë¡ ì¤‘ìš”\n",
    "\n",
    "4. **Block ìŒ“ê¸° = í‘œí˜„ë ¥ ì¦ê°€**\n",
    "   - ê° blockì´ ì ì§„ì  ê°œì„ \n",
    "   - ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì´ì œ Transformer Blockì„ ì´í•´í–ˆìœ¼ë‹ˆ:\n",
    "- **03_text_generation.ipynb**: ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„± ê³¼ì •\n",
    "- **04_training_loop.ipynb**: ëª¨ë¸ í•™ìŠµ ë°©ë²•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}