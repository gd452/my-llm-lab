"""
miniGPT ë¶„ì„ ë„êµ¬ - ëª¨ë¸ì´ ë¬´ì—‡ì„ í•™ìŠµí–ˆëŠ”ì§€ ì‹œê°í™”
Attention íŒ¨í„´ì„ ë³´ì—¬ì£¼ê³  ëª¨ë¸ì˜ ë™ì‘ì„ ì´í•´
"""

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from model import GPT

def visualize_attention(model, text, layer_idx=0, head_idx=0):
    """íŠ¹ì • ë ˆì´ì–´/í—¤ë“œì˜ attention íŒ¨í„´ ì‹œê°í™”"""
    
    # Tokenizer ì¤€ë¹„
    with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:
        full_text = f.read()
    chars = sorted(list(set(full_text)))
    stoi = {ch: i for i, ch in enumerate(chars)}
    encode = lambda s: [stoi[c] for c in s if c in stoi]
    
    # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    tokens = encode(text)
    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)
    
    # Forward pass with hooks to capture attention
    model.eval()
    with torch.no_grad():
        # ì„ë² ë”©
        tok_emb = model.token_embedding_table(x)
        pos_emb = model.position_embedding_table(torch.arange(len(tokens)))
        x_emb = tok_emb + pos_emb
        
        # íŠ¹ì • ë¸”ë¡ì˜ attention ê°€ì ¸ì˜¤ê¸°
        block = model.blocks[layer_idx]
        x_norm = block.ln1(x_emb)
        
        # íŠ¹ì • headì˜ attention ê³„ì‚°
        head = block.sa.heads[head_idx]
        B, T, C = x_norm.shape
        k = head.key(x_norm)
        q = head.query(x_norm)
        
        # Attention scores
        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5
        wei = wei.masked_fill(head.tril[:T, :T] == 0, float('-inf'))
        attention = F.softmax(wei, dim=-1).squeeze().numpy()
    
    # ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(attention, cmap='Blues', aspect='auto')
    
    # ì¶• ë ˆì´ë¸”
    ax.set_xticks(range(len(text)))
    ax.set_yticks(range(len(text)))
    ax.set_xticklabels(list(text), rotation=45, ha='right')
    ax.set_yticklabels(list(text))
    
    ax.set_xlabel('Keys (being attended to)')
    ax.set_ylabel('Queries (attending from)')
    ax.set_title(f'Attention Pattern - Layer {layer_idx}, Head {head_idx}')
    
    # Colorbar
    plt.colorbar(im, ax=ax)
    plt.tight_layout()
    plt.savefig(f'outputs/attention_L{layer_idx}_H{head_idx}.png')
    plt.show()
    
    print(f"Attention ì‹œê°í™” ì €ì¥: outputs/attention_L{layer_idx}_H{head_idx}.png")

def analyze_embeddings(model):
    """í† í° ì„ë² ë”© ë¶„ì„ - ìœ ì‚¬í•œ ë¬¸ìë“¤ì´ ê°€ê¹Œìš´ ë²¡í„°ë¥¼ ê°€ì§€ëŠ”ì§€"""
    
    # ë¬¸ì ë¦¬ìŠ¤íŠ¸
    with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    chars = sorted(list(set(text)))
    
    # ì„ë² ë”© ì¶”ì¶œ
    embeddings = model.token_embedding_table.weight.detach().numpy()
    
    # ì¼ë¶€ ë¬¸ìë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    sample_chars = ['a', 'e', 'i', 'o', 'u',  # ëª¨ìŒ
                   't', 'h', 's', 'n', 'r',  # ìì£¼ ì“°ì´ëŠ” ììŒ
                   '.', ',', '!', '?',       # êµ¬ë‘ì 
                   ' ', '\n']                # ê³µë°±
    
    print("\në¬¸ì ì„ë² ë”© ìœ ì‚¬ë„ ë¶„ì„:")
    print("-" * 50)
    
    for i, char1 in enumerate(sample_chars):
        if char1 not in chars:
            continue
        idx1 = chars.index(char1)
        similarities = []
        
        for char2 in sample_chars:
            if char2 not in chars or char1 == char2:
                continue
            idx2 = chars.index(char2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            vec1 = embeddings[idx1]
            vec2 = embeddings[idx2]
            sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            similarities.append((char2, sim))
        
        # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ìë“¤
        similarities.sort(key=lambda x: x[1], reverse=True)
        top3 = similarities[:3]
        
        char_repr = repr(char1)
        print(f"{char_repr:5} ì™€ ê°€ì¥ ìœ ì‚¬í•œ: {', '.join([f'{repr(c)}({s:.2f})' for c, s in top3])}")

def analyze_generation_probability(model, prompt="To be or not to be"):
    """ê° ìŠ¤í…ì—ì„œì˜ ìƒì„± í™•ë¥  ë¶„í¬ ë¶„ì„"""
    
    # Tokenizer
    with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    chars = sorted(list(set(text)))
    stoi = {ch: i for i, ch in enumerate(chars)}
    itos = {i: ch for i, ch in enumerate(chars)}
    encode = lambda s: [stoi[c] for c in s if c in stoi]
    
    # í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
    tokens = encode(prompt)
    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)
    
    model.eval()
    with torch.no_grad():
        logits, _ = model(x)
        probs = F.softmax(logits[0, -1, :], dim=-1)
    
    # Top 10 í™•ë¥ 
    top_probs, top_indices = torch.topk(probs, 10)
    
    print(f"\n'{prompt}' ë‹¤ìŒì— ì˜¬ í™•ë¥ ì´ ë†’ì€ ë¬¸ì:")
    print("-" * 50)
    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
        char = itos[idx.item()]
        char_repr = repr(char)
        bar = 'â–ˆ' * int(prob.item() * 50)
        print(f"{i+1:2}. {char_repr:5} {prob.item():.3f} {bar}")

def main():
    """ë¶„ì„ ì‹¤í–‰"""
    import os
    
    # ëª¨ë¸ ì²´í¬
    if not os.path.exists('outputs/model.pt'):
        print("ì—ëŸ¬: í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'python train.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # outputs ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('outputs', exist_ok=True)
    
    # ëª¨ë¸ ë¡œë“œ
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ëª¨ë¸ ì„¤ì • (train.pyì™€ ë™ì¼)
    vocab_size = 65
    n_embd = 384
    block_size = 256
    n_head = 6
    n_layer = 6
    dropout = 0.0
    
    model = GPT(vocab_size, n_embd, block_size, n_head, n_layer, dropout)
    model.load_state_dict(torch.load('outputs/model.pt', map_location=device))
    model.eval()
    
    # 1. Attention ì‹œê°í™”
    print("\n=== Attention íŒ¨í„´ ì‹œê°í™” ===")
    sample_text = "To be or not"
    print(f"ë¶„ì„í•  í…ìŠ¤íŠ¸: '{sample_text}'")
    visualize_attention(model, sample_text, layer_idx=0, head_idx=0)
    
    # 2. ì„ë² ë”© ë¶„ì„
    print("\n=== ë¬¸ì ì„ë² ë”© ë¶„ì„ ===")
    analyze_embeddings(model)
    
    # 3. ìƒì„± í™•ë¥  ë¶„ì„
    print("\n=== ìƒì„± í™•ë¥  ë¶„ì„ ===")
    analyze_generation_probability(model, "Romeo")
    analyze_generation_probability(model, "KING")
    
    print("\në¶„ì„ ì™„ë£Œ!")
    print("ğŸ’¡ Tip: layer_idxì™€ head_idxë¥¼ ë°”ê¿”ê°€ë©° ë‹¤ë¥¸ attention íŒ¨í„´ë„ í™•ì¸í•´ë³´ì„¸ìš”.")

if __name__ == '__main__':
    main()