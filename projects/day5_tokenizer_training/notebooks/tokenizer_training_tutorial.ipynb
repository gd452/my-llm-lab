{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: Tokenizer and Training Tutorial\n",
    "\n",
    "이 노트북에서는 토크나이저 구현, 학습 루프 구성, 그리고 텍스트 생성을 실습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), '../../..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 우리가 구현한 모듈들\n",
    "from core.tokenizer import CharacterTokenizer, SimpleBPETokenizer, DataLoader\n",
    "from core.training import (\n",
    "    cross_entropy_loss, perplexity, \n",
    "    SGD, Adam, TextGenerator,\n",
    "    gradient_clipping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Character-level Tokenizer\n",
    "\n",
    "가장 간단한 토크나이저부터 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "A journey of a thousand miles begins with a single step.\n",
    "To be or not to be, that is the question.\n",
    "\"\"\"\n",
    "\n",
    "# Character tokenizer 생성 및 학습\n",
    "char_tokenizer = CharacterTokenizer()\n",
    "char_tokenizer.fit(sample_text)\n",
    "\n",
    "print(f\"Vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "print(f\"First 20 characters in vocab: {list(char_tokenizer.char_to_id.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding 테스트\n",
    "test_sentence = \"The quick fox\"\n",
    "encoded = char_tokenizer.encode(test_sentence)\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "# Decoding 테스트\n",
    "decoded = char_tokenizer.decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Special tokens 포함\n",
    "encoded_special = char_tokenizer.encode(test_sentence, add_special_tokens=True)\n",
    "print(f\"\\nWith special tokens: {encoded_special}\")\n",
    "print(f\"Decoded: {char_tokenizer.decode(encoded_special, skip_special_tokens=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: BPE Tokenizer\n",
    "\n",
    "Byte Pair Encoding을 구현하여 더 효율적인 토큰화를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE Tokenizer 생성 및 학습\n",
    "bpe_tokenizer = SimpleBPETokenizer(vocab_size=256)\n",
    "bpe_tokenizer.fit(sample_text, num_merges=50)\n",
    "\n",
    "# 학습된 merges 확인\n",
    "print(\"First 10 merges:\")\n",
    "for i, (a, b) in enumerate(bpe_tokenizer.merges[:10]):\n",
    "    print(f\"{i+1}. '{a}' + '{b}' -> '{a}{b}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE vs Character tokenization 비교\n",
    "test_text = \"The quick brown fox\"\n",
    "\n",
    "char_encoded = char_tokenizer.encode(test_text)\n",
    "bpe_encoded = bpe_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Character tokens: {len(char_encoded)} tokens\")\n",
    "print(f\"BPE tokens: {len(bpe_encoded)} tokens\")\n",
    "print(f\"\\nCompression ratio: {len(char_encoded) / len(bpe_encoded):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: DataLoader 구현\n",
    "\n",
    "학습을 위한 배치 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    text=sample_text,\n",
    "    tokenizer=char_tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    seq_length=seq_length\n",
    ")\n",
    "\n",
    "print(f\"Number of tokens: {dataloader.num_tokens}\")\n",
    "print(f\"Number of batches: {dataloader.num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 확인\n",
    "inputs, targets = dataloader.get_batch(0)\n",
    "\n",
    "print(\"First batch:\")\n",
    "print(f\"Input shape: {np.array(inputs).shape}\")\n",
    "print(f\"Target shape: {np.array(targets).shape}\")\n",
    "\n",
    "print(\"\\nFirst sequence:\")\n",
    "print(f\"Input: {char_tokenizer.decode(inputs[0])}\")\n",
    "print(f\"Target: {char_tokenizer.decode(targets[0])}\")\n",
    "print(\"\\nNote: Target is input shifted by 1 position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loss Functions\n",
    "\n",
    "Cross-entropy loss와 perplexity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy predictions for testing\n",
    "vocab_size = char_tokenizer.vocab_size\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "# Random logits\n",
    "logits = np.random.randn(batch_size, seq_length, vocab_size)\n",
    "\n",
    "# Random targets\n",
    "targets = np.random.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Calculate loss\n",
    "loss = cross_entropy_loss(logits, targets)\n",
    "ppl = perplexity(loss)\n",
    "\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "print(f\"\\nExpected random loss: {np.log(vocab_size):.4f}\")\n",
    "print(f\"Expected random perplexity: {vocab_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Optimizers\n",
    "\n",
    "SGD와 Adam optimizer 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple optimization example\n",
    "class SimpleParam:\n",
    "    def __init__(self, value):\n",
    "        self.data = np.array(value, dtype=float)\n",
    "        self.grad = None\n",
    "\n",
    "# Create parameters\n",
    "param_sgd = SimpleParam([1.0, 2.0, 3.0])\n",
    "param_adam = SimpleParam([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create optimizers\n",
    "sgd = SGD([param_sgd], learning_rate=0.1)\n",
    "adam = Adam([param_adam], learning_rate=0.1)\n",
    "\n",
    "# Simulate gradients and updates\n",
    "sgd_history = [param_sgd.data.copy()]\n",
    "adam_history = [param_adam.data.copy()]\n",
    "\n",
    "for step in range(10):\n",
    "    # Simulate gradient (toward zero)\n",
    "    param_sgd.grad = param_sgd.data\n",
    "    param_adam.grad = param_adam.data\n",
    "    \n",
    "    # Update\n",
    "    sgd.step()\n",
    "    adam.step()\n",
    "    \n",
    "    sgd_history.append(param_sgd.data.copy())\n",
    "    adam_history.append(param_adam.data.copy())\n",
    "    \n",
    "    sgd.zero_grad()\n",
    "    adam.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization\n",
    "sgd_history = np.array(sgd_history)\n",
    "adam_history = np.array(adam_history)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sgd_history[:, 0], label='SGD')\n",
    "plt.plot(adam_history[:, 0], label='Adam')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.title('Optimization Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.linalg.norm(sgd_history, axis=1), label='SGD')\n",
    "plt.plot(np.linalg.norm(adam_history, axis=1), label='Adam')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('L2 Norm')\n",
    "plt.title('Parameter Norm Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Text Generation Strategies\n",
    "\n",
    "다양한 텍스트 생성 전략 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model for demonstration\n",
    "class DummyModel:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Return random logits for demo\n",
    "        batch_size = 1 if len(np.array(input_ids).shape) == 1 else input_ids.shape[0]\n",
    "        seq_len = len(input_ids) if len(np.array(input_ids).shape) == 1 else input_ids.shape[1]\n",
    "        \n",
    "        # Make some tokens more likely for interesting generation\n",
    "        logits = np.random.randn(batch_size, seq_len, self.vocab_size)\n",
    "        # Boost some specific tokens\n",
    "        if 'T' in char_tokenizer.char_to_id:\n",
    "            logits[:, :, char_tokenizer.char_to_id['T']] += 1\n",
    "        if 'h' in char_tokenizer.char_to_id:\n",
    "            logits[:, :, char_tokenizer.char_to_id['h']] += 0.5\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model and generator\n",
    "model = DummyModel(char_tokenizer.vocab_size)\n",
    "generator = TextGenerator(model, char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different generation strategies\n",
    "prompt = \"The \"\n",
    "\n",
    "print(\"Different generation strategies:\\n\")\n",
    "\n",
    "# Greedy (temperature=0)\n",
    "generated = generator.generate(prompt, max_length=20, temperature=0)\n",
    "print(f\"Greedy: {generated}\")\n",
    "\n",
    "# Low temperature\n",
    "generated = generator.generate(prompt, max_length=20, temperature=0.5)\n",
    "print(f\"Temp=0.5: {generated}\")\n",
    "\n",
    "# Normal temperature\n",
    "generated = generator.generate(prompt, max_length=20, temperature=1.0)\n",
    "print(f\"Temp=1.0: {generated}\")\n",
    "\n",
    "# High temperature\n",
    "generated = generator.generate(prompt, max_length=20, temperature=2.0)\n",
    "print(f\"Temp=2.0: {generated}\")\n",
    "\n",
    "# Top-k sampling\n",
    "generated = generator.generate(prompt, max_length=20, top_k=5)\n",
    "print(f\"Top-k=5: {generated}\")\n",
    "\n",
    "# Top-p sampling\n",
    "generated = generator.generate(prompt, max_length=20, top_p=0.9)\n",
    "print(f\"Top-p=0.9: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Mini Language Model Training\n",
    "\n",
    "작은 언어 모델을 실제로 학습시켜 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini LM for character-level modeling\n",
    "class MiniLM:\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Simple parameters\n",
    "        self.embedding = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.output = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # Simple forward pass\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = np.array(input_ids)\n",
    "        \n",
    "        batch_size = 1 if len(input_ids.shape) == 1 else input_ids.shape[0]\n",
    "        seq_len = input_ids.shape[-1]\n",
    "        \n",
    "        # Embedding lookup\n",
    "        embedded = np.zeros((batch_size, seq_len, self.hidden_size))\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                if len(input_ids.shape) == 1:\n",
    "                    embedded[b, t] = self.embedding[input_ids[t]]\n",
    "                else:\n",
    "                    embedded[b, t] = self.embedding[input_ids[b, t]]\n",
    "        \n",
    "        # Output projection\n",
    "        logits = embedded @ self.output\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.embedding, self.output]\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        # Simplified - just set dummy gradients\n",
    "        self.embedding.grad = np.random.randn(*self.embedding.shape) * 0.01\n",
    "        self.output.grad = np.random.randn(*self.output.shape) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mini LM\n",
    "mini_lm = MiniLM(char_tokenizer.vocab_size, hidden_size=32)\n",
    "optimizer = Adam([mini_lm.embedding, mini_lm.output], learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "print(\"Training mini language model...\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        # Forward pass\n",
    "        inputs = np.array(inputs)\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        logits = mini_lm.forward(inputs)\n",
    "        loss = cross_entropy_loss(logits, targets)\n",
    "        \n",
    "        # Backward pass (simplified)\n",
    "        mini_lm.backward(loss)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        gradient_clipping([mini_lm.embedding, mini_lm.output], max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Perplexity = {perplexity(avg_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([perplexity(l) for l in losses])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Training Perplexity')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with trained model\n",
    "trained_generator = TextGenerator(mini_lm, char_tokenizer)\n",
    "\n",
    "prompts = [\"The \", \"A \", \"To \"]\n",
    "\n",
    "print(\"Text generation with trained model:\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = trained_generator.generate(\n",
    "        prompt, \n",
    "        max_length=30, \n",
    "        temperature=0.8,\n",
    "        top_k=10\n",
    "    )\n",
    "    print(f\"Prompt: '{prompt}' -> {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "이 튜토리얼에서 다룬 내용:\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Character-level tokenizer\n",
    "   - BPE tokenizer\n",
    "   - Vocabulary 관리\n",
    "\n",
    "2. **Training Components**\n",
    "   - DataLoader\n",
    "   - Loss functions\n",
    "   - Optimizers (SGD, Adam)\n",
    "\n",
    "3. **Text Generation**\n",
    "   - Temperature sampling\n",
    "   - Top-k sampling\n",
    "   - Top-p sampling\n",
    "\n",
    "4. **Mini LM Training**\n",
    "   - Training loop\n",
    "   - Gradient clipping\n",
    "   - Loss monitoring\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- 더 큰 데이터셋으로 학습\n",
    "- 실제 Transformer 모델과 연결\n",
    "- Advanced tokenization (SentencePiece)\n",
    "- Pre-training strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}