{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Autograd Tutorial - 스칼라 자동미분 구현하기\n",
    "\n",
    "이 노트북에서는 PyTorch의 autograd와 유사한 자동미분(automatic differentiation) 엔진을 처음부터 구현해봅니다.\n",
    "\n",
    "## 목표\n",
    "- 연산 그래프(computation graph) 이해하기\n",
    "- 역전파(backpropagation) 알고리즘 구현하기\n",
    "- Chain rule을 통한 gradient 계산 이해하기\n",
    "\n",
    "## 주요 개념\n",
    "\n",
    "### 1. 자동미분(Automatic Differentiation)이란?\n",
    "- 컴퓨터가 함수의 미분을 자동으로 계산하는 기법\n",
    "- 딥러닝에서 gradient를 효율적으로 계산하는 핵심 기술\n",
    "\n",
    "### 2. 연산 그래프(Computation Graph)\n",
    "- 연산들의 순서와 의존관계를 나타내는 DAG(Directed Acyclic Graph)\n",
    "- 각 노드는 값(Value)을 가지고, 엣지는 연산을 나타냄\n",
    "\n",
    "### 3. Chain Rule\n",
    "- 합성함수의 미분법칙: $\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$\n",
    "- 역전파의 수학적 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Value 클래스 기본 구조\n",
    "\n",
    "먼저 자동미분을 위한 기본 데이터 구조를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Value:\n",
    "    \"\"\"스칼라 값과 그래디언트를 저장하는 클래스\n",
    "    \n",
    "    Attributes:\n",
    "        data: 실제 스칼라 값\n",
    "        grad: 이 노드에 대한 출력의 gradient\n",
    "        _prev: 이 노드를 만든 입력 노드들의 집합\n",
    "        _backward: 역전파 시 실행할 함수\n",
    "    \"\"\"\n",
    "    data: float\n",
    "    _prev: set[Value] = field(default_factory=set, repr=False)\n",
    "    _backward: Callable[[], None] = field(default=lambda: None, repr=False)\n",
    "    grad: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not isinstance(self.data, (int, float)):\n",
    "            raise TypeError(\"Value.data must be a number\")\n",
    "        self.data = float(self.data)\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return f\"Value(data={self.data:.6f}, grad={self.grad:.6f})\"\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self is other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "def append_item(x, items=[]):  # 가변 기본값\n",
    "    items.append(x)\n",
    "    return items\n",
    "\n",
    "def append_item2(x, items=None):\n",
    "    if items is None:\n",
    "        items = []\n",
    "    items.append(x)\n",
    "    return items\n",
    "\n",
    "print(append_item(1))  # [1]\n",
    "print(append_item(2))  # [1, 2]  ← 이전 호출과 공유됨\n",
    "\n",
    "print(append_item2(1))  # [1]\n",
    "print(append_item2(2))  # [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 덧셈 연산 구현\n",
    "\n",
    "덧셈의 미분 규칙:\n",
    "- $f(a, b) = a + b$\n",
    "- $\\frac{\\partial f}{\\partial a} = 1$\n",
    "- $\\frac{\\partial f}{\\partial b} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_operation(self, other: float | Value) -> Value:\n",
    "    \"\"\"덧셈 연산 구현\n",
    "    \n",
    "    덧셈의 로컬 gradient는 항상 1입니다.\n",
    "    out = self + other 일 때:\n",
    "    - d(out)/d(self) = 1\n",
    "    - d(out)/d(other) = 1\n",
    "    \"\"\"\n",
    "    # other가 스칼라면 Value로 변환\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    \n",
    "    # 새로운 Value 노드 생성 (forward pass)\n",
    "    out = Value(self.data + other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # Chain rule 적용: gradient 누적\n",
    "        # out.grad는 상위 노드에서 전파된 gradient\n",
    "        self.grad += out.grad * 1.0  # 덧셈의 로컬 gradient는 1\n",
    "        other.grad += out.grad * 1.0\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "# Value 클래스에 메서드 추가\n",
    "Value.__add__ = add_operation\n",
    "Value.__radd__ = lambda self, other: self.__add__(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(Value, \"__add__\")  # True가 나와야 정상\n",
    "\n",
    "a = Value(3.0)\n",
    "\n",
    "# 왼쪽이 숫자라서, int.__add__가 Value를 모르면 a.__radd__(2)가 호출됨\n",
    "c1 = 2 + a   # __radd__ 경유 → __add__ 재사용\n",
    "c2 = a + 2   # __add__\n",
    "\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 덧셈 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 덧셈 예제\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a + b\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = a + b = {c}\")\n",
    "print(f\"c의 부모 노드들: {c._prev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 곱셈 연산 구현\n",
    "\n",
    "곱셈의 미분 규칙 (Product Rule):\n",
    "- $f(a, b) = a \\times b$\n",
    "- $\\frac{\\partial f}{\\partial a} = b$\n",
    "- $\\frac{\\partial f}{\\partial b} = a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_operation(self, other: float | Value) -> Value:\n",
    "    \"\"\"곱셈 연산 구현\n",
    "    \n",
    "    곱셈의 로컬 gradient는 상대방의 값입니다.\n",
    "    out = self * other 일 때:\n",
    "    - d(out)/d(self) = other.data\n",
    "    - d(out)/d(other) = self.data\n",
    "    \"\"\"\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # Product rule 적용\n",
    "        print(\"backward is called, self.grad = \", self.grad, \"other.grad = \", other.grad, \"out.grad = \", out.grad)\n",
    "        self.grad += other.data * out.grad  # d(a*b)/da = b\n",
    "        other.grad += self.data * out.grad  # d(a*b)/db = a\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.__mul__ = mul_operation\n",
    "Value.__rmul__ = lambda self, other: self.__mul__(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 곱셈 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 예제\n",
    "x = Value(3.0)\n",
    "y = Value(4.0)\n",
    "z = x * y\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"z = x * y = {z}\")\n",
    "\n",
    "# 스칼라와의 곱셈도 가능\n",
    "w = z * 2\n",
    "print(f\"w = z * 2 = {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 활성화 함수 - tanh 구현\n",
    "\n",
    "tanh 함수의 미분:\n",
    "- $f(x) = \\tanh(x)$\n",
    "- $\\frac{df}{dx} = 1 - \\tanh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_operation(self) -> Value:\n",
    "    \"\"\"tanh 활성화 함수 구현\n",
    "    \n",
    "    tanh의 미분: dtanh/dx = 1 - tanh(x)^2\n",
    "    \"\"\"\n",
    "    t = math.tanh(self.data)\n",
    "    out = Value(t, {self})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # tanh의 로컬 gradient\n",
    "        self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.tanh = tanh_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ReLU 활성화 함수 (선택사항)\n",
    "\n",
    "ReLU 함수의 미분:\n",
    "- $f(x) = \\max(0, x)$\n",
    "- $\\frac{df}{dx} = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_operation(self) -> Value:\n",
    "    \"\"\"ReLU 활성화 함수 구현\n",
    "    \n",
    "    ReLU의 미분: x > 0 일 때 1, 아니면 0\n",
    "    \"\"\"\n",
    "    out = Value(self.data if self.data > 0 else 0.0, {self})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # ReLU의 로컬 gradient\n",
    "        self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.relu = relu_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 역전파(Backpropagation) 구현\n",
    "\n",
    "역전파 알고리즘:\n",
    "1. 연산 그래프를 위상정렬(topological sort)\n",
    "2. 출력 노드의 gradient를 1로 설정\n",
    "3. 역순으로 각 노드의 gradient 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_operation(self) -> None:\n",
    "    \"\"\"역전파 알고리즘 구현\n",
    "    \n",
    "    1. DFS로 그래프를 위상정렬\n",
    "    2. 출력(self)의 gradient를 1로 설정\n",
    "    3. 역순으로 각 노드의 _backward() 실행\n",
    "    \"\"\"\n",
    "    # 위상정렬을 위한 리스트와 방문 집합\n",
    "    topo: list[Value] = []\n",
    "    visited: set[Value] = set()\n",
    "\n",
    "    def build_topo(v: Value) -> None:\n",
    "        \"\"\"DFS로 위상정렬 구축\"\"\"\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            # 자식 노드들을 먼저 방문\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            # 자식들 처리 후 현재 노드 추가\n",
    "            topo.append(v)\n",
    "\n",
    "    # 그래프 구축\n",
    "    build_topo(self)\n",
    "    \n",
    "    # 출력의 gradient는 1\n",
    "    self.grad = 1.0\n",
    "    \n",
    "    # 역순으로 gradient 전파\n",
    "    for v in reversed(topo):\n",
    "        v._backward()\n",
    "\n",
    "Value.backward = backward_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 전체 구현 통합\n",
    "\n",
    "이제 모든 구성요소를 하나의 클래스로 통합해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Value 클래스 구현\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Set\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Value:\n",
    "    data: float\n",
    "    _prev: Set[Value] = field(default_factory=set, repr=False)\n",
    "    _backward: Callable[[], None] = field(default=lambda: None, repr=False)\n",
    "    grad: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not isinstance(self.data, (int, float)):\n",
    "            raise TypeError(\"Value.data must be a number\")\n",
    "        self.data = float(self.data)\n",
    "\n",
    "    def __add__(self, other: float | Value) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, {self, other})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other: float | Value) -> Value:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other: float | Value) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, {self, other})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other: float | Value) -> Value:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def tanh(self) -> Value:\n",
    "        t = math.tanh(self.data)\n",
    "        out = Value(t, {self})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Value:\n",
    "        out = Value(self.data if self.data > 0 else 0.0, {self})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        topo: list[Value] = []\n",
    "        visited: set[Value] = set()\n",
    "\n",
    "        def build(v: Value) -> None:\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self is other\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data:.6f}, grad={self.grad:.6f})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: 실제 예제 - 복잡한 함수의 gradient 계산\n",
    "\n",
    "이제 구현한 자동미분 엔진을 테스트해봅시다.\n",
    "\n",
    "테스트 함수: $f(a, b) = (a \\times b + a) \\times \\tanh(b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 함수 예제\n",
    "def test_complex_function():\n",
    "    # 입력값 정의\n",
    "    a = Value(1.3)\n",
    "    b = Value(-0.7)\n",
    "    \n",
    "    # Forward pass: f(a,b) = (a*b + a) * tanh(b)\n",
    "    c = a * b  # a * b\n",
    "    d = c + a  # a * b + a\n",
    "    e = b.tanh()  # tanh(b)\n",
    "    out = d * e  # (a * b + a) * tanh(b)\n",
    "    \n",
    "    print(\"=== Forward Pass ===\")\n",
    "    print(f\"a = {a.data:.6f}\")\n",
    "    print(f\"b = {b.data:.6f}\")\n",
    "    print(f\"c = a * b = {c.data:.6f}\")\n",
    "    print(f\"d = c + a = {d.data:.6f}\")\n",
    "    print(f\"e = tanh(b) = {e.data:.6f}\")\n",
    "    print(f\"out = d * e = {out.data:.6f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    out.backward()\n",
    "    \n",
    "    print(\"\\n=== Backward Pass ===\")\n",
    "    print(f\"∂out/∂a = {a.grad:.6f}\")\n",
    "    print(f\"∂out/∂b = {b.grad:.6f}\")\n",
    "    \n",
    "    return out, a.grad, b.grad\n",
    "\n",
    "result = test_complex_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: 수치 미분과 비교 검증\n",
    "\n",
    "우리의 자동미분이 올바른지 수치 미분(numerical differentiation)과 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def numerical_gradient(f, a0: float, b0: float, eps: float = 1e-6):\n",
    "    \"\"\"중앙 차분법을 사용한 수치 미분\n",
    "    \n",
    "    f'(x) ≈ [f(x+ε) - f(x-ε)] / 2ε\n",
    "    \"\"\"\n",
    "    # a에 대한 편미분\n",
    "    grad_a = (f(a0 + eps, b0) - f(a0 - eps, b0)) / (2 * eps)\n",
    "    \n",
    "    # b에 대한 편미분\n",
    "    grad_b = (f(a0, b0 + eps) - f(a0, b0 - eps)) / (2 * eps)\n",
    "    \n",
    "    return grad_a, grad_b\n",
    "\n",
    "def f_scalar(a0: float, b0: float) -> float:\n",
    "    \"\"\"테스트 함수의 스칼라 버전\"\"\"\n",
    "    return (a0 * b0 + a0) * math.tanh(b0)\n",
    "\n",
    "def verify_gradients():\n",
    "    \"\"\"자동미분과 수치미분 비교\"\"\"\n",
    "    test_cases = [\n",
    "        (1.3, -0.7),\n",
    "        (0.5, 0.5),\n",
    "        (-1.2, 2.0)\n",
    "    ]\n",
    "    \n",
    "    for a0, b0 in test_cases:\n",
    "        # 자동미분\n",
    "        a, b = Value(a0), Value(b0)\n",
    "        out = (a * b + a) * b.tanh()\n",
    "        out.backward()\n",
    "        \n",
    "        # 수치미분\n",
    "        grad_a_num, grad_b_num = numerical_gradient(f_scalar, a0, b0)\n",
    "        \n",
    "        # 상대 오차 계산\n",
    "        def relative_error(x, y):\n",
    "            denom = max(1.0, abs(x), abs(y))\n",
    "            return abs(x - y) / denom\n",
    "        \n",
    "        err_a = relative_error(a.grad, grad_a_num)\n",
    "        err_b = relative_error(b.grad, grad_b_num)\n",
    "        \n",
    "        print(f\"\\n테스트 케이스: a={a0}, b={b0}\")\n",
    "        print(f\"  자동미분: ∂f/∂a = {a.grad:.8f}, ∂f/∂b = {b.grad:.8f}\")\n",
    "        print(f\"  수치미분: ∂f/∂a = {grad_a_num:.8f}, ∂f/∂b = {grad_b_num:.8f}\")\n",
    "        print(f\"  상대오차: a = {err_a:.2e}, b = {err_b:.2e}\")\n",
    "        print(f\"  통과: {err_a < 1e-4 and err_b < 1e-4}\")\n",
    "\n",
    "verify_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: 시각화 - 연산 그래프 그리기 (선택사항)\n",
    "\n",
    "연산 그래프를 시각화해서 자동미분이 어떻게 동작하는지 더 잘 이해해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_graph(root):\n",
    "    \"\"\"연산 그래프 추적\"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "    nodes, edges = list(), set()\n",
    "    \n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.append(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    \n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_graph(root):\n",
    "    \"\"\"간단한 텍스트 기반 그래프 표현\"\"\"\n",
    "    nodes, edges = trace_graph(root)\n",
    "    \n",
    "    print(\"\\n=== 연산 그래프 구조 ===\")\n",
    "    print(f\"노드 개수: {len(nodes)}\")\n",
    "    print(f\"엣지 개수: {len(edges)}\")\n",
    "    \n",
    "    print(\"\\n노드 정보:\")\n",
    "    for i, node in enumerate(nodes):\n",
    "        print(f\"  Node {i}: data={node.data:.4f}, grad={node.grad:.4f}\")\n",
    "    \n",
    "    print(\"\\n연결 정보:\")\n",
    "    for src, dst in edges:\n",
    "        print(f\"  {src.data:.4f} -> {dst.data:.4f}\")\n",
    "\n",
    "# 예제 그래프 생성 및 시각화\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b\n",
    "d = c + a\n",
    "e = d.tanh()\n",
    "\n",
    "e.backward()\n",
    "draw_graph(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습 문제\n",
    "\n",
    "이제 자동미분 엔진이 완성되었습니다! 다음 연습문제를 풀어보세요:\n",
    "\n",
    "### 문제 1: 새로운 연산 추가하기\n",
    "빼기(`__sub__`)와 나누기(`__truediv__`) 연산을 구현해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습: 빼기와 나누기 구현\n",
    "def sub_operation(self, other):\n",
    "    # TODO: 빼기 구현\n",
    "    # 힌트: a - b = a + (-b)\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data - other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        self.grad += out.grad * 1.0\n",
    "        other.grad += out.grad * -1.0\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def div_operation(self, other):\n",
    "    # TODO: 나누기 구현\n",
    "    # 힌트: a / b의 미분\n",
    "    # d(a/b)/da = 1/b\n",
    "    # d(a/b)/db = -a/b^2\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data / other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        self.grad += out.grad * 1.0 / other.data\n",
    "        other.grad += out.grad * -self.data / other.data**2\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "Value.__sub__ = sub_operation\n",
    "Value.__rsub__ = lambda self, other: self.__sub__(other)\n",
    "Value.__truediv__ = div_operation\n",
    "Value.__rtruediv__ = lambda self, other: self.__truediv__(other)\n",
    "\n",
    "\n",
    "x = Value(5.0) \n",
    "y = Value(2.0)\n",
    "z = x - y\n",
    "print(z)\n",
    "\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2: 더 복잡한 함수 테스트\n",
    "다음 함수의 gradient를 계산해보세요:\n",
    "$f(x, y) = \\frac{x^2 + y}{\\tanh(x \\cdot y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습: 복잡한 함수의 gradient 계산\n",
    "def complex_function_exercise():\n",
    "    x = Value(1.0)\n",
    "    y = Value(2.0)\n",
    "    \n",
    "    # TODO: f(x,y) = (x^2 + y) / tanh(x*y) 구현\n",
    "    # 힌트: 거듭제곱은 x * x로 구현\n",
    "    a = x * x\n",
    "    b = a + y\n",
    "    c = x * y\n",
    "    out = b / c.tanh()\n",
    "    \n",
    "    out.backward()\n",
    "    print(out, x.grad, y.grad)\n",
    "\n",
    "complex_function_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이 튜토리얼에서 우리는:\n",
    "\n",
    "1. **연산 그래프**를 구축하는 `Value` 클래스를 만들었습니다\n",
    "2. **Forward pass**에서 연산 결과와 함께 로컬 gradient를 저장했습니다\n",
    "3. **Backward pass**에서 chain rule을 사용해 gradient를 계산했습니다\n",
    "4. **위상정렬**을 사용해 올바른 순서로 gradient를 전파했습니다\n",
    "\n",
    "### 핵심 개념 요약\n",
    "\n",
    "- **자동미분**: 프로그램이 실행되면서 자동으로 미분값을 계산\n",
    "- **Chain Rule**: 합성함수의 미분법칙, 딥러닝의 핵심\n",
    "- **연산 그래프**: 계산 과정을 DAG로 표현\n",
    "- **역전파**: 출력에서 입력으로 gradient를 전파\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "- 벡터/행렬 연산으로 확장 (Day 2)\n",
    "- GPU 가속 지원\n",
    "- 더 많은 연산과 최적화 기법 추가\n",
    "- 실제 신경망 학습에 적용\n",
    "\n",
    "이제 여러분은 PyTorch나 TensorFlow의 autograd가 어떻게 동작하는지 이해했습니다! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 부록: 프로젝트 실행 방법\n",
    "\n",
    "### 1. 의존성 설치\n",
    "```bash\n",
    "cd tiny_autograd_project\n",
    "make setup  # 또는 pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 2. 테스트 실행\n",
    "```bash\n",
    "make test  # pytest 실행\n",
    "```\n",
    "\n",
    "### 3. 스모크 테스트\n",
    "```bash\n",
    "make smoke  # 또는 python 50_eval/smoke.py\n",
    "```\n",
    "\n",
    "### 4. 코드 품질 검사\n",
    "```bash\n",
    "make fmt  # ruff, black, mypy 실행\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
