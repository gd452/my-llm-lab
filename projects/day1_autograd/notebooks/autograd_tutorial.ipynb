{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Autograd Tutorial - ìŠ¤ì¹¼ë¼ ìë™ë¯¸ë¶„ êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” PyTorchì˜ autogradì™€ ìœ ì‚¬í•œ ìë™ë¯¸ë¶„(automatic differentiation) ì—”ì§„ì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•´ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- ì—°ì‚° ê·¸ë˜í”„(computation graph) ì´í•´í•˜ê¸°\n",
    "- ì—­ì „íŒŒ(backpropagation) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°\n",
    "- Chain ruleì„ í†µí•œ gradient ê³„ì‚° ì´í•´í•˜ê¸°\n",
    "\n",
    "## ì£¼ìš” ê°œë…\n",
    "\n",
    "### 1. ìë™ë¯¸ë¶„(Automatic Differentiation)ì´ë€?\n",
    "- ì»´í“¨í„°ê°€ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê¸°ë²•\n",
    "- ë”¥ëŸ¬ë‹ì—ì„œ gradientë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” í•µì‹¬ ê¸°ìˆ \n",
    "\n",
    "### 2. ì—°ì‚° ê·¸ë˜í”„(Computation Graph)\n",
    "- ì—°ì‚°ë“¤ì˜ ìˆœì„œì™€ ì˜ì¡´ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” DAG(Directed Acyclic Graph)\n",
    "- ê° ë…¸ë“œëŠ” ê°’(Value)ì„ ê°€ì§€ê³ , ì—£ì§€ëŠ” ì—°ì‚°ì„ ë‚˜íƒ€ëƒ„\n",
    "\n",
    "### 3. Chain Rule\n",
    "- í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ë²•ì¹™: $\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$\n",
    "- ì—­ì „íŒŒì˜ ìˆ˜í•™ì  ê¸°ì´ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Value í´ë˜ìŠ¤ ê¸°ë³¸ êµ¬ì¡°\n",
    "\n",
    "ë¨¼ì € ìë™ë¯¸ë¶„ì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Value:\n",
    "    \"\"\"ìŠ¤ì¹¼ë¼ ê°’ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    \n",
    "    Attributes:\n",
    "        data: ì‹¤ì œ ìŠ¤ì¹¼ë¼ ê°’\n",
    "        grad: ì´ ë…¸ë“œì— ëŒ€í•œ ì¶œë ¥ì˜ gradient\n",
    "        _prev: ì´ ë…¸ë“œë¥¼ ë§Œë“  ì…ë ¥ ë…¸ë“œë“¤ì˜ ì§‘í•©\n",
    "        _backward: ì—­ì „íŒŒ ì‹œ ì‹¤í–‰í•  í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    data: float\n",
    "    _prev: set[Value] = field(default_factory=set, repr=False)\n",
    "    _backward: Callable[[], None] = field(default=lambda: None, repr=False)\n",
    "    grad: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not isinstance(self.data, (int, float)):\n",
    "            raise TypeError(\"Value.data must be a number\")\n",
    "        self.data = float(self.data)\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return f\"Value(data={self.data:.6f}, grad={self.grad:.6f})\"\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self is other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "def append_item(x, items=[]):  # ê°€ë³€ ê¸°ë³¸ê°’\n",
    "    items.append(x)\n",
    "    return items\n",
    "\n",
    "def append_item2(x, items=None):\n",
    "    if items is None:\n",
    "        items = []\n",
    "    items.append(x)\n",
    "    return items\n",
    "\n",
    "print(append_item(1))  # [1]\n",
    "print(append_item(2))  # [1, 2]  â† ì´ì „ í˜¸ì¶œê³¼ ê³µìœ ë¨\n",
    "\n",
    "print(append_item2(1))  # [1]\n",
    "print(append_item2(2))  # [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ë§ì…ˆ ì—°ì‚° êµ¬í˜„\n",
    "\n",
    "ë§ì…ˆì˜ ë¯¸ë¶„ ê·œì¹™:\n",
    "- $f(a, b) = a + b$\n",
    "- $\\frac{\\partial f}{\\partial a} = 1$\n",
    "- $\\frac{\\partial f}{\\partial b} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_operation(self, other: float | Value) -> Value:\n",
    "    \"\"\"ë§ì…ˆ ì—°ì‚° êµ¬í˜„\n",
    "    \n",
    "    ë§ì…ˆì˜ ë¡œì»¬ gradientëŠ” í•­ìƒ 1ì…ë‹ˆë‹¤.\n",
    "    out = self + other ì¼ ë•Œ:\n",
    "    - d(out)/d(self) = 1\n",
    "    - d(out)/d(other) = 1\n",
    "    \"\"\"\n",
    "    # otherê°€ ìŠ¤ì¹¼ë¼ë©´ Valueë¡œ ë³€í™˜\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    \n",
    "    # ìƒˆë¡œìš´ Value ë…¸ë“œ ìƒì„± (forward pass)\n",
    "    out = Value(self.data + other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # Chain rule ì ìš©: gradient ëˆ„ì \n",
    "        # out.gradëŠ” ìƒìœ„ ë…¸ë“œì—ì„œ ì „íŒŒëœ gradient\n",
    "        self.grad += out.grad * 1.0  # ë§ì…ˆì˜ ë¡œì»¬ gradientëŠ” 1\n",
    "        other.grad += out.grad * 1.0\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "# Value í´ë˜ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€\n",
    "Value.__add__ = add_operation\n",
    "Value.__radd__ = lambda self, other: self.__add__(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(Value, \"__add__\")  # Trueê°€ ë‚˜ì™€ì•¼ ì •ìƒ\n",
    "\n",
    "a = Value(3.0)\n",
    "\n",
    "# ì™¼ìª½ì´ ìˆ«ìë¼ì„œ, int.__add__ê°€ Valueë¥¼ ëª¨ë¥´ë©´ a.__radd__(2)ê°€ í˜¸ì¶œë¨\n",
    "c1 = 2 + a   # __radd__ ê²½ìœ  â†’ __add__ ì¬ì‚¬ìš©\n",
    "c2 = a + 2   # __add__\n",
    "\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§ì…ˆ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ë§ì…ˆ ì˜ˆì œ\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a + b\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = a + b = {c}\")\n",
    "print(f\"cì˜ ë¶€ëª¨ ë…¸ë“œë“¤: {c._prev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ê³±ì…ˆ ì—°ì‚° êµ¬í˜„\n",
    "\n",
    "ê³±ì…ˆì˜ ë¯¸ë¶„ ê·œì¹™ (Product Rule):\n",
    "- $f(a, b) = a \\times b$\n",
    "- $\\frac{\\partial f}{\\partial a} = b$\n",
    "- $\\frac{\\partial f}{\\partial b} = a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_operation(self, other: float | Value) -> Value:\n",
    "    \"\"\"ê³±ì…ˆ ì—°ì‚° êµ¬í˜„\n",
    "    \n",
    "    ê³±ì…ˆì˜ ë¡œì»¬ gradientëŠ” ìƒëŒ€ë°©ì˜ ê°’ì…ë‹ˆë‹¤.\n",
    "    out = self * other ì¼ ë•Œ:\n",
    "    - d(out)/d(self) = other.data\n",
    "    - d(out)/d(other) = self.data\n",
    "    \"\"\"\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # Product rule ì ìš©\n",
    "        print(\"backward is called, self.grad = \", self.grad, \"other.grad = \", other.grad, \"out.grad = \", out.grad)\n",
    "        self.grad += other.data * out.grad  # d(a*b)/da = b\n",
    "        other.grad += self.data * out.grad  # d(a*b)/db = a\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.__mul__ = mul_operation\n",
    "Value.__rmul__ = lambda self, other: self.__mul__(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê³±ì…ˆ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³±ì…ˆ ì˜ˆì œ\n",
    "x = Value(3.0)\n",
    "y = Value(4.0)\n",
    "z = x * y\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"z = x * y = {z}\")\n",
    "\n",
    "# ìŠ¤ì¹¼ë¼ì™€ì˜ ê³±ì…ˆë„ ê°€ëŠ¥\n",
    "w = z * 2\n",
    "print(f\"w = z * 2 = {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: í™œì„±í™” í•¨ìˆ˜ - tanh êµ¬í˜„\n",
    "\n",
    "tanh í•¨ìˆ˜ì˜ ë¯¸ë¶„:\n",
    "- $f(x) = \\tanh(x)$\n",
    "- $\\frac{df}{dx} = 1 - \\tanh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_operation(self) -> Value:\n",
    "    \"\"\"tanh í™œì„±í™” í•¨ìˆ˜ êµ¬í˜„\n",
    "    \n",
    "    tanhì˜ ë¯¸ë¶„: dtanh/dx = 1 - tanh(x)^2\n",
    "    \"\"\"\n",
    "    t = math.tanh(self.data)\n",
    "    out = Value(t, {self})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # tanhì˜ ë¡œì»¬ gradient\n",
    "        self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.tanh = tanh_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ReLU í™œì„±í™” í•¨ìˆ˜ (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ReLU í•¨ìˆ˜ì˜ ë¯¸ë¶„:\n",
    "- $f(x) = \\max(0, x)$\n",
    "- $\\frac{df}{dx} = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_operation(self) -> Value:\n",
    "    \"\"\"ReLU í™œì„±í™” í•¨ìˆ˜ êµ¬í˜„\n",
    "    \n",
    "    ReLUì˜ ë¯¸ë¶„: x > 0 ì¼ ë•Œ 1, ì•„ë‹ˆë©´ 0\n",
    "    \"\"\"\n",
    "    out = Value(self.data if self.data > 0 else 0.0, {self})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        # ReLUì˜ ë¡œì»¬ gradient\n",
    "        self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "Value.relu = relu_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: ì—­ì „íŒŒ(Backpropagation) êµ¬í˜„\n",
    "\n",
    "ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜:\n",
    "1. ì—°ì‚° ê·¸ë˜í”„ë¥¼ ìœ„ìƒì •ë ¬(topological sort)\n",
    "2. ì¶œë ¥ ë…¸ë“œì˜ gradientë¥¼ 1ë¡œ ì„¤ì •\n",
    "3. ì—­ìˆœìœ¼ë¡œ ê° ë…¸ë“œì˜ gradient ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_operation(self) -> None:\n",
    "    \"\"\"ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\n",
    "    \n",
    "    1. DFSë¡œ ê·¸ë˜í”„ë¥¼ ìœ„ìƒì •ë ¬\n",
    "    2. ì¶œë ¥(self)ì˜ gradientë¥¼ 1ë¡œ ì„¤ì •\n",
    "    3. ì—­ìˆœìœ¼ë¡œ ê° ë…¸ë“œì˜ _backward() ì‹¤í–‰\n",
    "    \"\"\"\n",
    "    # ìœ„ìƒì •ë ¬ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ì™€ ë°©ë¬¸ ì§‘í•©\n",
    "    topo: list[Value] = []\n",
    "    visited: set[Value] = set()\n",
    "\n",
    "    def build_topo(v: Value) -> None:\n",
    "        \"\"\"DFSë¡œ ìœ„ìƒì •ë ¬ êµ¬ì¶•\"\"\"\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            # ìì‹ ë…¸ë“œë“¤ì„ ë¨¼ì € ë°©ë¬¸\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            # ìì‹ë“¤ ì²˜ë¦¬ í›„ í˜„ì¬ ë…¸ë“œ ì¶”ê°€\n",
    "            topo.append(v)\n",
    "\n",
    "    # ê·¸ë˜í”„ êµ¬ì¶•\n",
    "    build_topo(self)\n",
    "    \n",
    "    # ì¶œë ¥ì˜ gradientëŠ” 1\n",
    "    self.grad = 1.0\n",
    "    \n",
    "    # ì—­ìˆœìœ¼ë¡œ gradient ì „íŒŒ\n",
    "    for v in reversed(topo):\n",
    "        v._backward()\n",
    "\n",
    "Value.backward = backward_operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: ì „ì²´ êµ¬í˜„ í†µí•©\n",
    "\n",
    "ì´ì œ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ í†µí•©í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ Value í´ë˜ìŠ¤ êµ¬í˜„\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Set\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Value:\n",
    "    data: float\n",
    "    _prev: Set[Value] = field(default_factory=set, repr=False)\n",
    "    _backward: Callable[[], None] = field(default=lambda: None, repr=False)\n",
    "    grad: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not isinstance(self.data, (int, float)):\n",
    "            raise TypeError(\"Value.data must be a number\")\n",
    "        self.data = float(self.data)\n",
    "\n",
    "    def __add__(self, other: float | Value) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, {self, other})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other: float | Value) -> Value:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other: float | Value) -> Value:\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, {self, other})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other: float | Value) -> Value:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def tanh(self) -> Value:\n",
    "        t = math.tanh(self.data)\n",
    "        out = Value(t, {self})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self) -> Value:\n",
    "        out = Value(self.data if self.data > 0 else 0.0, {self})\n",
    "\n",
    "        def _backward() -> None:\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        topo: list[Value] = []\n",
    "        visited: set[Value] = set()\n",
    "\n",
    "        def build(v: Value) -> None:\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self is other\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data:.6f}, grad={self.grad:.6f})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: ì‹¤ì œ ì˜ˆì œ - ë³µì¡í•œ í•¨ìˆ˜ì˜ gradient ê³„ì‚°\n",
    "\n",
    "ì´ì œ êµ¬í˜„í•œ ìë™ë¯¸ë¶„ ì—”ì§„ì„ í…ŒìŠ¤íŠ¸í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ í•¨ìˆ˜: $f(a, b) = (a \\times b + a) \\times \\tanh(b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³µì¡í•œ í•¨ìˆ˜ ì˜ˆì œ\n",
    "def test_complex_function():\n",
    "    # ì…ë ¥ê°’ ì •ì˜\n",
    "    a = Value(1.3)\n",
    "    b = Value(-0.7)\n",
    "    \n",
    "    # Forward pass: f(a,b) = (a*b + a) * tanh(b)\n",
    "    c = a * b  # a * b\n",
    "    d = c + a  # a * b + a\n",
    "    e = b.tanh()  # tanh(b)\n",
    "    out = d * e  # (a * b + a) * tanh(b)\n",
    "    \n",
    "    print(\"=== Forward Pass ===\")\n",
    "    print(f\"a = {a.data:.6f}\")\n",
    "    print(f\"b = {b.data:.6f}\")\n",
    "    print(f\"c = a * b = {c.data:.6f}\")\n",
    "    print(f\"d = c + a = {d.data:.6f}\")\n",
    "    print(f\"e = tanh(b) = {e.data:.6f}\")\n",
    "    print(f\"out = d * e = {out.data:.6f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    out.backward()\n",
    "    \n",
    "    print(\"\\n=== Backward Pass ===\")\n",
    "    print(f\"âˆ‚out/âˆ‚a = {a.grad:.6f}\")\n",
    "    print(f\"âˆ‚out/âˆ‚b = {b.grad:.6f}\")\n",
    "    \n",
    "    return out, a.grad, b.grad\n",
    "\n",
    "result = test_complex_function()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë¹„êµ ê²€ì¦\n",
    "\n",
    "ìš°ë¦¬ì˜ ìë™ë¯¸ë¶„ì´ ì˜¬ë°”ë¥¸ì§€ ìˆ˜ì¹˜ ë¯¸ë¶„(numerical differentiation)ê³¼ ë¹„êµí•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def numerical_gradient(f, a0: float, b0: float, eps: float = 1e-6):\n",
    "    \"\"\"ì¤‘ì•™ ì°¨ë¶„ë²•ì„ ì‚¬ìš©í•œ ìˆ˜ì¹˜ ë¯¸ë¶„\n",
    "    \n",
    "    f'(x) â‰ˆ [f(x+Îµ) - f(x-Îµ)] / 2Îµ\n",
    "    \"\"\"\n",
    "    # aì— ëŒ€í•œ í¸ë¯¸ë¶„\n",
    "    grad_a = (f(a0 + eps, b0) - f(a0 - eps, b0)) / (2 * eps)\n",
    "    \n",
    "    # bì— ëŒ€í•œ í¸ë¯¸ë¶„\n",
    "    grad_b = (f(a0, b0 + eps) - f(a0, b0 - eps)) / (2 * eps)\n",
    "    \n",
    "    return grad_a, grad_b\n",
    "\n",
    "def f_scalar(a0: float, b0: float) -> float:\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ì˜ ìŠ¤ì¹¼ë¼ ë²„ì „\"\"\"\n",
    "    return (a0 * b0 + a0) * math.tanh(b0)\n",
    "\n",
    "def verify_gradients():\n",
    "    \"\"\"ìë™ë¯¸ë¶„ê³¼ ìˆ˜ì¹˜ë¯¸ë¶„ ë¹„êµ\"\"\"\n",
    "    test_cases = [\n",
    "        (1.3, -0.7),\n",
    "        (0.5, 0.5),\n",
    "        (-1.2, 2.0)\n",
    "    ]\n",
    "    \n",
    "    for a0, b0 in test_cases:\n",
    "        # ìë™ë¯¸ë¶„\n",
    "        a, b = Value(a0), Value(b0)\n",
    "        out = (a * b + a) * b.tanh()\n",
    "        out.backward()\n",
    "        \n",
    "        # ìˆ˜ì¹˜ë¯¸ë¶„\n",
    "        grad_a_num, grad_b_num = numerical_gradient(f_scalar, a0, b0)\n",
    "        \n",
    "        # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°\n",
    "        def relative_error(x, y):\n",
    "            denom = max(1.0, abs(x), abs(y))\n",
    "            return abs(x - y) / denom\n",
    "        \n",
    "        err_a = relative_error(a.grad, grad_a_num)\n",
    "        err_b = relative_error(b.grad, grad_b_num)\n",
    "        \n",
    "        print(f\"\\ní…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: a={a0}, b={b0}\")\n",
    "        print(f\"  ìë™ë¯¸ë¶„: âˆ‚f/âˆ‚a = {a.grad:.8f}, âˆ‚f/âˆ‚b = {b.grad:.8f}\")\n",
    "        print(f\"  ìˆ˜ì¹˜ë¯¸ë¶„: âˆ‚f/âˆ‚a = {grad_a_num:.8f}, âˆ‚f/âˆ‚b = {grad_b_num:.8f}\")\n",
    "        print(f\"  ìƒëŒ€ì˜¤ì°¨: a = {err_a:.2e}, b = {err_b:.2e}\")\n",
    "        print(f\"  í†µê³¼: {err_a < 1e-4 and err_b < 1e-4}\")\n",
    "\n",
    "verify_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: ì‹œê°í™” - ì—°ì‚° ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ì—°ì‚° ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•´ì„œ ìë™ë¯¸ë¶„ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë” ì˜ ì´í•´í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_graph(root):\n",
    "    \"\"\"ì—°ì‚° ê·¸ë˜í”„ ì¶”ì \"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "    nodes, edges = list(), set()\n",
    "    \n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.append(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    \n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_graph(root):\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„ í‘œí˜„\"\"\"\n",
    "    nodes, edges = trace_graph(root)\n",
    "    \n",
    "    print(\"\\n=== ì—°ì‚° ê·¸ë˜í”„ êµ¬ì¡° ===\")\n",
    "    print(f\"ë…¸ë“œ ê°œìˆ˜: {len(nodes)}\")\n",
    "    print(f\"ì—£ì§€ ê°œìˆ˜: {len(edges)}\")\n",
    "    \n",
    "    print(\"\\në…¸ë“œ ì •ë³´:\")\n",
    "    for i, node in enumerate(nodes):\n",
    "        print(f\"  Node {i}: data={node.data:.4f}, grad={node.grad:.4f}\")\n",
    "    \n",
    "    print(\"\\nì—°ê²° ì •ë³´:\")\n",
    "    for src, dst in edges:\n",
    "        print(f\"  {src.data:.4f} -> {dst.data:.4f}\")\n",
    "\n",
    "# ì˜ˆì œ ê·¸ë˜í”„ ìƒì„± ë° ì‹œê°í™”\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b\n",
    "d = c + a\n",
    "e = d.tanh()\n",
    "\n",
    "e.backward()\n",
    "draw_graph(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "ì´ì œ ìë™ë¯¸ë¶„ ì—”ì§„ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ì—°ìŠµë¬¸ì œë¥¼ í’€ì–´ë³´ì„¸ìš”:\n",
    "\n",
    "### ë¬¸ì œ 1: ìƒˆë¡œìš´ ì—°ì‚° ì¶”ê°€í•˜ê¸°\n",
    "ë¹¼ê¸°(`__sub__`)ì™€ ë‚˜ëˆ„ê¸°(`__truediv__`) ì—°ì‚°ì„ êµ¬í˜„í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ: ë¹¼ê¸°ì™€ ë‚˜ëˆ„ê¸° êµ¬í˜„\n",
    "def sub_operation(self, other):\n",
    "    # TODO: ë¹¼ê¸° êµ¬í˜„\n",
    "    # íŒíŠ¸: a - b = a + (-b)\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data - other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        self.grad += out.grad * 1.0\n",
    "        other.grad += out.grad * -1.0\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "def div_operation(self, other):\n",
    "    # TODO: ë‚˜ëˆ„ê¸° êµ¬í˜„\n",
    "    # íŒíŠ¸: a / bì˜ ë¯¸ë¶„\n",
    "    # d(a/b)/da = 1/b\n",
    "    # d(a/b)/db = -a/b^2\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data / other.data, {self, other})\n",
    "\n",
    "    def _backward() -> None:\n",
    "        self.grad += out.grad * 1.0 / other.data\n",
    "        other.grad += out.grad * -self.data / other.data**2\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "\n",
    "Value.__sub__ = sub_operation\n",
    "Value.__rsub__ = lambda self, other: self.__sub__(other)\n",
    "Value.__truediv__ = div_operation\n",
    "Value.__rtruediv__ = lambda self, other: self.__truediv__(other)\n",
    "\n",
    "\n",
    "x = Value(5.0) \n",
    "y = Value(2.0)\n",
    "z = x - y\n",
    "print(z)\n",
    "\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¬¸ì œ 2: ë” ë³µì¡í•œ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "ë‹¤ìŒ í•¨ìˆ˜ì˜ gradientë¥¼ ê³„ì‚°í•´ë³´ì„¸ìš”:\n",
    "$f(x, y) = \\frac{x^2 + y}{\\tanh(x \\cdot y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ìŠµ: ë³µì¡í•œ í•¨ìˆ˜ì˜ gradient ê³„ì‚°\n",
    "def complex_function_exercise():\n",
    "    x = Value(1.0)\n",
    "    y = Value(2.0)\n",
    "    \n",
    "    # TODO: f(x,y) = (x^2 + y) / tanh(x*y) êµ¬í˜„\n",
    "    # íŒíŠ¸: ê±°ë“­ì œê³±ì€ x * xë¡œ êµ¬í˜„\n",
    "    a = x * x\n",
    "    b = a + y\n",
    "    c = x * y\n",
    "    out = b / c.tanh()\n",
    "    \n",
    "    out.backward()\n",
    "    print(out, x.grad, y.grad)\n",
    "\n",
    "complex_function_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ìš°ë¦¬ëŠ”:\n",
    "\n",
    "1. **ì—°ì‚° ê·¸ë˜í”„**ë¥¼ êµ¬ì¶•í•˜ëŠ” `Value` í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤\n",
    "2. **Forward pass**ì—ì„œ ì—°ì‚° ê²°ê³¼ì™€ í•¨ê»˜ ë¡œì»¬ gradientë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤\n",
    "3. **Backward pass**ì—ì„œ chain ruleì„ ì‚¬ìš©í•´ gradientë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤\n",
    "4. **ìœ„ìƒì •ë ¬**ì„ ì‚¬ìš©í•´ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ gradientë¥¼ ì „íŒŒí–ˆìŠµë‹ˆë‹¤\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "- **ìë™ë¯¸ë¶„**: í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë˜ë©´ì„œ ìë™ìœ¼ë¡œ ë¯¸ë¶„ê°’ì„ ê³„ì‚°\n",
    "- **Chain Rule**: í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ë²•ì¹™, ë”¥ëŸ¬ë‹ì˜ í•µì‹¬\n",
    "- **ì—°ì‚° ê·¸ë˜í”„**: ê³„ì‚° ê³¼ì •ì„ DAGë¡œ í‘œí˜„\n",
    "- **ì—­ì „íŒŒ**: ì¶œë ¥ì—ì„œ ì…ë ¥ìœ¼ë¡œ gradientë¥¼ ì „íŒŒ\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- ë²¡í„°/í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ í™•ì¥ (Day 2)\n",
    "- GPU ê°€ì† ì§€ì›\n",
    "- ë” ë§ì€ ì—°ì‚°ê³¼ ìµœì í™” ê¸°ë²• ì¶”ê°€\n",
    "- ì‹¤ì œ ì‹ ê²½ë§ í•™ìŠµì— ì ìš©\n",
    "\n",
    "ì´ì œ ì—¬ëŸ¬ë¶„ì€ PyTorchë‚˜ TensorFlowì˜ autogradê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì´í•´í–ˆìŠµë‹ˆë‹¤! ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶€ë¡: í”„ë¡œì íŠ¸ ì‹¤í–‰ ë°©ë²•\n",
    "\n",
    "### 1. ì˜ì¡´ì„± ì„¤ì¹˜\n",
    "```bash\n",
    "cd tiny_autograd_project\n",
    "make setup  # ë˜ëŠ” pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "```bash\n",
    "make test  # pytest ì‹¤í–‰\n",
    "```\n",
    "\n",
    "### 3. ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸\n",
    "```bash\n",
    "make smoke  # ë˜ëŠ” python 50_eval/smoke.py\n",
    "```\n",
    "\n",
    "### 4. ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬\n",
    "```bash\n",
    "make fmt  # ruff, black, mypy ì‹¤í–‰\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
