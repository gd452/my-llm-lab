# ğŸ“– ìë™ë¯¸ë¶„(Autograd) í•µì‹¬ ê°œë… ì •ë¦¬

## 1. ë¯¸ë¶„(Differentiation) ê¸°ì´ˆ

### 1.1 ë„í•¨ìˆ˜(Derivative)ë€?
- **ì •ì˜**: í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨
- **ê¸°í•˜í•™ì  ì˜ë¯¸**: ì ‘ì„ ì˜ ê¸°ìš¸ê¸°
- **ìˆ˜ì‹**: f'(x) = lim(hâ†’0) [f(x+h) - f(x)] / h

### 1.2 í¸ë¯¸ë¶„(Partial Derivative)
- **ì •ì˜**: ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì—ì„œ í•œ ë³€ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„
- **í‘œê¸°**: âˆ‚f/âˆ‚x (fë¥¼ xì— ëŒ€í•´ í¸ë¯¸ë¶„)
- **ì˜ˆì‹œ**: 
  ```
  f(x,y) = xÂ²y + xyÂ²
  âˆ‚f/âˆ‚x = 2xy + yÂ²
  âˆ‚f/âˆ‚y = xÂ² + 2xy
  ```

## 2. Chain Rule (ì—°ì‡„ ë²•ì¹™)

### 2.1 ê°œë…
- **í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ë²•ì¹™**
- z = f(g(x)) ì¼ ë•Œ: dz/dx = dz/dg Ã— dg/dx

### 2.2 ì‹ ê²½ë§ì—ì„œì˜ ì ìš©
```python
# ì˜ˆì‹œ: z = (x * y) + x
# x = 2, y = 3 ì¼ ë•Œ

# Forward pass:
a = x * y  # a = 6
z = a + x  # z = 8

# Backward pass (Chain Rule):
dz/dz = 1           # ì¶œë ¥ì˜ gradientëŠ” 1
dz/da = 1           # ë§ì…ˆì˜ ë¡œì»¬ gradient
dz/dx = dz/da * da/dx + 1  # Chain rule + ì§ì ‘ ì—°ê²°
      = 1 * y + 1 = 4
dz/dy = dz/da * da/dy = 1 * x = 2
```

## 3. ê³„ì‚° ê·¸ë˜í”„(Computation Graph)

### 3.1 êµ¬ì„± ìš”ì†Œ
- **ë…¸ë“œ(Node)**: ê°’ ë˜ëŠ” ì—°ì‚°
- **ì—£ì§€(Edge)**: ë°ì´í„° íë¦„
- **ë°©í–¥**: Forward(ìˆœë°©í–¥) â†’ Backward(ì—­ë°©í–¥)

### 3.2 ê·¸ë˜í”„ ì˜ˆì‹œ
```
     x (2) â”€â”€â”
              â”œâ”€[Ã—]â”€â†’ a (6) â”€â”€â”
     y (3) â”€â”€â”˜                 â”œâ”€[+]â”€â†’ z (8)
                      x (2) â”€â”€â”˜
```

### 3.3 ìœ„ìƒì •ë ¬(Topological Sort)
- **ëª©ì **: ì˜ì¡´ì„± ìˆœì„œëŒ€ë¡œ ë…¸ë“œ ì²˜ë¦¬
- **ì—­ì „íŒŒ ì‹œ**: ì—­ìˆœìœ¼ë¡œ gradient ê³„ì‚°

## 4. ìë™ë¯¸ë¶„ êµ¬í˜„ ì›ë¦¬

### 4.1 Forward Pass
```python
# ê° ì—°ì‚°ë§ˆë‹¤:
1. ê²°ê³¼ê°’ ê³„ì‚°
2. ì—°ì‚° ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€
3. ë¡œì»¬ gradient ì €ì¥ (_backward í•¨ìˆ˜)
```

### 4.2 Backward Pass
```python
# ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜:
1. ì¶œë ¥ ë…¸ë“œì˜ grad = 1.0
2. ìœ„ìƒì •ë ¬ ì—­ìˆœìœ¼ë¡œ:
   - ê° ë…¸ë“œì˜ _backward() ì‹¤í–‰
   - gradient ëˆ„ì  (+=)
```

## 5. ì£¼ìš” ì—°ì‚°ì˜ ë¯¸ë¶„

### 5.1 ê¸°ë³¸ ì—°ì‚°
| ì—°ì‚° | Forward | ë¡œì»¬ Gradient |
|------|---------|---------------|
| ë§ì…ˆ | z = x + y | âˆ‚z/âˆ‚x = 1, âˆ‚z/âˆ‚y = 1 |
| ê³±ì…ˆ | z = x Ã— y | âˆ‚z/âˆ‚x = y, âˆ‚z/âˆ‚y = x |
| ê±°ë“­ì œê³± | z = x^n | âˆ‚z/âˆ‚x = nÃ—x^(n-1) |

### 5.2 í™œì„±í™” í•¨ìˆ˜
| í•¨ìˆ˜ | Forward | Gradient |
|------|---------|----------|
| tanh | y = tanh(x) | dy/dx = 1 - tanhÂ²(x) |
| ReLU | y = max(0,x) | dy/dx = 1 if x>0 else 0 |
| Sigmoid | y = 1/(1+e^-x) | dy/dx = y(1-y) |

## 6. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Value í´ë˜ìŠ¤ í•„ìˆ˜ ìš”ì†Œ
- [ ] `data`: ì‹¤ì œ ê°’ ì €ì¥
- [ ] `grad`: gradient ì €ì¥
- [ ] `_prev`: ë¶€ëª¨ ë…¸ë“œë“¤ (set)
- [ ] `_backward`: ì—­ì „íŒŒ í•¨ìˆ˜
- [ ] `__hash__`, `__eq__`: setì— ì €ì¥ ê°€ëŠ¥í•˜ê²Œ

### ì—°ì‚° êµ¬í˜„ íŒ¨í„´
```python
def operation(self, other):
    # 1. Forward ê³„ì‚°
    out = Value(ê³„ì‚°_ê²°ê³¼, {self, other})
    
    # 2. Backward í•¨ìˆ˜ ì •ì˜
    def _backward():
        self.grad += ë¡œì»¬_gradient * out.grad
        other.grad += ë¡œì»¬_gradient * out.grad
    
    # 3. í•¨ìˆ˜ ì—°ê²°
    out._backward = _backward
    return out
```

## 7. ë””ë²„ê¹… íŒ

### 7.1 Gradient ê²€ì¦
```python
# ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë¹„êµ
def check_gradient(f, x, h=1e-5):
    # ìˆ˜ì¹˜ ë¯¸ë¶„
    grad_numerical = (f(x+h) - f(x-h)) / (2*h)
    
    # ìë™ ë¯¸ë¶„
    y = f(x)
    y.backward()
    grad_auto = x.grad
    
    # ìƒëŒ€ ì˜¤ì°¨
    error = abs(grad_numerical - grad_auto) / max(abs(grad_numerical), 1e-8)
    assert error < 1e-4
```

### 7.2 ì¼ë°˜ì ì¸ ë¬¸ì œë“¤
1. **Gradient í­ë°œ**: ê°’ì´ ë„ˆë¬´ ì»¤ì§ â†’ í•™ìŠµë¥  ì¡°ì •
2. **Gradient ì†Œì‹¤**: ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ â†’ í™œì„±í™” í•¨ìˆ˜ ë³€ê²½
3. **NaN ë°œìƒ**: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°, log(0) ë“± â†’ ì•ˆì •í™” ê¸°ë²• ì ìš©

## 8. ì—°ìŠµ ë¬¸ì œ

### Level 1: ê¸°ì´ˆ
1. `__sub__` êµ¬í˜„í•˜ê¸° (íŒíŠ¸: a-b = a+(-b))
2. `__neg__` êµ¬í˜„í•˜ê¸° (ë¶€í˜¸ ë°˜ì „)
3. `__pow__` êµ¬í˜„í•˜ê¸° (ê±°ë“­ì œê³±)

### Level 2: ì¤‘ê¸‰
1. `sigmoid` í™œì„±í™” í•¨ìˆ˜ ì¶”ê°€
2. `log`, `exp` ì—°ì‚° ì¶”ê°€
3. MSE ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„

### Level 3: ê³ ê¸‰
1. í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ í™•ì¥
2. Batch ì²˜ë¦¬ ì§€ì›
3. GPU ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜

## 9. ì°¸ê³  ìë£Œ

### í•„ë… ìë£Œ
- [Calculus on Computational Graphs](https://colah.github.io/posts/2015-08-Backprop/)
- [Yes you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)
- [Automatic Differentiation in Machine Learning](https://arxiv.org/abs/1502.05767)

### ì˜ìƒ ìë£Œ
- [3Blue1Brown - Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
- [Andrej Karpathy - Building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)