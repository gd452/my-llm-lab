"""
ğŸ¯ Mini MNIST: ë²¡í„°í™”ëœ ì‹ ê²½ë§ìœ¼ë¡œ ìˆ«ì ë¶„ë¥˜

MNISTì˜ ì¼ë¶€(0-4)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°í™”ëœ ì‹ ê²½ë§ì„ í•™ìŠµí•©ë‹ˆë‹¤.
sklearnì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from core.nn_vectorized import MLPVectorized, SGDOptimizer
from core.tensor_ops import softmax, cross_entropy, accuracy
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def load_mini_mnist(num_classes=5):
    """
    Mini MNIST ë°ì´í„° ë¡œë“œ (0-4 ìˆ«ìë§Œ)
    
    Returns:
        X_train, X_test, y_train, y_test
    """
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    
    # sklearnì˜ digits ë°ì´í„°ì…‹ ì‚¬ìš© (8x8 ì´ë¯¸ì§€)
    digits = load_digits()
    X, y = digits.data, digits.target
    
    # 0-4 í´ë˜ìŠ¤ë§Œ ì„ íƒ
    mask = y < num_classes
    X, y = X[mask], y[mask]
    
    # ë°ì´í„° ì •ê·œí™”
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    print(f"   í´ë˜ìŠ¤: {num_classes}ê°œ (0-{num_classes-1})")
    
    return X_train, X_test, y_train, y_test


class DataLoader:
    """ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° ë¡œë”"""
    
    def __init__(self, X, y, batch_size=32, shuffle=True):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.n_samples = len(X)
    
    def __iter__(self):
        indices = np.arange(self.n_samples)
        if self.shuffle:
            np.random.shuffle(indices)
        
        for start_idx in range(0, self.n_samples, self.batch_size):
            batch_indices = indices[start_idx:start_idx + self.batch_size]
            yield self.X[batch_indices], self.y[batch_indices]
    
    def __len__(self):
        return (self.n_samples + self.batch_size - 1) // self.batch_size


def train_epoch(model, dataloader, optimizer):
    """í•œ ì—í­ í•™ìŠµ"""
    total_loss = 0
    total_acc = 0
    n_batches = 0
    
    for X_batch, y_batch in dataloader:
        # Forward pass
        logits = model.forward(X_batch)
        probs = softmax(logits)
        
        # Loss ê³„ì‚°
        loss = cross_entropy(probs, y_batch)
        acc = accuracy(probs, y_batch)
        
        # Backward pass
        grad_out = probs.copy()
        grad_out[np.arange(len(y_batch)), y_batch] -= 1
        grad_out /= len(y_batch)
        
        model.backward(grad_out)
        
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()
        
        total_loss += loss
        total_acc += acc
        n_batches += 1
    
    return total_loss / n_batches, total_acc / n_batches


def evaluate(model, X_test, y_test, batch_size=32):
    """ëª¨ë¸ í‰ê°€"""
    dataloader = DataLoader(X_test, y_test, batch_size=batch_size, shuffle=False)
    
    total_acc = 0
    n_batches = 0
    all_preds = []
    
    for X_batch, y_batch in dataloader:
        probs = model.predict_proba(X_batch)
        batch_acc = accuracy(probs, y_batch)
        total_acc += batch_acc
        n_batches += 1
        all_preds.extend(np.argmax(probs, axis=1))
    
    return total_acc / n_batches, np.array(all_preds)


def print_confusion_matrix(y_true, y_pred, num_classes=5):
    """í˜¼ë™ í–‰ë ¬ ì¶œë ¥"""
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    for true, pred in zip(y_true, y_pred):
        cm[true, pred] += 1
    
    print("\nğŸ“Š Confusion Matrix:")
    print("    ì˜ˆì¸¡ â†’")
    print("ì‹¤ì œ ", end="")
    for i in range(num_classes):
        print(f"  {i}", end="")
    print()
    
    for i in range(num_classes):
        print(f" {i}  ", end="")
        for j in range(num_classes):
            if i == j:
                print(f" \033[92m{cm[i, j]:2d}\033[0m", end="")  # ë…¹ìƒ‰
            else:
                print(f" {cm[i, j]:2d}", end="")
        print(f"  ({cm[i, i]/cm[i].sum()*100:.1f}%)")


def main():
    """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
    print("=" * 60)
    print("ğŸ”¢ Mini MNIST Classifier (Vectorized)")
    print("=" * 60)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    num_classes = 5
    hidden_dims = [128, 64]
    learning_rate = 0.1
    momentum = 0.9
    batch_size = 32
    epochs = 50
    
    # ë°ì´í„° ë¡œë“œ
    X_train, X_test, y_train, y_test = load_mini_mnist(num_classes)
    
    # ëª¨ë¸ ìƒì„±
    model = MLPVectorized(
        input_dim=X_train.shape[1],  # 64 (8x8)
        hidden_dims=hidden_dims,
        output_dim=num_classes
    )
    
    print(f"\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°: {X_train.shape[1]} â†’ {' â†’ '.join(map(str, hidden_dims))} â†’ {num_classes}")
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = SGDOptimizer(model, learning_rate=learning_rate, momentum=momentum)
    
    # ë°ì´í„° ë¡œë”
    train_loader = DataLoader(X_train, y_train, batch_size=batch_size)
    
    # í•™ìŠµ
    print(f"\nğŸ“ˆ í•™ìŠµ ì‹œì‘...")
    print(f"   Batch size: {batch_size}")
    print(f"   Learning rate: {learning_rate}")
    print(f"   Momentum: {momentum}")
    print("-" * 40)
    
    history = {'loss': [], 'acc': [], 'val_acc': []}
    
    for epoch in range(epochs):
        # í•™ìŠµ
        train_loss, train_acc = train_epoch(model, train_loader, optimizer)
        
        # í‰ê°€
        val_acc, _ = evaluate(model, X_test, y_test)
        
        # ê¸°ë¡
        history['loss'].append(train_loss)
        history['acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # ì¶œë ¥
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}: "
                  f"Loss={train_loss:.4f}, "
                  f"Train Acc={train_acc:.3f}, "
                  f"Val Acc={val_acc:.3f}")
    
    print("-" * 40)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ“Š ìµœì¢… í‰ê°€:")
    test_acc, predictions = evaluate(model, X_test, y_test)
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f} ({int(test_acc * len(y_test))}/{len(y_test)})")
    
    # í˜¼ë™ í–‰ë ¬
    print_confusion_matrix(y_test, predictions, num_classes)
    
    # í•™ìŠµ ê³¡ì„  ì‹œê°í™” (í…ìŠ¤íŠ¸)
    print("\nğŸ“ˆ í•™ìŠµ ê³¡ì„ :")
    print("Acc â†‘")
    
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê·¸ë˜í”„
    max_acc = max(max(history['acc']), max(history['val_acc']))
    height = 10
    
    for h in range(height, -1, -1):
        line = "   â”‚"
        
        # ìƒ˜í”Œë§ (50 ì—í­ì„ 20ê°œ í¬ì¸íŠ¸ë¡œ)
        step = max(1, len(history['acc']) // 20)
        
        for i in range(0, len(history['acc']), step):
            if history['acc'][i] / max_acc * height >= h:
                line += "â—"  # Train
            elif history['val_acc'][i] / max_acc * height >= h:
                line += "â—‹"  # Val
            else:
                line += " "
        
        if h == height:
            line += f" {max_acc:.2f}"
        elif h == 0:
            line += " 0.00"
            
        print(line)
    
    print("   â””" + "â”€" * 20 + "â†’ Epoch")
    print("    0" + " " * 16 + f"{epochs}")
    print("\n   â— Train  â—‹ Validation")
    
    # ì„±ê³µ ë©”ì‹œì§€
    if test_acc > 0.9:
        print("\n" + "ğŸ‰" * 20)
        print("ì¶•í•˜í•©ë‹ˆë‹¤! 90% ì´ìƒì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
        print("ë²¡í„°í™”ì˜ í˜ì„ ê²½í—˜í•˜ì…¨ìŠµë‹ˆë‹¤!")
        print("ğŸ‰" * 20)
    
    return model, history


if __name__ == "__main__":
    model, history = main()