# ğŸª Batch Processing: ë³‘ë ¬ ì²˜ë¦¬ì˜ í˜

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- Batch ì²˜ë¦¬ì˜ ì¥ì  ì´í•´
- Mini-batch êµ¬í˜„
- íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©

## 1. ì™œ Batch Processingì¸ê°€?

### ë‹¨ì¼ ìƒ˜í”Œ vs Batch
```python
# ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ (ëŠë¦¼)
for x in dataset:
    y = model(x)  # í•œ ë²ˆì— í•˜ë‚˜ì”©
    loss = compute_loss(y, target)
    update_weights()

# Batch ì²˜ë¦¬ (ë¹ ë¦„)
for batch in dataloader:
    Y = model(batch)  # ì—¬ëŸ¬ ê°œ ë™ì‹œ ì²˜ë¦¬!
    loss = compute_loss(Y, targets)
    update_weights()
```

### Batch ì²˜ë¦¬ì˜ ì¥ì 
1. **ì—°ì‚° íš¨ìœ¨ì„±**: í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
2. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ìºì‹œ í™œìš©ë„ ì¦ê°€
3. **í•™ìŠµ ì•ˆì •ì„±**: ë…¸ì´ì¦ˆ ê°ì†Œ, ë¶€ë“œëŸ¬ìš´ ìˆ˜ë ´
4. **GPU í™œìš©**: GPUëŠ” ë³‘ë ¬ ì²˜ë¦¬ì— ìµœì í™”

## 2. Batch ì°¨ì› ì´í•´í•˜ê¸°

### ì°¨ì› ê·œì•½
```python
# ì¼ë°˜ì ì¸ ì°¨ì› ìˆœì„œ
# Images: (N, H, W, C) - TensorFlow
# Images: (N, C, H, W) - PyTorch
# Sequences: (N, T, D)
# Tabular: (N, D)

# N: Batch size
# H, W: Height, Width
# C: Channels
# T: Time steps / Sequence length
# D: Features / Dimensions
```

### Batch ì°¨ì› ë‹¤ë£¨ê¸°
```python
import numpy as np

# Batch ì°¨ì› ì¶”ê°€
single_image = np.random.randn(28, 28)  # (28, 28)
batch_image = single_image[np.newaxis, :]  # (1, 28, 28)

# Batch ì°¨ì› ì œê±°
predictions = np.random.randn(1, 10)  # (1, 10)
single_pred = predictions.squeeze(0)  # (10,)

# Batch í•©ì¹˜ê¸°
batch1 = np.random.randn(32, 784)
batch2 = np.random.randn(32, 784)
combined = np.concatenate([batch1, batch2], axis=0)  # (64, 784)
```

## 3. Mini-batch êµ¬í˜„

### DataLoader êµ¬í˜„
```python
class DataLoader:
    def __init__(self, X, y, batch_size=32, shuffle=True):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.n_samples = len(X)
        self.n_batches = (self.n_samples + batch_size - 1) // batch_size
        
    def __iter__(self):
        indices = np.arange(self.n_samples)
        
        if self.shuffle:
            np.random.shuffle(indices)
        
        for start_idx in range(0, self.n_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, self.n_samples)
            batch_indices = indices[start_idx:end_idx]
            
            yield self.X[batch_indices], self.y[batch_indices]
    
    def __len__(self):
        return self.n_batches

# ì‚¬ìš© ì˜ˆ
X_train = np.random.randn(1000, 784)
y_train = np.random.randint(0, 10, 1000)

dataloader = DataLoader(X_train, y_train, batch_size=32)

for epoch in range(10):
    for X_batch, y_batch in dataloader:
        # í•™ìŠµ ìˆ˜í–‰
        print(f"Batch shape: {X_batch.shape}")
        break
    break
```

## 4. Batch ì—°ì‚° ìµœì í™”

### ë²¡í„°í™”ëœ í™œì„±í™” í•¨ìˆ˜
```python
def relu_batch(X):
    """
    Batch ReLU
    X: (batch_size, features)
    """
    return np.maximum(0, X)

def sigmoid_batch(X):
    """
    Batch Sigmoid (ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •)
    """
    # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    X = np.clip(X, -500, 500)
    return 1 / (1 + np.exp(-X))

def softmax_batch(X):
    """
    Batch Softmax
    X: (batch_size, num_classes)
    """
    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ“ê°’ ë¹¼ê¸°
    X_max = X.max(axis=1, keepdims=True)
    exp_X = np.exp(X - X_max)
    return exp_X / exp_X.sum(axis=1, keepdims=True)
```

### Batch ì†ì‹¤ í•¨ìˆ˜
```python
def mse_loss_batch(y_pred, y_true):
    """
    Batch MSE Loss
    y_pred, y_true: (batch_size, output_dim)
    """
    return np.mean((y_pred - y_true) ** 2)

def cross_entropy_batch(y_pred, y_true):
    """
    Batch Cross Entropy Loss
    y_pred: (batch_size, num_classes) - probabilities
    y_true: (batch_size,) - class indices
    """
    batch_size = y_pred.shape[0]
    # ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ë§Œ ì„ íƒ
    log_probs = -np.log(y_pred[np.arange(batch_size), y_true] + 1e-8)
    return np.mean(log_probs)
```

## 5. Batch Normalization

### êµ¬í˜„
```python
class BatchNorm:
    def __init__(self, num_features, eps=1e-5, momentum=0.9):
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)
        
        # Running statistics (ì¶”ë¡ ìš©)
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)
        
        self.training = True
    
    def forward(self, X):
        """
        X: (batch_size, num_features)
        """
        if self.training:
            # ë°°ì¹˜ í†µê³„ ê³„ì‚°
            batch_mean = X.mean(axis=0)
            batch_var = X.var(axis=0)
            
            # Running statistics ì—…ë°ì´íŠ¸
            self.running_mean = (self.momentum * self.running_mean + 
                                (1 - self.momentum) * batch_mean)
            self.running_var = (self.momentum * self.running_var + 
                               (1 - self.momentum) * batch_var)
            
            # ì •ê·œí™”
            X_norm = (X - batch_mean) / np.sqrt(batch_var + self.eps)
        else:
            # ì¶”ë¡  ì‹œ running statistics ì‚¬ìš©
            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.eps)
        
        # ìŠ¤ì¼€ì¼ê³¼ ì‹œí”„íŠ¸
        out = self.gamma * X_norm + self.beta
        
        # ì—­ì „íŒŒë¥¼ ìœ„í•´ ì €ì¥
        self.cache = (X, X_norm, batch_mean, batch_var)
        
        return out
    
    def backward(self, dout):
        """
        ì—­ì „íŒŒ
        dout: (batch_size, num_features)
        """
        X, X_norm, mean, var = self.cache
        batch_size = X.shape[0]
        
        # íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
        self.dgamma = (dout * X_norm).sum(axis=0)
        self.dbeta = dout.sum(axis=0)
        
        # ì…ë ¥ ê·¸ë˜ë””ì–¸íŠ¸ (ë³µì¡!)
        dX_norm = dout * self.gamma
        dvar = ((dX_norm * (X - mean) * -0.5 * 
                (var + self.eps) ** (-1.5)).sum(axis=0))
        dmean = (dX_norm * -1 / np.sqrt(var + self.eps)).sum(axis=0)
        
        dX = (dX_norm / np.sqrt(var + self.eps) + 
              dvar * 2 * (X - mean) / batch_size + 
              dmean / batch_size)
        
        return dX
```

## 6. Batch í¬ê¸° ì„ íƒ

### Trade-offs
```python
# ì‘ì€ ë°°ì¹˜ (ì˜ˆ: 32)
# + ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
# + ì •ê·œí™” íš¨ê³¼ (ë…¸ì´ì¦ˆ)
# - ëŠë¦° ìˆ˜ë ´
# - GPU í™œìš©ë„ ë‚®ìŒ

# í° ë°°ì¹˜ (ì˜ˆ: 256, 512)
# + ë¹ ë¥¸ í•™ìŠµ
# + GPU íš¨ìœ¨ì 
# - ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
# - Sharp minima ìœ„í—˜

# ì ì‘ì  ë°°ì¹˜ í¬ê¸°
def get_batch_size(epoch, initial_bs=32, max_bs=256):
    """ì—í­ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¦ê°€"""
    return min(initial_bs * (2 ** (epoch // 10)), max_bs)
```

## 7. ë©”ëª¨ë¦¬ ê´€ë¦¬

### Gradient Accumulation
```python
def train_with_gradient_accumulation(model, dataloader, accumulation_steps=4):
    """
    í° ë°°ì¹˜ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
    """
    optimizer.zero_grad()
    
    for i, (X_batch, y_batch) in enumerate(dataloader):
        # Forward pass
        predictions = model(X_batch)
        loss = compute_loss(predictions, y_batch)
        
        # Backward pass (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì )
        loss = loss / accumulation_steps
        loss.backward()
        
        # accumulation_stepsë§ˆë‹¤ ì—…ë°ì´íŠ¸
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì—°ì‚°
```python
def memory_efficient_attention(Q, K, V, chunk_size=32):
    """
    ì²­í¬ ë‹¨ìœ„ë¡œ Attention ê³„ì‚° (ë©”ëª¨ë¦¬ ì ˆì•½)
    Q, K, V: (batch_size, seq_len, d_model)
    """
    batch_size, seq_len, d_model = Q.shape
    outputs = []
    
    for i in range(0, seq_len, chunk_size):
        q_chunk = Q[:, i:i+chunk_size]
        
        # ì²­í¬ë³„ attention
        scores = q_chunk @ K.transpose(-2, -1) / np.sqrt(d_model)
        attention_weights = softmax_batch(scores)
        output_chunk = attention_weights @ V
        
        outputs.append(output_chunk)
    
    return np.concatenate(outputs, axis=1)
```

## 8. ì‹¤ì „ ì˜ˆì œ: Mini-batch SGD

```python
class MiniBatchSGD:
    def __init__(self, model, learning_rate=0.01):
        self.model = model
        self.lr = learning_rate
    
    def train_epoch(self, dataloader):
        epoch_loss = 0
        n_batches = 0
        
        for X_batch, y_batch in dataloader:
            # Forward pass
            predictions = self.model.forward(X_batch)
            
            # Compute loss
            loss = cross_entropy_batch(predictions, y_batch)
            epoch_loss += loss
            
            # Backward pass
            grad_output = self.compute_grad_loss(predictions, y_batch)
            self.model.backward(grad_output)
            
            # Update weights
            self.update_parameters()
            
            n_batches += 1
        
        return epoch_loss / n_batches
    
    def compute_grad_loss(self, predictions, y_true):
        """Cross entropy gradient"""
        batch_size = predictions.shape[0]
        grad = predictions.copy()
        grad[np.arange(batch_size), y_true] -= 1
        return grad / batch_size
    
    def update_parameters(self):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        for param, grad in self.model.get_params_and_grads():
            param -= self.lr * grad
```

## ğŸ’¡ Batch Processing ìµœì í™” íŒ

1. **2ì˜ ê±°ë“­ì œê³±**: ë°°ì¹˜ í¬ê¸°ë¥¼ 32, 64, 128 ë“±ìœ¼ë¡œ
2. **Prefetching**: ë‹¤ìŒ ë°°ì¹˜ë¥¼ ë¯¸ë¦¬ ë¡œë“œ
3. **Pin Memory**: GPU ì „ì†¡ ì†ë„ í–¥ìƒ
4. **Mixed Precision**: FP16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

## ğŸ” í”„ë¡œíŒŒì¼ë§

```python
import time

def profile_batch_sizes(model, X, y):
    """ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° ì„±ëŠ¥ ì¸¡ì •"""
    batch_sizes = [1, 8, 32, 128, 512]
    
    for bs in batch_sizes:
        dataloader = DataLoader(X, y, batch_size=bs)
        
        start = time.time()
        for X_batch, y_batch in dataloader:
            _ = model(X_batch)
        
        elapsed = time.time() - start
        throughput = len(X) / elapsed
        
        print(f"Batch size {bs:3d}: "
              f"{elapsed:.2f}s, "
              f"{throughput:.0f} samples/sec")
```

## ğŸ“ ì—°ìŠµ ë¬¸ì œ

1. Variable batch sizeë¥¼ ì§€ì›í•˜ëŠ” DataLoaderë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
2. Batch ë‹¨ìœ„ Dropoutì„ êµ¬í˜„í•˜ì„¸ìš”.
3. Learning rate warm-upì„ í¬í•¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë§Œë“œì„¸ìš”.

## ë‹¤ìŒ ë‹¨ê³„

ì†ì‹¤ í•¨ìˆ˜ì˜ ì„¸ê³„ë¡œ! â†’ [04_loss_functions.md](04_loss_functions.md)