# ğŸ¯ Broadcasting: NumPyì˜ ë§ˆë²•

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- Broadcasting ê·œì¹™ ì™„ë²½ ì´í•´
- íš¨ìœ¨ì ì¸ ì—°ì‚° íŒ¨í„´ ìŠµë“
- ì‹¤ì „ í™œìš© ëŠ¥ë ¥ ë°°ì–‘

## 1. Broadcastingì´ë€?

Broadcastingì€ shapeì´ ë‹¤ë¥¸ ë°°ì—´ ê°„ì˜ ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” NumPyì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ì˜ˆì œ
```python
import numpy as np

# ìŠ¤ì¹¼ë¼ì™€ ë²¡í„°
a = np.array([1, 2, 3])
b = 10
c = a + b  # [11, 12, 13]

# ë²¡í„°ì™€ í–‰ë ¬
matrix = np.array([[1, 2, 3],
                   [4, 5, 6]])
vector = np.array([10, 20, 30])
result = matrix + vector  # ê° í–‰ì— vector ë”í•˜ê¸°
# [[11, 22, 33],
#  [14, 25, 36]]
```

## 2. Broadcasting ê·œì¹™

### ê·œì¹™ 1: ì°¨ì› ë§ì¶”ê¸°
ë’¤ì—ì„œë¶€í„° ì°¨ì›ì„ ë¹„êµí•©ë‹ˆë‹¤.

```python
A: (2, 3, 4)
B:    (3, 4)  # ì•ì— 1 ì¶”ê°€ â†’ (1, 3, 4)
# ì—°ì‚° ê°€ëŠ¥!
```

### ê·œì¹™ 2: í¬ê¸° 1ì¸ ì°¨ì› í™•ì¥
í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì€ ë‹¤ë¥¸ ë°°ì—´ì˜ í¬ê¸°ë¡œ í™•ì¥ë©ë‹ˆë‹¤.

```python
A: (2, 3, 1)
B: (1, 3, 5)
# ê²°ê³¼: (2, 3, 5)
```

### ê·œì¹™ 3: í˜¸í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
```python
A: (2, 3)
B: (4,)  # Error! 3 â‰  4
```

## 3. Broadcasting ì‹œê°í™”

```python
# ì˜ˆì œ 1: ë²¡í„° + ìŠ¤ì¹¼ë¼
a = [1, 2, 3]         # (3,)
b = 5                 # ()
# bê°€ [5, 5, 5]ë¡œ í™•ì¥
# ê²°ê³¼: [6, 7, 8]

# ì˜ˆì œ 2: í–‰ë ¬ + ë²¡í„° (í–‰ ë°©í–¥)
A = [[1, 2, 3],       # (2, 3)
     [4, 5, 6]]
b = [10, 20, 30]      # (3,)
# bê°€ ê° í–‰ì— ì ìš©
# ê²°ê³¼: [[11, 22, 33],
#        [14, 25, 36]]

# ì˜ˆì œ 3: í–‰ë ¬ + ë²¡í„° (ì—´ ë°©í–¥)
A = [[1, 2, 3],       # (2, 3)
     [4, 5, 6]]
b = [[10],            # (2, 1)
     [20]]
# bê°€ ê° ì—´ì— ì ìš©
# ê²°ê³¼: [[11, 12, 13],
#        [24, 25, 26]]
```

## 4. ì‹¤ì „ Broadcasting íŒ¨í„´

### íŒ¨í„´ 1: í‰ê·  ë¹¼ê¸° (Centering)
```python
# ê° íŠ¹ì„±ì˜ í‰ê· ì„ ë¹¼ì„œ ì¤‘ì‹¬í™”
X = np.random.randn(100, 5)  # 100ê°œ ìƒ˜í”Œ, 5ê°œ íŠ¹ì„±
mean = X.mean(axis=0)         # (5,) - ê° íŠ¹ì„±ì˜ í‰ê· 
X_centered = X - mean          # Broadcasting!
```

### íŒ¨í„´ 2: ì •ê·œí™” (Normalization)
```python
# Min-Max ì •ê·œí™”
X_min = X.min(axis=0)  # (5,)
X_max = X.max(axis=0)  # (5,)
X_normalized = (X - X_min) / (X_max - X_min)  # 0~1 ë²”ìœ„ë¡œ
```

### íŒ¨í„´ 3: Softmax êµ¬í˜„
```python
def softmax(X):
    """
    ì•ˆì •ì ì¸ Softmax êµ¬í˜„
    X: (batch_size, num_classes)
    """
    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ“ê°’ ë¹¼ê¸°
    X_max = X.max(axis=1, keepdims=True)  # (batch_size, 1)
    X_exp = np.exp(X - X_max)             # Broadcasting!
    X_sum = X_exp.sum(axis=1, keepdims=True)  # (batch_size, 1)
    return X_exp / X_sum                   # Broadcasting!
```

### íŒ¨í„´ 4: Batch Normalization
```python
def batch_norm(X, gamma, beta, eps=1e-8):
    """
    Batch Normalization
    X: (batch_size, features)
    gamma, beta: (features,) - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    """
    mean = X.mean(axis=0)  # (features,)
    var = X.var(axis=0)    # (features,)
    
    # ì •ê·œí™”
    X_norm = (X - mean) / np.sqrt(var + eps)  # Broadcasting!
    
    # ìŠ¤ì¼€ì¼ê³¼ ì‹œí”„íŠ¸
    out = gamma * X_norm + beta  # Broadcasting!
    return out
```

## 5. Broadcastingê³¼ ë©”ëª¨ë¦¬

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì½”ë“œ
```python
# ë‚˜ìœ ì˜ˆ: ëª…ì‹œì  ë³µì œ
A = np.array([[1, 2, 3],
              [4, 5, 6]])
b = np.array([10, 20, 30])
b_repeated = np.tile(b, (2, 1))  # ë©”ëª¨ë¦¬ ë‚­ë¹„!
result = A + b_repeated

# ì¢‹ì€ ì˜ˆ: Broadcasting í™œìš©
result = A + b  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì !
```

### Broadcastingì˜ ë‚´ë¶€ ë™ì‘
```python
# Broadcastingì€ ì‹¤ì œë¡œ ë©”ëª¨ë¦¬ë¥¼ ë³µì‚¬í•˜ì§€ ì•ŠìŒ
# strideë¥¼ ì¡°ì •í•˜ì—¬ ê°€ìƒìœ¼ë¡œ í™•ì¥

a = np.array([1, 2, 3])
b = a[:, np.newaxis]  # (3, 1)ë¡œ reshape
print(b.strides)  # (8, 0) - ë‘ ë²ˆì§¸ ì°¨ì›ì€ strideê°€ 0!
```

## 6. ê³ ê¸‰ Broadcasting ê¸°ë²•

### ì™¸ì  (Outer Product)
```python
a = np.array([1, 2, 3])      # (3,)
b = np.array([4, 5, 6, 7])   # (4,)

# ì™¸ì  ê³„ì‚°
outer = a[:, np.newaxis] * b  # (3, 1) * (4,) â†’ (3, 4)
# [[4, 5, 6, 7],
#  [8, 10, 12, 14],
#  [12, 15, 18, 21]]
```

### ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
```python
def pairwise_distances(X, Y):
    """
    ë‘ ì  ì§‘í•© ê°„ì˜ ê±°ë¦¬ í–‰ë ¬
    X: (n, d), Y: (m, d)
    Returns: (n, m) ê±°ë¦¬ í–‰ë ¬
    """
    X2 = (X**2).sum(axis=1)[:, np.newaxis]  # (n, 1)
    Y2 = (Y**2).sum(axis=1)[np.newaxis, :]  # (1, m)
    XY = X @ Y.T                             # (n, m)
    
    # Broadcastingìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚°
    distances = np.sqrt(X2 + Y2 - 2*XY)
    return distances
```

## 7. Broadcasting í•¨ì •ê³¼ í•´ê²°ì±…

### í•¨ì • 1: ì˜ˆìƒì¹˜ ëª»í•œ Broadcasting
```python
# ì˜ë„: ê° í–‰ì˜ í•©
A = np.array([[1, 2], [3, 4]])
row_sums = A.sum(axis=1)  # [3, 7]

# ì‹¤ìˆ˜: shape ë¶ˆì¼ì¹˜
# A / row_sums  # Error!

# í•´ê²°: keepdims ì‚¬ìš©
row_sums = A.sum(axis=1, keepdims=True)  # [[3], [7]]
A_normalized = A / row_sums  # OK!
```

### í•¨ì • 2: ì„±ëŠ¥ ì €í•˜
```python
# ë‚˜ìœ ì˜ˆ: í° ë°°ì—´ì„ ì‘ì€ ë°°ì—´ì— ë§ì¶¤
large = np.random.randn(1000000, 10)
small = np.random.randn(10, 1)

# ì‹¤ìˆ˜: transposeë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì¬ë°°ì¹˜
result = large.T + small  # ëŠë¦¼!

# ì¢‹ì€ ì˜ˆ: Broadcasting ë°©í–¥ ê³ ë ¤
result = large + small.T  # ë¹ ë¦„!
```

## 8. ì‹¤ì „ ì˜ˆì œ: ì‹ ê²½ë§ ë ˆì´ì–´

```python
class LinearLayer:
    def __init__(self, input_dim, output_dim):
        self.W = np.random.randn(input_dim, output_dim) * 0.01
        self.b = np.zeros(output_dim)
    
    def forward(self, X):
        """
        X: (batch_size, input_dim)
        Returns: (batch_size, output_dim)
        """
        # Broadcastingìœ¼ë¡œ bias ì¶”ê°€
        return X @ self.W + self.b  # self.bê°€ ê° ìƒ˜í”Œì— broadcast
    
    def backward(self, X, dZ):
        """
        ì—­ì „íŒŒ ê³„ì‚°
        dZ: (batch_size, output_dim)
        """
        batch_size = X.shape[0]
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        self.dW = X.T @ dZ / batch_size
        self.db = dZ.sum(axis=0) / batch_size  # Broadcasting ì¤€ë¹„
        
        # ì…ë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸
        dX = dZ @ self.W.T
        return dX
```

## ğŸ’¡ Broadcasting ë§ˆìŠ¤í„° íŒ

1. **Shape ë¨¼ì € ìƒê°**: ì—°ì‚° ì „ shape í™•ì¸
2. **keepdims=True í™œìš©**: ì°¨ì› ìœ ì§€ë¡œ Broadcasting ìš©ì´
3. **newaxis í™œìš©**: ì°¨ì› ì¶”ê°€ë¡œ Broadcasting ì œì–´
4. **ì„±ëŠ¥ ê³ ë ¤**: Broadcasting ë°©í–¥ì´ ì„±ëŠ¥ì— ì˜í–¥

## ğŸ” ë””ë²„ê¹… ë„êµ¬

```python
def broadcast_shapes(a_shape, b_shape):
    """ë‘ shapeì˜ broadcast ê²°ê³¼ ì˜ˆì¸¡"""
    # ì°¨ì› ë§ì¶”ê¸°
    ndim = max(len(a_shape), len(b_shape))
    a_shape = (1,) * (ndim - len(a_shape)) + a_shape
    b_shape = (1,) * (ndim - len(b_shape)) + b_shape
    
    # Broadcasting ê·œì¹™ ì ìš©
    result_shape = []
    for a, b in zip(a_shape, b_shape):
        if a == 1:
            result_shape.append(b)
        elif b == 1:
            result_shape.append(a)
        elif a == b:
            result_shape.append(a)
        else:
            raise ValueError(f"Cannot broadcast {a_shape} with {b_shape}")
    
    return tuple(result_shape)

# í…ŒìŠ¤íŠ¸
print(broadcast_shapes((2, 3), (3,)))     # (2, 3)
print(broadcast_shapes((2, 1), (1, 3)))   # (2, 3)
```

## ğŸ“ ì—°ìŠµ ë¬¸ì œ

1. ì´ë¯¸ì§€ ë°°ì¹˜ (32, 224, 224, 3)ì—ì„œ ê° ì±„ë„ì˜ í‰ê· ì„ ë¹¼ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
2. Attention Score ê³„ì‚°: Q(n, d) @ K(m, d).Të¥¼ Broadcastingìœ¼ë¡œ êµ¬í˜„í•˜ì„¸ìš”.
3. Dropout ë§ˆìŠ¤í¬ë¥¼ Broadcastingìœ¼ë¡œ ì ìš©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ë‹¤ìŒ ë‹¨ê³„

Batch ì²˜ë¦¬ì˜ í˜ì„ ì•Œì•„ë´…ì‹œë‹¤! â†’ [03_batch_processing.md](03_batch_processing.md)