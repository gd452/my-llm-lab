# ğŸ“‰ Loss Functions: í•™ìŠµì˜ ë‚˜ì¹¨ë°˜

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ ì´í•´
- ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ êµ¬í˜„
- ì ì ˆí•œ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ

## 1. ì†ì‹¤ í•¨ìˆ˜ì˜ ì—­í• 

ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
ì´ëŠ” ìµœì í™”ì˜ ëª©í‘œê°€ ë˜ë©°, í•™ìŠµì˜ ë°©í–¥ì„ ê²°ì •í•©ë‹ˆë‹¤.

### ì¢‹ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ì¡°ê±´
1. **ë¯¸ë¶„ ê°€ëŠ¥**: ì—­ì „íŒŒë¥¼ ìœ„í•´ í•„ìˆ˜
2. **ë³¼ë¡ì„±**: ìµœì í™”ê°€ ìš©ì´ (í•­ìƒ ê°€ëŠ¥í•œ ê±´ ì•„ë‹˜)
3. **í•´ì„ ê°€ëŠ¥**: ê°’ì˜ ì˜ë¯¸ê°€ ëª…í™•
4. **ìˆ˜ì¹˜ ì•ˆì •ì„±**: ì˜¤ë²„í”Œë¡œìš°/ì–¸ë”í”Œë¡œìš° ë°©ì§€

## 2. íšŒê·€ ì†ì‹¤ í•¨ìˆ˜

### Mean Squared Error (MSE)
```python
def mse_loss(y_pred, y_true):
    """
    í‰ê·  ì œê³± ì˜¤ì°¨
    y_pred, y_true: (batch_size, output_dim)
    """
    return np.mean((y_pred - y_true) ** 2)

def mse_grad(y_pred, y_true):
    """MSEì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    batch_size = y_pred.shape[0]
    return 2 * (y_pred - y_true) / batch_size
```

### Mean Absolute Error (MAE)
```python
def mae_loss(y_pred, y_true):
    """
    í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (ì´ìƒì¹˜ì— ê°•ê±´)
    """
    return np.mean(np.abs(y_pred - y_true))

def mae_grad(y_pred, y_true):
    """MAEì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    batch_size = y_pred.shape[0]
    return np.sign(y_pred - y_true) / batch_size
```

### Huber Loss
```python
def huber_loss(y_pred, y_true, delta=1.0):
    """
    Huber Loss: MSEì™€ MAEì˜ ì¥ì  ê²°í•©
    ì‘ì€ ì˜¤ì°¨ì—ëŠ” MSE, í° ì˜¤ì°¨ì—ëŠ” MAE
    """
    error = y_pred - y_true
    is_small_error = np.abs(error) <= delta
    
    small_error_loss = 0.5 * error ** 2
    large_error_loss = delta * (np.abs(error) - 0.5 * delta)
    
    return np.mean(np.where(is_small_error, small_error_loss, large_error_loss))

def huber_grad(y_pred, y_true, delta=1.0):
    """Huber Lossì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    error = y_pred - y_true
    batch_size = y_pred.shape[0]
    
    grad = np.where(
        np.abs(error) <= delta,
        error,  # MSE ë¶€ë¶„
        delta * np.sign(error)  # MAE ë¶€ë¶„
    )
    return grad / batch_size
```

## 3. ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜

### Cross Entropy Loss
```python
def cross_entropy_loss(y_pred, y_true, eps=1e-8):
    """
    Cross Entropy Loss (ì•ˆì •ì  êµ¬í˜„)
    y_pred: (batch_size, num_classes) - probabilities
    y_true: (batch_size,) - class indices
    """
    batch_size = y_pred.shape[0]
    
    # Clip to prevent log(0)
    y_pred = np.clip(y_pred, eps, 1 - eps)
    
    # ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ë§Œ ì„ íƒ
    correct_log_probs = -np.log(y_pred[np.arange(batch_size), y_true])
    
    return np.mean(correct_log_probs)

def cross_entropy_grad(y_pred, y_true):
    """
    Cross Entropyì˜ ê·¸ë˜ë””ì–¸íŠ¸ (Softmax ì¶œë ¥ ê°€ì •)
    """
    batch_size = y_pred.shape[0]
    grad = y_pred.copy()
    grad[np.arange(batch_size), y_true] -= 1
    return grad / batch_size
```

### Binary Cross Entropy
```python
def binary_cross_entropy(y_pred, y_true, eps=1e-8):
    """
    ì´ì§„ ë¶„ë¥˜ìš© Cross Entropy
    y_pred, y_true: (batch_size,)
    """
    y_pred = np.clip(y_pred, eps, 1 - eps)
    
    loss = -(y_true * np.log(y_pred) + 
             (1 - y_true) * np.log(1 - y_pred))
    
    return np.mean(loss)

def binary_cross_entropy_grad(y_pred, y_true, eps=1e-8):
    """BCEì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    y_pred = np.clip(y_pred, eps, 1 - eps)
    batch_size = len(y_pred)
    
    grad = -(y_true / y_pred - (1 - y_true) / (1 - y_pred))
    return grad / batch_size
```

### Focal Loss
```python
def focal_loss(y_pred, y_true, gamma=2.0, alpha=0.25, eps=1e-8):
    """
    Focal Loss: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°
    ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘
    """
    batch_size = y_pred.shape[0]
    num_classes = y_pred.shape[1]
    
    # One-hot encoding
    y_true_one_hot = np.zeros_like(y_pred)
    y_true_one_hot[np.arange(batch_size), y_true] = 1
    
    # Clip predictions
    y_pred = np.clip(y_pred, eps, 1 - eps)
    
    # Focal loss ê³„ì‚°
    ce = -y_true_one_hot * np.log(y_pred)
    focal_weight = (1 - y_pred) ** gamma
    fl = alpha * focal_weight * ce
    
    return np.mean(np.sum(fl, axis=1))
```

## 4. ìˆ˜ì¹˜ ì•ˆì •ì„± ê¸°ë²•

### LogSumExp Trick
```python
def logsumexp(x, axis=None):
    """
    ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ log(sum(exp(x)))
    """
    x_max = np.max(x, axis=axis, keepdims=True)
    return x_max + np.log(np.sum(np.exp(x - x_max), axis=axis, keepdims=True))

def stable_softmax_cross_entropy(logits, y_true):
    """
    Softmax + Cross Entropyë¥¼ í•œ ë²ˆì— (ì•ˆì •ì )
    logits: (batch_size, num_classes) - raw scores
    """
    batch_size = logits.shape[0]
    
    # LogSumExp trick
    log_probs = logits - logsumexp(logits, axis=1)
    
    # Cross entropy
    loss = -log_probs[np.arange(batch_size), y_true]
    
    return np.mean(loss)
```

### Gradient Clipping
```python
def clip_gradients(gradients, max_norm=1.0):
    """
    ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (í­ë°œ ë°©ì§€)
    """
    total_norm = np.sqrt(sum(np.sum(g ** 2) for g in gradients))
    
    if total_norm > max_norm:
        scale = max_norm / total_norm
        gradients = [g * scale for g in gradients]
    
    return gradients
```

## 5. ì •ê·œí™” ì†ì‹¤

### L2 Regularization
```python
def l2_regularization(params, lambda_reg=0.01):
    """
    L2 ì •ê·œí™” (Weight Decay)
    """
    l2_loss = 0
    for param in params:
        l2_loss += np.sum(param ** 2)
    
    return lambda_reg * l2_loss

def l2_regularization_grad(params, lambda_reg=0.01):
    """L2 ì •ê·œí™”ì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    return [2 * lambda_reg * param for param in params]
```

### L1 Regularization
```python
def l1_regularization(params, lambda_reg=0.01):
    """
    L1 ì •ê·œí™” (Sparsity ìœ ë„)
    """
    l1_loss = 0
    for param in params:
        l1_loss += np.sum(np.abs(param))
    
    return lambda_reg * l1_loss

def l1_regularization_grad(params, lambda_reg=0.01):
    """L1 ì •ê·œí™”ì˜ ê·¸ë˜ë””ì–¸íŠ¸"""
    return [lambda_reg * np.sign(param) for param in params]
```

## 6. ê³ ê¸‰ ì†ì‹¤ í•¨ìˆ˜

### Contrastive Loss
```python
def contrastive_loss(embeddings, labels, margin=1.0):
    """
    Contrastive Loss: ìœ ì‚¬ë„ í•™ìŠµ
    embeddings: (batch_size, embedding_dim)
    labels: (batch_size,) - 0: different, 1: similar
    """
    # Pairwise distances
    distances = pairwise_distances(embeddings)
    
    # Contrastive loss
    similar_loss = labels * distances ** 2
    dissimilar_loss = (1 - labels) * np.maximum(0, margin - distances) ** 2
    
    return np.mean(similar_loss + dissimilar_loss)
```

### Triplet Loss
```python
def triplet_loss(anchor, positive, negative, margin=1.0):
    """
    Triplet Loss: ìˆœìœ„ í•™ìŠµ
    anchor: (batch_size, embedding_dim)
    positive: (batch_size, embedding_dim) - ê°™ì€ í´ë˜ìŠ¤
    negative: (batch_size, embedding_dim) - ë‹¤ë¥¸ í´ë˜ìŠ¤
    """
    # ê±°ë¦¬ ê³„ì‚°
    pos_dist = np.sum((anchor - positive) ** 2, axis=1)
    neg_dist = np.sum((anchor - negative) ** 2, axis=1)
    
    # Triplet loss
    loss = np.maximum(0, pos_dist - neg_dist + margin)
    
    return np.mean(loss)
```

## 7. ì†ì‹¤ í•¨ìˆ˜ ì¡°í•©

```python
class CombinedLoss:
    """ì—¬ëŸ¬ ì†ì‹¤ í•¨ìˆ˜ ì¡°í•©"""
    
    def __init__(self, losses, weights=None):
        self.losses = losses
        self.weights = weights or [1.0] * len(losses)
    
    def __call__(self, y_pred, y_true):
        total_loss = 0
        
        for loss_fn, weight in zip(self.losses, self.weights):
            total_loss += weight * loss_fn(y_pred, y_true)
        
        return total_loss

# ì‚¬ìš© ì˜ˆ
combined_loss = CombinedLoss(
    losses=[cross_entropy_loss, l2_regularization],
    weights=[1.0, 0.01]
)
```

## 8. ì‹¤ì „ ì˜ˆì œ: ìë™ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ

```python
class AdaptiveLoss:
    """íƒœìŠ¤í¬ì— ë§ëŠ” ì†ì‹¤ í•¨ìˆ˜ ìë™ ì„ íƒ"""
    
    def __init__(self, task_type='classification', num_classes=None):
        self.task_type = task_type
        self.num_classes = num_classes
        
        if task_type == 'classification':
            if num_classes == 2:
                self.loss_fn = binary_cross_entropy
            else:
                self.loss_fn = cross_entropy_loss
        elif task_type == 'regression':
            self.loss_fn = mse_loss
        elif task_type == 'ranking':
            self.loss_fn = triplet_loss
        else:
            raise ValueError(f"Unknown task type: {task_type}")
    
    def __call__(self, y_pred, y_true):
        return self.loss_fn(y_pred, y_true)
    
    def get_info(self):
        """ì†ì‹¤ í•¨ìˆ˜ ì •ë³´ ë°˜í™˜"""
        return {
            'task_type': self.task_type,
            'loss_function': self.loss_fn.__name__,
            'num_classes': self.num_classes
        }
```

## ğŸ’¡ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ

| íƒœìŠ¤í¬ | ì¶”ì²œ ì†ì‹¤ í•¨ìˆ˜ | ì´ìœ  |
|--------|---------------|------|
| ì´ì§„ ë¶„ë¥˜ | BCE | í™•ë¥  í•´ì„ ê°€ëŠ¥ |
| ë‹¤ì¤‘ ë¶„ë¥˜ | Cross Entropy | í´ë˜ìŠ¤ ê°„ ê²½ìŸ |
| íšŒê·€ | MSE | ë¯¸ë¶„ ìš©ì´ |
| ì´ìƒì¹˜ ìˆëŠ” íšŒê·€ | Huber | ê°•ê±´ì„± |
| í´ë˜ìŠ¤ ë¶ˆê· í˜• | Focal Loss | ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ |
| ì„ë² ë”© í•™ìŠµ | Triplet Loss | ìƒëŒ€ì  ê±°ë¦¬ |

## ğŸ” ë””ë²„ê¹… íŒ

```python
def debug_loss(loss_fn, y_pred, y_true):
    """ì†ì‹¤ í•¨ìˆ˜ ë””ë²„ê¹…"""
    loss = loss_fn(y_pred, y_true)
    
    print(f"Loss value: {loss:.6f}")
    print(f"Is NaN: {np.isnan(loss)}")
    print(f"Is Inf: {np.isinf(loss)}")
    
    if hasattr(loss_fn, '__name__'):
        print(f"Loss function: {loss_fn.__name__}")
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬
    if loss_fn.__name__.endswith('_loss'):
        grad_fn_name = loss_fn.__name__.replace('_loss', '_grad')
        if grad_fn_name in globals():
            grad = globals()[grad_fn_name](y_pred, y_true)
            print(f"Gradient norm: {np.linalg.norm(grad):.6f}")
```

## ğŸ“ ì—°ìŠµ ë¬¸ì œ

1. KL Divergence ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
2. Smooth L1 Lossë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
3. Label Smoothingì„ í¬í•¨í•œ Cross Entropyë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

Day 2ì˜ ëª¨ë“  í•™ìŠµ ìë£Œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!
ì´ì œ ë²¡í„° ì—°ì‚°ì˜ í˜ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„: Day 3 - Attention Mechanismìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”!