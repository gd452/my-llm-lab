{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Complete Transformer Architecture Implementation\n",
    "\n",
    "이 노트북에서는 완전한 Transformer 아키텍처를 단계별로 구현합니다.\n",
    "- Layer Normalization\n",
    "- Position-wise Feed-Forward Network\n",
    "- Transformer Block (Encoder & Decoder)\n",
    "- Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer Normalization 구현\n",
    "\n",
    "Layer Normalization은 각 샘플 내에서 feature dimension을 따라 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer Normalization implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "# Test Layer Normalization\n",
    "batch_size, seq_len, d_model = 2, 3, 4\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "layer_norm = LayerNorm(d_model)\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output mean: {output.mean(dim=-1)}\")\n",
    "print(f\"Output std: {output.std(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Position-wise Feed-Forward Network 구현\n",
    "\n",
    "각 position에 독립적으로 적용되는 2-layer feed-forward network입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1, activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation function selection\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        hidden = self.w_1(x)  # [batch_size, seq_len, d_ff]\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.w_2(hidden)  # [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "\n",
    "# Test FFN\n",
    "d_model, d_ff = 512, 2048\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = ffn(x)\n",
    "print(f\"FFN Input shape: {x.shape}\")\n",
    "print(f\"FFN Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention (Day 3 복습)\n",
    "\n",
    "Day 3에서 배운 Multi-Head Attention을 다시 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # 1. Linear projections in batch from d_model => h x d_k\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 3. Apply attention to values\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "d_model, n_heads = 512, 8\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = mha(x, x, x)\n",
    "print(f\"MHA Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding\n",
    "\n",
    "Transformer는 순서 정보가 없으므로 Positional Encoding을 추가해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding using sinusoidal functions\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "# Visualize Positional Encoding\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Get the positional encoding matrix\n",
    "pe_matrix = pos_encoding.pe[0, :max_len, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_matrix, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Encoder Block\n",
    "\n",
    "이제 모든 구성 요소를 결합하여 Encoder Block을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-Attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # FFN with residual connection and layer norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test Encoder Block\n",
    "encoder_block = EncoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = encoder_block(x)\n",
    "print(f\"Encoder Block Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Decoder Block\n",
    "\n",
    "Decoder Block은 Encoder Block에 Cross-Attention이 추가됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Decoder Block\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Masked Self-Attention\n",
    "        self.masked_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Cross-Attention\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        # Masked Self-Attention\n",
    "        masked_attn_output = self.masked_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(masked_attn_output))\n",
    "        \n",
    "        # Cross-Attention with encoder output\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test Decoder Block\n",
    "decoder_block = DecoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
    "decoder_input = torch.randn(2, 10, 512)\n",
    "encoder_output = torch.randn(2, 15, 512)\n",
    "output = decoder_block(decoder_input, encoder_output)\n",
    "print(f\"Decoder Block Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Transformer Model\n",
    "\n",
    "모든 구성 요소를 결합하여 완전한 Transformer 모델을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer Model\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int = 512,\n",
    "                 n_heads: int = 8,\n",
    "                 n_encoder_layers: int = 6,\n",
    "                 n_decoder_layers: int = 6,\n",
    "                 d_ff: int = 2048,\n",
    "                 max_len: int = 5000,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings and Positional Encoding\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def generate_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"Generate causal mask for decoder\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, \n",
    "                src: torch.Tensor,\n",
    "                tgt: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \n",
    "        # Source embedding\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(src_emb.size(-1))\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        \n",
    "        # Target embedding\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(tgt_emb.size(-1))\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output = src_emb\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output = encoder_layer(encoder_output, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_output = tgt_emb\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_output = decoder_layer(decoder_output, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(decoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model instance\n",
    "model = Transformer(\n",
    "    src_vocab_size=10000,\n",
    "    tgt_vocab_size=10000,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_encoder_layers=6,\n",
    "    n_decoder_layers=6,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Example\n",
    "\n",
    "간단한 학습 예제를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module, \n",
    "               src: torch.Tensor,\n",
    "               tgt: torch.Tensor,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               criterion: nn.Module) -> float:\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Prepare target input and output\n",
    "    tgt_input = tgt[:, :-1]  # All tokens except last\n",
    "    tgt_output = tgt[:, 1:]  # All tokens except first\n",
    "    \n",
    "    # Generate target mask\n",
    "    tgt_mask = model.generate_mask(tgt_input.size(1)).to(tgt_input.device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "    \n",
    "    # Calculate loss\n",
    "    output = output.reshape(-1, output.size(-1))\n",
    "    tgt_output = tgt_output.reshape(-1)\n",
    "    loss = criterion(output, tgt_output)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Create dummy data for demonstration\n",
    "batch_size = 8\n",
    "src_seq_len = 20\n",
    "tgt_seq_len = 20\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "\n",
    "# Random source and target sequences\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# Setup training\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=256,  # Smaller for demo\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=1024\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "\n",
    "# Training loop (simplified)\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_step(model, src.to(device), tgt.to(device), optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Example\n",
    "\n",
    "추론 시 auto-regressive 생성을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model: nn.Module,\n",
    "             src: torch.Tensor,\n",
    "             max_len: int = 50,\n",
    "             start_token: int = 1,\n",
    "             end_token: int = 2) -> torch.Tensor:\n",
    "    \"\"\"Generate output sequence using greedy decoding\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Encode source\n",
    "    src_emb = model.src_embedding(src) * math.sqrt(model.src_embedding.embedding_dim)\n",
    "    src_emb = model.positional_encoding(src_emb)\n",
    "    \n",
    "    encoder_output = src_emb\n",
    "    for encoder_layer in model.encoder_layers:\n",
    "        encoder_output = encoder_layer(encoder_output)\n",
    "    \n",
    "    # Start with start token\n",
    "    tgt = torch.tensor([[start_token]], device=device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Decode\n",
    "        tgt_emb = model.tgt_embedding(tgt) * math.sqrt(model.tgt_embedding.embedding_dim)\n",
    "        tgt_emb = model.positional_encoding(tgt_emb)\n",
    "        \n",
    "        # Generate target mask\n",
    "        tgt_mask = model.generate_mask(tgt.size(1)).to(device)\n",
    "        \n",
    "        decoder_output = tgt_emb\n",
    "        for decoder_layer in model.decoder_layers:\n",
    "            decoder_output = decoder_layer(decoder_output, encoder_output, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Get next token\n",
    "        output = model.output_projection(decoder_output)\n",
    "        next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Append to target sequence\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        # Stop if end token is generated\n",
    "        if next_token.item() == end_token:\n",
    "            break\n",
    "    \n",
    "    return tgt\n",
    "\n",
    "# Test generation\n",
    "src_sequence = torch.randint(0, src_vocab_size, (1, 10)).to(device)\n",
    "generated = generate(model, src_sequence)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")\n",
    "print(f\"Generated tokens: {generated[0].tolist()[:20]}...\")  # Show first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Attention Visualization\n",
    "\n",
    "Attention weights를 시각화하여 모델이 어떻게 작동하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model: nn.Module, src: torch.Tensor, tgt: torch.Tensor):\n",
    "    \"\"\"Visualize attention weights\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Hook을 사용하여 attention weights 추출\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        attention_weights.append(output)\n",
    "    \n",
    "    # Register hooks on attention layers\n",
    "    hooks = []\n",
    "    for layer in model.decoder_layers:\n",
    "        hook = layer.cross_attention.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(src, tgt)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Visualize attention from first decoder layer\n",
    "    if attention_weights:\n",
    "        attn = attention_weights[0].cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(attn[0, 0], cmap='Blues', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Source Position')\n",
    "        plt.ylabel('Target Position')\n",
    "        plt.title('Cross-Attention Weights (Layer 1, Head 1)')\n",
    "        plt.show()\n",
    "\n",
    "# Note: This visualization would require actual attention weights\n",
    "# which are not directly returned in our current implementation\n",
    "print(\"Attention visualization would require modifying the attention module to return weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Analysis\n",
    "\n",
    "모델의 다양한 측면을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model: nn.Module):\n",
    "    \"\"\"Analyze model architecture and parameters\"\"\"\n",
    "    \n",
    "    # Parameter count by component\n",
    "    component_params = {\n",
    "        'Embeddings': 0,\n",
    "        'Encoder': 0,\n",
    "        'Decoder': 0,\n",
    "        'Output': 0\n",
    "    }\n",
    "    \n",
    "    # Count embedding parameters\n",
    "    component_params['Embeddings'] = (\n",
    "        sum(p.numel() for p in model.src_embedding.parameters()) +\n",
    "        sum(p.numel() for p in model.tgt_embedding.parameters())\n",
    "    )\n",
    "    \n",
    "    # Count encoder parameters\n",
    "    for layer in model.encoder_layers:\n",
    "        component_params['Encoder'] += sum(p.numel() for p in layer.parameters())\n",
    "    \n",
    "    # Count decoder parameters\n",
    "    for layer in model.decoder_layers:\n",
    "        component_params['Decoder'] += sum(p.numel() for p in layer.parameters())\n",
    "    \n",
    "    # Count output projection parameters\n",
    "    component_params['Output'] = sum(p.numel() for p in model.output_projection.parameters())\n",
    "    \n",
    "    # Display results\n",
    "    total_params = sum(component_params.values())\n",
    "    \n",
    "    print(\"\\n=== Model Parameter Analysis ===\")\n",
    "    print(f\"Total Parameters: {total_params:,}\\n\")\n",
    "    \n",
    "    for component, count in component_params.items():\n",
    "        percentage = (count / total_params) * 100\n",
    "        print(f\"{component:12s}: {count:10,} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Memory estimation\n",
    "    param_size_mb = (total_params * 4) / (1024 * 1024)  # 4 bytes per float32\n",
    "    print(f\"\\nEstimated Model Size: {param_size_mb:.1f} MB (float32)\")\n",
    "    \n",
    "    # Computational complexity\n",
    "    print(\"\\n=== Computational Complexity ===\")\n",
    "    batch_size = 32\n",
    "    seq_len = 100\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    # Attention complexity: O(n²·d)\n",
    "    attention_flops = batch_size * seq_len * seq_len * d_model\n",
    "    print(f\"Attention FLOPs per layer: {attention_flops:,}\")\n",
    "    \n",
    "    # FFN complexity: O(n·d·4d)\n",
    "    ffn_flops = batch_size * seq_len * d_model * 4 * d_model\n",
    "    print(f\"FFN FLOPs per layer: {ffn_flops:,}\")\n",
    "\n",
    "# Analyze the model\n",
    "analyze_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 실습 과제\n",
    "\n",
    "다음 과제들을 수행해보세요:\n",
    "\n",
    "1. **Pre-Norm vs Post-Norm**: Layer Normalization 위치를 바꿔보고 성능 비교\n",
    "2. **Activation Functions**: ReLU, GELU, SwiGLU 등 다양한 활성화 함수 비교\n",
    "3. **Hidden Dimension**: FFN의 hidden dimension 비율 실험 (2x, 4x, 8x)\n",
    "4. **Depth vs Width**: Layer 수와 dimension 크기의 trade-off 분석\n",
    "5. **Attention Patterns**: 다양한 attention mask 패턴 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 과제 예제: Pre-Norm Transformer Block\n",
    "class PreNormEncoderBlock(nn.Module):\n",
    "    \"\"\"Pre-Norm version of Transformer Encoder Block\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Pre-Norm for attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output = self.attention(x_norm, x_norm, x_norm, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        \n",
    "        # Pre-Norm for FFN\n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + self.dropout2(ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Pre-Norm Encoder Block 구현 완료!\")\n",
    "print(\"이제 Post-Norm과 Pre-Norm의 성능을 비교해보세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "이 노트북에서 우리는:\n",
    "1. Layer Normalization과 Residual Connection의 중요성을 이해했습니다\n",
    "2. Position-wise Feed-Forward Network를 구현했습니다\n",
    "3. 완전한 Transformer Encoder/Decoder Block을 만들었습니다\n",
    "4. 전체 Transformer 모델을 조립했습니다\n",
    "5. Training과 Inference 과정을 구현했습니다\n",
    "\n",
    "다음 단계 (Day 5)에서는 GPT 스타일의 Decoder-only 모델과 텍스트 생성을 다룰 예정입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}