"""
ğŸ§ª Neural Network ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

ê° êµ¬ì„± ìš”ì†Œê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import pytest
import math

# ê¹”ë”í•œ import!
from core.autograd import Value
from core.neuron import Neuron
from core.layer import Layer
from core.mlp import MLP
from core.losses import mse_loss
from core.optimizer import SGD


class TestNeuron:
    """Neuron í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: Neuron êµ¬í˜„ í•„ìš”")
    def test_neuron_creation(self):
        """ë‰´ëŸ° ìƒì„± í…ŒìŠ¤íŠ¸"""
        neuron = Neuron(3)
        assert len(neuron.w) == 3
        assert neuron.b is not None
        assert len(neuron.parameters()) == 4  # 3 weights + 1 bias
    
    @pytest.mark.skip(reason="TODO: Neuron êµ¬í˜„ í•„ìš”")
    def test_neuron_forward(self):
        """ë‰´ëŸ° forward pass í…ŒìŠ¤íŠ¸"""
        neuron = Neuron(2)
        x = [Value(1.0), Value(2.0)]
        output = neuron(x)
        
        assert isinstance(output, Value)
        assert -1 <= output.data <= 1  # tanh ì¶œë ¥ ë²”ìœ„
    
    @pytest.mark.skip(reason="TODO: Neuron êµ¬í˜„ í•„ìš”")
    def test_neuron_linear(self):
        """ì„ í˜• ë‰´ëŸ° í…ŒìŠ¤íŠ¸"""
        neuron = Neuron(2, nonlin=False)
        x = [Value(1.0), Value(1.0)]
        
        # ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
        neuron.w[0].data = 0.5
        neuron.w[1].data = 0.5
        neuron.b.data = 0.0
        
        output = neuron(x)
        assert abs(output.data - 1.0) < 1e-6  # 0.5*1 + 0.5*1 + 0 = 1


class TestLayer:
    """Layer í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: Layer êµ¬í˜„ í•„ìš”")
    def test_layer_creation(self):
        """ë ˆì´ì–´ ìƒì„± í…ŒìŠ¤íŠ¸"""
        layer = Layer(3, 2)
        assert len(layer.neurons) == 2
        assert len(layer.parameters()) == 8  # (3+1)*2 = 8
    
    @pytest.mark.skip(reason="TODO: Layer êµ¬í˜„ í•„ìš”")
    def test_layer_forward(self):
        """ë ˆì´ì–´ forward pass í…ŒìŠ¤íŠ¸"""
        layer = Layer(2, 3)
        x = [Value(1.0), Value(2.0)]
        outputs = layer(x)
        
        assert len(outputs) == 3
        assert all(isinstance(o, Value) for o in outputs)


class TestMLP:
    """MLP í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: MLP êµ¬í˜„ í•„ìš”")
    def test_mlp_creation(self):
        """MLP ìƒì„± í…ŒìŠ¤íŠ¸"""
        mlp = MLP(2, [4, 1])
        assert len(mlp.layers) == 2
        assert mlp.architecture == [2, 4, 1]
    
    @pytest.mark.skip(reason="TODO: MLP êµ¬í˜„ í•„ìš”")
    def test_mlp_forward(self):
        """MLP forward pass í…ŒìŠ¤íŠ¸"""
        mlp = MLP(2, [3, 1])
        x = [Value(0.5), Value(0.5)]
        output = mlp(x)
        
        assert isinstance(output, Value)
    
    @pytest.mark.skip(reason="TODO: MLP êµ¬í˜„ í•„ìš”")
    def test_mlp_parameters(self):
        """MLP íŒŒë¼ë¯¸í„° ê°œìˆ˜ í…ŒìŠ¤íŠ¸"""
        mlp = MLP(2, [4, 1])
        # ì²« ë²ˆì§¸ ì¸µ: (2+1)*4 = 12
        # ë‘ ë²ˆì§¸ ì¸µ: (4+1)*1 = 5
        # ì´: 17
        assert len(mlp.parameters()) == 17


class TestLosses:
    """ì†ì‹¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: losses êµ¬í˜„ í•„ìš”")
    def test_mse_loss(self):
        """MSE ì†ì‹¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        predictions = [Value(0.5), Value(0.8)]
        targets = [Value(0.0), Value(1.0)]
        
        loss = mse_loss(predictions, targets)
        
        # MSE = ((0.5-0)^2 + (0.8-1)^2) / 2 = (0.25 + 0.04) / 2 = 0.145
        expected = 0.145
        assert abs(loss.data - expected) < 1e-6
    
    @pytest.mark.skip(reason="TODO: losses êµ¬í˜„ í•„ìš”")
    def test_mse_gradient(self):
        """MSE gradient í…ŒìŠ¤íŠ¸"""
        pred = Value(0.5)
        target = Value(0.0)
        
        loss = mse_loss([pred], [target])
        loss.backward()
        
        # d(MSE)/d(pred) = 2*(pred - target) / n = 2*0.5 / 1 = 1.0
        assert abs(pred.grad - 1.0) < 1e-6


class TestOptimizer:
    """ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: optimizer êµ¬í˜„ í•„ìš”")
    def test_sgd_step(self):
        """SGD step í…ŒìŠ¤íŠ¸"""
        params = [Value(1.0), Value(2.0)]
        params[0].grad = 0.1
        params[1].grad = -0.2
        
        optimizer = SGD(params, lr=0.1)
        optimizer.step()
        
        # param = param - lr * grad
        assert abs(params[0].data - 0.99) < 1e-6  # 1.0 - 0.1*0.1
        assert abs(params[1].data - 2.02) < 1e-6  # 2.0 - 0.1*(-0.2)
    
    @pytest.mark.skip(reason="TODO: optimizer êµ¬í˜„ í•„ìš”")
    def test_sgd_zero_grad(self):
        """SGD zero_grad í…ŒìŠ¤íŠ¸"""
        params = [Value(1.0), Value(2.0)]
        params[0].grad = 0.5
        params[1].grad = 0.5
        
        optimizer = SGD(params)
        optimizer.zero_grad()
        
        assert all(p.grad == 0.0 for p in params)


class TestIntegration:
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.skip(reason="TODO: ì „ì²´ êµ¬í˜„ í•„ìš”")
    def test_xor_learning(self):
        """XOR í•™ìŠµ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸"""
        # ê°„ë‹¨í•œ XOR í•™ìŠµ í…ŒìŠ¤íŠ¸
        X = [[0, 0], [0, 1], [1, 0], [1, 1]]
        y = [0, 1, 1, 0]
        
        model = MLP(2, [4, 1])
        optimizer = SGD(model.parameters(), lr=0.1)
        
        # 100 epoch í•™ìŠµ
        for _ in range(100):
            loss_val = 0
            
            for inputs, target in zip(X, y):
                x_vals = [Value(x) for x in inputs]
                pred = model(x_vals)
                loss = (pred - Value(target)) ** 2
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                loss_val += loss.data
        
        # í•™ìŠµ í›„ lossê°€ ê°ì†Œí–ˆëŠ”ì§€ í™•ì¸
        assert loss_val / 4 < 0.1  # í‰ê·  loss < 0.1


if __name__ == "__main__":
    pytest.main([__file__, "-v"])