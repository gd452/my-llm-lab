{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Attention Mechanism Tutorial\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ê³  ì‹¤í—˜í•©ë‹ˆë‹¤.\n",
    "\"Attention is All You Need\" ë…¼ë¬¸ì˜ í•µì‹¬ ê°œë…ì„ ì§ì ‘ ì½”ë”©í•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('.')))))\n",
    "\n",
    "# core ëª¨ë“ˆ import\n",
    "from core.attention import (\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    "    positional_encoding,\n",
    "    add_positional_encoding,\n",
    "    create_causal_mask,\n",
    "    visualize_attention\n",
    ")\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attentionì˜ ì§ê´€ì  ì´í•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionì˜ í•µì‹¬: Query, Key, Value\n",
    "print(\"ğŸ” Attentionì˜ í•µì‹¬ ê°œë…\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"ğŸ“ ë¹„ìœ : ë„ì„œê´€ì—ì„œ ì±… ì°¾ê¸°\")\n",
    "print(\"  Query (ì§ˆë¬¸): 'íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì±…ì„ ì°¾ê³  ìˆì–´ìš”'\")\n",
    "print(\"  Key (ìƒ‰ì¸): ê° ì±…ì˜ ì œëª©ê³¼ ì£¼ì œ\")\n",
    "print(\"  Value (ë‚´ìš©): ì‹¤ì œ ì±…ì˜ ë‚´ìš©\")\n",
    "print()\n",
    "print(\"â†’ Attentionì€ Queryì™€ ê°€ì¥ ê´€ë ¨ìˆëŠ” Keyë¥¼ ì°¾ì•„\")\n",
    "print(\"  í•´ë‹¹í•˜ëŠ” Valueë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.\")\n",
    "print()\n",
    "print(\"ìˆ˜ì‹: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ì˜ˆì œë¡œ ì‹œì‘\n",
    "print(\"ğŸ“Š Simple Attention Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 3ê°œì˜ ë‹¨ì–´, ê° 4ì°¨ì› ë²¡í„°\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Query, Key, Value ìƒì„±\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "# Attention ê³„ì‚°\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nì¶œë ¥ shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights:\\n{attention_weights}\")\n",
    "print(f\"\\nê° í–‰ì˜ í•© (should be 1): {attention_weights.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention ê³„ì‚° ë‹¨ê³„ë³„ ë¶„í•´\n",
    "print(\"ğŸ”§ Attention ê³„ì‚° ë‹¨ê³„ë³„ ë¶„í•´\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Qì™€ Kì˜ ë‚´ì \n",
    "scores = Q @ K.T\n",
    "print(\"Step 1 - Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\\n\")\n",
    "\n",
    "# Step 2: Scaling\n",
    "d_k = K.shape[-1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "print(f\"Step 2 - Scaled scores (Ã·âˆš{d_k}):\")\n",
    "print(scaled_scores)\n",
    "print()\n",
    "\n",
    "# Step 3: Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights_manual = softmax(scaled_scores)\n",
    "print(\"Step 3 - Attention weights (softmax):\")\n",
    "print(attention_weights_manual)\n",
    "print()\n",
    "\n",
    "# Step 4: ê°€ì¤‘í•©\n",
    "output_manual = attention_weights_manual @ V\n",
    "print(\"Step 4 - Output (weighted sum of V):\")\n",
    "print(output_manual)\n",
    "\n",
    "# ê²€ì¦\n",
    "print(f\"\\nâœ… ìˆ˜ë™ ê³„ì‚°ê³¼ í•¨ìˆ˜ ê²°ê³¼ ë™ì¼: {np.allclose(output, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ì—ì„œ Self-Attention\n",
    "print(\"ğŸ“ ë¬¸ì¥ì—ì„œ Self-Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ê°„ë‹¨í•œ ë¬¸ì¥\n",
    "sentence = \"The cat sat\"\n",
    "tokens = sentence.split()\n",
    "print(f\"ë¬¸ì¥: '{sentence}'\")\n",
    "print(f\"í† í°: {tokens}\\n\")\n",
    "\n",
    "# ê° ë‹¨ì–´ë¥¼ ì„ì˜ì˜ ë²¡í„°ë¡œ í‘œí˜„\n",
    "np.random.seed(42)\n",
    "word_embeddings = {\n",
    "    \"The\": np.random.randn(8),\n",
    "    \"cat\": np.random.randn(8),\n",
    "    \"sat\": np.random.randn(8)\n",
    "}\n",
    "\n",
    "# ì„ë² ë”© í–‰ë ¬ ìƒì„±\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "print(f\"ì„ë² ë”© shape: {X.shape}\")\n",
    "\n",
    "# Self-attention (Q=K=V=X)\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X)\n",
    "\n",
    "# Attention ì‹œê°í™”\n",
    "print(\"\\nAttention Matrix:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        weight = attention_weights[i, j]\n",
    "        print(f\"{weight:8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# í•´ì„\n",
    "print(\"\\nğŸ’¡ í•´ì„:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    max_idx = np.argmax(attention_weights[i])\n",
    "    print(f\"  '{token}'ì´ ê°€ì¥ ì£¼ëª©í•˜ëŠ” ë‹¨ì–´: '{tokens[max_idx]}' \"\n",
    "          f\"(weight: {attention_weights[i, max_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal Attention (GPT ìŠ¤íƒ€ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal mask ìƒì„± ë° ì ìš©\n",
    "print(\"ğŸ”® Causal Attention (ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë” ê¸´ ë¬¸ì¥\n",
    "sentence = \"I think therefore I am\"\n",
    "tokens = sentence.split()\n",
    "seq_len = len(tokens)\n",
    "print(f\"ë¬¸ì¥: '{sentence}'\")\n",
    "print(f\"í† í°: {tokens}\\n\")\n",
    "\n",
    "# ì„ë² ë”©\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(seq_len, 16)\n",
    "\n",
    "# Causal mask ìƒì„±\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "print()\n",
    "\n",
    "# Causal attention ì ìš©\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X, mask=causal_mask)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "print(\"Causal Attention Weights:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>10}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(seq_len):\n",
    "        weight = attention_weights[i, j]\n",
    "        if weight < 0.001:\n",
    "            print(\"         -\", end=\"\")\n",
    "        else:\n",
    "            print(f\"{weight:10.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ íŠ¹ì§•:\")\n",
    "print(\"  - í•˜ì‚¼ê° í–‰ë ¬ í˜•íƒœ\")\n",
    "print(\"  - ê° í† í°ì€ ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\")\n",
    "print(\"  - GPTì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì—ì„œ ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention ì‹¤ìŠµ\n",
    "print(\"ğŸ­ Multi-Head Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ì„¤ì •\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {d_model // num_heads}\\n\")\n",
    "\n",
    "# Multi-Head Attention ìƒì„±\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "print(f\"ì…ë ¥ shape: {X.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attention_weights = mha.forward(X, X, X)\n",
    "\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  (num_heads, seq_len, seq_len)\\n\")\n",
    "\n",
    "# ê° headì˜ attention íŒ¨í„´ ë¶„ì„\n",
    "print(\"ê° Headì˜ íŠ¹ì„±:\")\n",
    "for head_idx in range(num_heads):\n",
    "    head_weights = attention_weights[head_idx]\n",
    "    \n",
    "    # ëŒ€ê°ì„  ê°•ë„ (ìê¸° ìì‹ ì— ëŒ€í•œ attention)\n",
    "    diagonal_strength = np.mean(np.diag(head_weights))\n",
    "    \n",
    "    # ë¶„ì‚° (attentionì˜ ì§‘ì¤‘ë„)\n",
    "    variance = np.var(head_weights)\n",
    "    \n",
    "    print(f\"  Head {head_idx + 1}: \"\n",
    "          f\"ëŒ€ê°ì„  ê°•ë„={diagonal_strength:.3f}, \"\n",
    "          f\"ë¶„ì‚°={variance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Headì˜ ì¥ì  ì‹œì—°\n",
    "print(\"ğŸ’¡ Multi-Headì˜ ì¥ì \")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Single Head vs Multi-Head ë¹„êµ:\")\n",
    "print()\n",
    "\n",
    "# Single Head (í° ì°¨ì›)\n",
    "single_head = MultiHeadAttention(d_model=64, num_heads=1)\n",
    "\n",
    "# Multi-Head (ì—¬ëŸ¬ ì‘ì€ ì°¨ì›)\n",
    "multi_head = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# ë™ì¼í•œ ì…ë ¥\n",
    "X = np.random.randn(10, 64)\n",
    "\n",
    "# Forward pass\n",
    "output_single, weights_single = single_head.forward(X, X, X)\n",
    "output_multi, weights_multi = multi_head.forward(X, X, X)\n",
    "\n",
    "print(\"1. Single Head:\")\n",
    "print(f\"   - 1ê°œì˜ 64ì°¨ì› attention\")\n",
    "print(f\"   - í•˜ë‚˜ì˜ ê´€ì ë§Œ í•™ìŠµ\")\n",
    "print(f\"   - Attention ë¶„ì‚°: {np.var(weights_single):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"2. Multi-Head (8 heads):\")\n",
    "print(f\"   - 8ê°œì˜ 8ì°¨ì› attention\")\n",
    "print(f\"   - ë‹¤ì–‘í•œ ê´€ì  ë™ì‹œ í•™ìŠµ\")\n",
    "\n",
    "# ê° headì˜ ë‹¤ì–‘ì„±\n",
    "head_variances = [np.var(weights_multi[i]) for i in range(8)]\n",
    "print(f\"   - Headë³„ ë¶„ì‚°: {[f'{v:.4f}' for v in head_variances[:4]]} ...\")\n",
    "print(f\"   - í‰ê·  ë¶„ì‚°: {np.mean(head_variances):.4f}\")\n",
    "print()\n",
    "print(\"â†’ Multi-HeadëŠ” ë‹¤ì–‘í•œ íŒ¨í„´ì„ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding ìƒì„± ë° ë¶„ì„\n",
    "print(\"ğŸ“ Positional Encoding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PE ìƒì„±\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "print(f\"PE shape: {pe.shape}\")\n",
    "print(f\"PE ê°’ ë²”ìœ„: [{pe.min():.3f}, {pe.max():.3f}]\\n\")\n",
    "\n",
    "# ì²˜ìŒ ëª‡ ê°œ ìœ„ì¹˜ì˜ íŒ¨í„´\n",
    "print(\"ì²˜ìŒ 5ê°œ ìœ„ì¹˜ì˜ ì²˜ìŒ 8ì°¨ì›:\")\n",
    "print(\"Pos  \", end=\"\")\n",
    "for dim in range(8):\n",
    "    print(f\"Dim{dim:2d}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "for pos in range(5):\n",
    "    print(f\"{pos:3d}  \", end=\"\")\n",
    "    for dim in range(8):\n",
    "        val = pe[pos, dim]\n",
    "        print(f\"{val:6.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# ì£¼íŒŒìˆ˜ íŠ¹ì„±\n",
    "print(\"\\nğŸ“Š ì°¨ì›ë³„ ì£¼íŒŒìˆ˜ íŠ¹ì„±:\")\n",
    "for dim in [0, 32, 64, 127]:\n",
    "    # ì£¼ê¸° ê³„ì‚°\n",
    "    if dim % 2 == 0:\n",
    "        wavelength = 2 * np.pi * (10000 ** (dim / d_model))\n",
    "        print(f\"  Dim {dim:3d}: íŒŒì¥ â‰ˆ {wavelength:.1f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEì˜ íš¨ê³¼ ì‹œì—°\n",
    "print(\"ğŸ”¬ Positional Encodingì˜ íš¨ê³¼\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë™ì¼í•œ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ìœ„ì¹˜ì— ìˆì„ ë•Œ\n",
    "sentence = \"The cat and the dog\"\n",
    "tokens = sentence.split()\n",
    "print(f\"ë¬¸ì¥: '{sentence}'\")\n",
    "print(f\"í† í°: {tokens}\")\n",
    "print(f\"  'the'ê°€ ìœ„ì¹˜ 0ê³¼ 3ì— ë‚˜íƒ€ë‚¨\\n\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ì„ë² ë”© (theëŠ” ê°™ì€ ë²¡í„°)\n",
    "embedding_dim = 32\n",
    "word_embeddings = {\n",
    "    \"The\": np.ones(embedding_dim) * 0.1,\n",
    "    \"the\": np.ones(embedding_dim) * 0.1,  # ê°™ì€ ë²¡í„°\n",
    "    \"cat\": np.ones(embedding_dim) * 0.2,\n",
    "    \"and\": np.ones(embedding_dim) * 0.3,\n",
    "    \"dog\": np.ones(embedding_dim) * 0.4\n",
    "}\n",
    "\n",
    "# ì„ë² ë”© í–‰ë ¬\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "\n",
    "# PE ì—†ì´\n",
    "print(\"1. PE ì—†ì´:\")\n",
    "print(f\"   'The' ë²¡í„° í‰ê· : {X[0].mean():.3f}\")\n",
    "print(f\"   'the' ë²¡í„° í‰ê· : {X[3].mean():.3f}\")\n",
    "print(f\"   â†’ ë™ì¼í•œ ë²¡í„°!\\n\")\n",
    "\n",
    "# PE ì¶”ê°€\n",
    "X_with_pe = add_positional_encoding(X)\n",
    "print(\"2. PE ì¶”ê°€ í›„:\")\n",
    "print(f\"   'The' (pos 0) ë²¡í„° í‰ê· : {X_with_pe[0].mean():.3f}\")\n",
    "print(f\"   'the' (pos 3) ë²¡í„° í‰ê· : {X_with_pe[3].mean():.3f}\")\n",
    "print(f\"   â†’ ë‹¤ë¥¸ ë²¡í„°!\")\n",
    "print()\n",
    "print(\"â†’ PEë¥¼ í†µí•´ ìœ„ì¹˜ ì •ë³´ê°€ ì¶”ê°€ë˜ì–´ ê°™ì€ ë‹¨ì–´ë„ êµ¬ë³„ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì‹¤ì „ ì˜ˆì œ: ê°„ë‹¨í•œ Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ Transformer Block êµ¬í˜„\n",
    "class SimpleTransformerBlock:\n",
    "    \"\"\"ê°„ë‹¨í•œ Transformer ë¸”ë¡\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Feed-forward network weights\n",
    "        self.ff_w1 = np.random.randn(d_model, d_model * 4) * 0.1\n",
    "        self.ff_w2 = np.random.randn(d_model * 4, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Multi-Head Attention\n",
    "        attn_output, attn_weights = self.attention.forward(x, x, x, mask)\n",
    "        \n",
    "        # 2. Residual connection + Layer Norm (simplified)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # 3. Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # 4. Residual connection + Layer Norm\n",
    "        x = x + ff_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x, attn_weights\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Position-wise feed-forward network\"\"\"\n",
    "        # Linear -> ReLU -> Linear\n",
    "        hidden = np.maximum(0, x @ self.ff_w1)  # ReLU\n",
    "        output = hidden @ self.ff_w2\n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x, eps=1e-6):\n",
    "        \"\"\"Layer normalization\"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + eps)\n",
    "\n",
    "# Transformer Block í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ—ï¸ Simple Transformer Block\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ìƒì„±\n",
    "transformer = SimpleTransformerBlock(d_model=64, num_heads=8)\n",
    "\n",
    "# ì…ë ¥ (with PE)\n",
    "seq_len = 10\n",
    "X = np.random.randn(seq_len, 64)\n",
    "X = add_positional_encoding(X)\n",
    "\n",
    "# Forward pass\n",
    "output, attention = transformer.forward(X)\n",
    "\n",
    "print(f\"ì…ë ¥ shape: {X.shape}\")\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print()\n",
    "print(\"êµ¬ì„± ìš”ì†Œ:\")\n",
    "print(\"  1. Multi-Head Attention\")\n",
    "print(\"  2. Residual Connection\")\n",
    "print(\"  3. Layer Normalization\")\n",
    "print(\"  4. Feed-Forward Network\")\n",
    "print(\"  5. Another Residual + LayerNorm\")\n",
    "print()\n",
    "print(\"â†’ ì´ê²ƒì´ Transformerì˜ ê¸°ë³¸ ë¸”ë¡ì…ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention íŒ¨í„´ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ Attention íŒ¨í„´ ìƒì„± ë° ë¹„êµ\n",
    "print(\"ğŸ¨ ë‹¤ì–‘í•œ Attention íŒ¨í„´\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 8\n",
    "\n",
    "# 1. Identity Attention (ìê¸° ìì‹ ë§Œ)\n",
    "identity_attention = np.eye(seq_len)\n",
    "\n",
    "# 2. Uniform Attention (ê· ë“±)\n",
    "uniform_attention = np.ones((seq_len, seq_len)) / seq_len\n",
    "\n",
    "# 3. Local Attention (ì¸ì ‘ í† í°)\n",
    "local_attention = np.zeros((seq_len, seq_len))\n",
    "for i in range(seq_len):\n",
    "    for j in range(max(0, i-1), min(seq_len, i+2)):\n",
    "        local_attention[i, j] = 1/3\n",
    "\n",
    "# 4. Global + Local (í˜¼í•©)\n",
    "mixed_attention = 0.7 * local_attention + 0.3 * uniform_attention\n",
    "\n",
    "# ì‹œê°í™”\n",
    "patterns = [\n",
    "    (\"Identity (ìê¸° ìì‹ ë§Œ)\", identity_attention),\n",
    "    (\"Uniform (ê· ë“± ë¶„í¬)\", uniform_attention),\n",
    "    (\"Local (ì¸ì ‘ í† í°)\", local_attention),\n",
    "    (\"Mixed (Global + Local)\", mixed_attention)\n",
    "]\n",
    "\n",
    "for name, pattern in patterns:\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i in range(min(5, seq_len)):\n",
    "        print(\" \", end=\"\")\n",
    "        for j in range(min(8, seq_len)):\n",
    "            val = pattern[i, j]\n",
    "            if val > 0.5:\n",
    "                print(\"â–ˆâ–ˆ\", end=\"\")\n",
    "            elif val > 0.2:\n",
    "                print(\"â–“â–“\", end=\"\")\n",
    "            elif val > 0.05:\n",
    "                print(\"â–‘â–‘\", end=\"\")\n",
    "            else:\n",
    "                print(\"Â·Â·\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì„±ëŠ¥ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Attention ê³„ì‚° ë³µì¡ë„ ë¶„ì„\n",
    "print(\"âš¡ Attention ê³„ì‚° ë³µì¡ë„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ í…ŒìŠ¤íŠ¸\n",
    "seq_lengths = [10, 50, 100, 200]\n",
    "d_model = 64\n",
    "\n",
    "print(f\"d_model = {d_model}\\n\")\n",
    "print(\"Seq Length | Time (ms) | Memory (MB) | Complexity\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    # ì…ë ¥ ìƒì„±\n",
    "    Q = K = V = np.random.randn(seq_len, d_model)\n",
    "    \n",
    "    # ì‹œê°„ ì¸¡ì •\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _, _ = scaled_dot_product_attention(Q, K, V)\n",
    "    elapsed = (time.time() - start) / 100 * 1000  # ms\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì¶”ì • (attention matrix)\n",
    "    memory_mb = (seq_len * seq_len * 8) / (1024 * 1024)  # 8 bytes per float64\n",
    "    \n",
    "    print(f\"{seq_len:10d} | {elapsed:9.2f} | {memory_mb:11.2f} | O(nÂ²)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ê´€ì°°:\")\n",
    "print(\"  - ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì œê³±ì— ë¹„ë¡€í•˜ëŠ” ê³„ì‚°ëŸ‰\")\n",
    "print(\"  - ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥\")\n",
    "print(\"  - ì´ë˜ì„œ íš¨ìœ¨ì ì¸ Attention ë³€í˜•ë“¤ì´ ì—°êµ¬ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
    "\n",
    "Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ì„ ëª¨ë‘ êµ¬í˜„í•˜ê³  ì´í•´í–ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "### ë°°ìš´ ë‚´ìš©:\n",
    "1. âœ… Scaled Dot-Product Attention\n",
    "2. âœ… Self-Attentionê³¼ Cross-Attention\n",
    "3. âœ… Causal Masking\n",
    "4. âœ… Multi-Head Attention\n",
    "5. âœ… Positional Encoding\n",
    "6. âœ… ê°„ë‹¨í•œ Transformer Block\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "- Day 4: ì™„ì „í•œ Transformer êµ¬í˜„\n",
    "- Layer Normalization, Residual Connection\n",
    "- Encoder-Decoder êµ¬ì¡°\n",
    "\n",
    "\"Attention is literally all you need!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}