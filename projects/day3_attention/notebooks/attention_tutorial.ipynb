{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Attention Mechanism Tutorial\n",
    "\n",
    "이 노트북에서는 Attention 메커니즘을 처음부터 구현하고 실험합니다.\n",
    "\"Attention is All You Need\" 논문의 핵심 개념을 직접 코딩해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 상위 디렉토리를 path에 추가\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('.')))))\n",
    "\n",
    "# core 모듈 import\n",
    "from core.attention import (\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    "    positional_encoding,\n",
    "    add_positional_encoding,\n",
    "    create_causal_mask,\n",
    "    visualize_attention\n",
    ")\n",
    "\n",
    "print(\"✅ 환경 설정 완료!\")\n",
    "print(f\"NumPy 버전: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention의 직관적 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention의 핵심: Query, Key, Value\n",
    "print(\"🔍 Attention의 핵심 개념\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"📝 비유: 도서관에서 책 찾기\")\n",
    "print(\"  Query (질문): '파이썬 프로그래밍 책을 찾고 있어요'\")\n",
    "print(\"  Key (색인): 각 책의 제목과 주제\")\n",
    "print(\"  Value (내용): 실제 책의 내용\")\n",
    "print()\n",
    "print(\"→ Attention은 Query와 가장 관련있는 Key를 찾아\")\n",
    "print(\"  해당하는 Value를 가져오는 메커니즘입니다.\")\n",
    "print()\n",
    "print(\"수식: Attention(Q,K,V) = softmax(QK^T/√d_k)V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 예제로 시작\n",
    "print(\"📊 Simple Attention Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 3개의 단어, 각 4차원 벡터\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Query, Key, Value 생성\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "# Attention 계산\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\n출력 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights:\\n{attention_weights}\")\n",
    "print(f\"\\n각 행의 합 (should be 1): {attention_weights.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 계산 단계별 분해\n",
    "print(\"🔧 Attention 계산 단계별 분해\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Q와 K의 내적\n",
    "scores = Q @ K.T\n",
    "print(\"Step 1 - Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\\n\")\n",
    "\n",
    "# Step 2: Scaling\n",
    "d_k = K.shape[-1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "print(f\"Step 2 - Scaled scores (÷√{d_k}):\")\n",
    "print(scaled_scores)\n",
    "print()\n",
    "\n",
    "# Step 3: Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights_manual = softmax(scaled_scores)\n",
    "print(\"Step 3 - Attention weights (softmax):\")\n",
    "print(attention_weights_manual)\n",
    "print()\n",
    "\n",
    "# Step 4: 가중합\n",
    "output_manual = attention_weights_manual @ V\n",
    "print(\"Step 4 - Output (weighted sum of V):\")\n",
    "print(output_manual)\n",
    "\n",
    "# 검증\n",
    "print(f\"\\n✅ 수동 계산과 함수 결과 동일: {np.allclose(output, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에서 Self-Attention\n",
    "print(\"📝 문장에서 Self-Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 간단한 문장\n",
    "sentence = \"The cat sat\"\n",
    "tokens = sentence.split()\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\\n\")\n",
    "\n",
    "# 각 단어를 임의의 벡터로 표현\n",
    "np.random.seed(42)\n",
    "word_embeddings = {\n",
    "    \"The\": np.random.randn(8),\n",
    "    \"cat\": np.random.randn(8),\n",
    "    \"sat\": np.random.randn(8)\n",
    "}\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "print(f\"임베딩 shape: {X.shape}\")\n",
    "\n",
    "# Self-attention (Q=K=V=X)\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X)\n",
    "\n",
    "# Attention 시각화\n",
    "print(\"\\nAttention Matrix:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        weight = attention_weights[i, j]\n",
    "        print(f\"{weight:8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# 해석\n",
    "print(\"\\n💡 해석:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    max_idx = np.argmax(attention_weights[i])\n",
    "    print(f\"  '{token}'이 가장 주목하는 단어: '{tokens[max_idx]}' \"\n",
    "          f\"(weight: {attention_weights[i, max_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal Attention (GPT 스타일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal mask 생성 및 적용\n",
    "print(\"🔮 Causal Attention (미래를 볼 수 없음)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 더 긴 문장\n",
    "sentence = \"I think therefore I am\"\n",
    "tokens = sentence.split()\n",
    "seq_len = len(tokens)\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\\n\")\n",
    "\n",
    "# 임베딩\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(seq_len, 16)\n",
    "\n",
    "# Causal mask 생성\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "print()\n",
    "\n",
    "# Causal attention 적용\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X, mask=causal_mask)\n",
    "\n",
    "# 시각화\n",
    "print(\"Causal Attention Weights:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>10}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(seq_len):\n",
    "        weight = attention_weights[i, j]\n",
    "        if weight < 0.001:\n",
    "            print(\"         -\", end=\"\")\n",
    "        else:\n",
    "            print(f\"{weight:10.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n💡 특징:\")\n",
    "print(\"  - 하삼각 행렬 형태\")\n",
    "print(\"  - 각 토큰은 자신과 이전 토큰만 볼 수 있음\")\n",
    "print(\"  - GPT와 같은 생성 모델에서 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention 실습\n",
    "print(\"🎭 Multi-Head Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 설정\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {d_model // num_heads}\\n\")\n",
    "\n",
    "# Multi-Head Attention 생성\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 입력 데이터\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "print(f\"입력 shape: {X.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attention_weights = mha.forward(X, X, X)\n",
    "\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  (num_heads, seq_len, seq_len)\\n\")\n",
    "\n",
    "# 각 head의 attention 패턴 분석\n",
    "print(\"각 Head의 특성:\")\n",
    "for head_idx in range(num_heads):\n",
    "    head_weights = attention_weights[head_idx]\n",
    "    \n",
    "    # 대각선 강도 (자기 자신에 대한 attention)\n",
    "    diagonal_strength = np.mean(np.diag(head_weights))\n",
    "    \n",
    "    # 분산 (attention의 집중도)\n",
    "    variance = np.var(head_weights)\n",
    "    \n",
    "    print(f\"  Head {head_idx + 1}: \"\n",
    "          f\"대각선 강도={diagonal_strength:.3f}, \"\n",
    "          f\"분산={variance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head의 장점 시연\n",
    "print(\"💡 Multi-Head의 장점\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Single Head vs Multi-Head 비교:\")\n",
    "print()\n",
    "\n",
    "# Single Head (큰 차원)\n",
    "single_head = MultiHeadAttention(d_model=64, num_heads=1)\n",
    "\n",
    "# Multi-Head (여러 작은 차원)\n",
    "multi_head = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# 동일한 입력\n",
    "X = np.random.randn(10, 64)\n",
    "\n",
    "# Forward pass\n",
    "output_single, weights_single = single_head.forward(X, X, X)\n",
    "output_multi, weights_multi = multi_head.forward(X, X, X)\n",
    "\n",
    "print(\"1. Single Head:\")\n",
    "print(f\"   - 1개의 64차원 attention\")\n",
    "print(f\"   - 하나의 관점만 학습\")\n",
    "print(f\"   - Attention 분산: {np.var(weights_single):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"2. Multi-Head (8 heads):\")\n",
    "print(f\"   - 8개의 8차원 attention\")\n",
    "print(f\"   - 다양한 관점 동시 학습\")\n",
    "\n",
    "# 각 head의 다양성\n",
    "head_variances = [np.var(weights_multi[i]) for i in range(8)]\n",
    "print(f\"   - Head별 분산: {[f'{v:.4f}' for v in head_variances[:4]]} ...\")\n",
    "print(f\"   - 평균 분산: {np.mean(head_variances):.4f}\")\n",
    "print()\n",
    "print(\"→ Multi-Head는 다양한 패턴을 동시에 학습할 수 있습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 생성 및 분석\n",
    "print(\"📍 Positional Encoding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PE 생성\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "print(f\"PE shape: {pe.shape}\")\n",
    "print(f\"PE 값 범위: [{pe.min():.3f}, {pe.max():.3f}]\\n\")\n",
    "\n",
    "# 처음 몇 개 위치의 패턴\n",
    "print(\"처음 5개 위치의 처음 8차원:\")\n",
    "print(\"Pos  \", end=\"\")\n",
    "for dim in range(8):\n",
    "    print(f\"Dim{dim:2d}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "for pos in range(5):\n",
    "    print(f\"{pos:3d}  \", end=\"\")\n",
    "    for dim in range(8):\n",
    "        val = pe[pos, dim]\n",
    "        print(f\"{val:6.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# 주파수 특성\n",
    "print(\"\\n📊 차원별 주파수 특성:\")\n",
    "for dim in [0, 32, 64, 127]:\n",
    "    # 주기 계산\n",
    "    if dim % 2 == 0:\n",
    "        wavelength = 2 * np.pi * (10000 ** (dim / d_model))\n",
    "        print(f\"  Dim {dim:3d}: 파장 ≈ {wavelength:.1f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE의 효과 시연\n",
    "print(\"🔬 Positional Encoding의 효과\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 동일한 단어가 다른 위치에 있을 때\n",
    "sentence = \"The cat and the dog\"\n",
    "tokens = sentence.split()\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\")\n",
    "print(f\"  'the'가 위치 0과 3에 나타남\\n\")\n",
    "\n",
    "# 간단한 임베딩 (the는 같은 벡터)\n",
    "embedding_dim = 32\n",
    "word_embeddings = {\n",
    "    \"The\": np.ones(embedding_dim) * 0.1,\n",
    "    \"the\": np.ones(embedding_dim) * 0.1,  # 같은 벡터\n",
    "    \"cat\": np.ones(embedding_dim) * 0.2,\n",
    "    \"and\": np.ones(embedding_dim) * 0.3,\n",
    "    \"dog\": np.ones(embedding_dim) * 0.4\n",
    "}\n",
    "\n",
    "# 임베딩 행렬\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "\n",
    "# PE 없이\n",
    "print(\"1. PE 없이:\")\n",
    "print(f\"   'The' 벡터 평균: {X[0].mean():.3f}\")\n",
    "print(f\"   'the' 벡터 평균: {X[3].mean():.3f}\")\n",
    "print(f\"   → 동일한 벡터!\\n\")\n",
    "\n",
    "# PE 추가\n",
    "X_with_pe = add_positional_encoding(X)\n",
    "print(\"2. PE 추가 후:\")\n",
    "print(f\"   'The' (pos 0) 벡터 평균: {X_with_pe[0].mean():.3f}\")\n",
    "print(f\"   'the' (pos 3) 벡터 평균: {X_with_pe[3].mean():.3f}\")\n",
    "print(f\"   → 다른 벡터!\")\n",
    "print()\n",
    "print(\"→ PE를 통해 위치 정보가 추가되어 같은 단어도 구별 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실전 예제: 간단한 Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Transformer Block 구현\n",
    "class SimpleTransformerBlock:\n",
    "    \"\"\"간단한 Transformer 블록\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Feed-forward network weights\n",
    "        self.ff_w1 = np.random.randn(d_model, d_model * 4) * 0.1\n",
    "        self.ff_w2 = np.random.randn(d_model * 4, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Multi-Head Attention\n",
    "        attn_output, attn_weights = self.attention.forward(x, x, x, mask)\n",
    "        \n",
    "        # 2. Residual connection + Layer Norm (simplified)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # 3. Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # 4. Residual connection + Layer Norm\n",
    "        x = x + ff_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x, attn_weights\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Position-wise feed-forward network\"\"\"\n",
    "        # Linear -> ReLU -> Linear\n",
    "        hidden = np.maximum(0, x @ self.ff_w1)  # ReLU\n",
    "        output = hidden @ self.ff_w2\n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x, eps=1e-6):\n",
    "        \"\"\"Layer normalization\"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + eps)\n",
    "\n",
    "# Transformer Block 테스트\n",
    "print(\"🏗️ Simple Transformer Block\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 생성\n",
    "transformer = SimpleTransformerBlock(d_model=64, num_heads=8)\n",
    "\n",
    "# 입력 (with PE)\n",
    "seq_len = 10\n",
    "X = np.random.randn(seq_len, 64)\n",
    "X = add_positional_encoding(X)\n",
    "\n",
    "# Forward pass\n",
    "output, attention = transformer.forward(X)\n",
    "\n",
    "print(f\"입력 shape: {X.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print()\n",
    "print(\"구성 요소:\")\n",
    "print(\"  1. Multi-Head Attention\")\n",
    "print(\"  2. Residual Connection\")\n",
    "print(\"  3. Layer Normalization\")\n",
    "print(\"  4. Feed-Forward Network\")\n",
    "print(\"  5. Another Residual + LayerNorm\")\n",
    "print()\n",
    "print(\"→ 이것이 Transformer의 기본 블록입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention 패턴 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 Attention 패턴 생성 및 비교\n",
    "print(\"🎨 다양한 Attention 패턴\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 8\n",
    "\n",
    "# 1. Identity Attention (자기 자신만)\n",
    "identity_attention = np.eye(seq_len)\n",
    "\n",
    "# 2. Uniform Attention (균등)\n",
    "uniform_attention = np.ones((seq_len, seq_len)) / seq_len\n",
    "\n",
    "# 3. Local Attention (인접 토큰)\n",
    "local_attention = np.zeros((seq_len, seq_len))\n",
    "for i in range(seq_len):\n",
    "    for j in range(max(0, i-1), min(seq_len, i+2)):\n",
    "        local_attention[i, j] = 1/3\n",
    "\n",
    "# 4. Global + Local (혼합)\n",
    "mixed_attention = 0.7 * local_attention + 0.3 * uniform_attention\n",
    "\n",
    "# 시각화\n",
    "patterns = [\n",
    "    (\"Identity (자기 자신만)\", identity_attention),\n",
    "    (\"Uniform (균등 분포)\", uniform_attention),\n",
    "    (\"Local (인접 토큰)\", local_attention),\n",
    "    (\"Mixed (Global + Local)\", mixed_attention)\n",
    "]\n",
    "\n",
    "for name, pattern in patterns:\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i in range(min(5, seq_len)):\n",
    "        print(\" \", end=\"\")\n",
    "        for j in range(min(8, seq_len)):\n",
    "            val = pattern[i, j]\n",
    "            if val > 0.5:\n",
    "                print(\"██\", end=\"\")\n",
    "            elif val > 0.2:\n",
    "                print(\"▓▓\", end=\"\")\n",
    "            elif val > 0.05:\n",
    "                print(\"░░\", end=\"\")\n",
    "            else:\n",
    "                print(\"··\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Attention 계산 복잡도 분석\n",
    "print(\"⚡ Attention 계산 복잡도\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 다양한 시퀀스 길이에서 테스트\n",
    "seq_lengths = [10, 50, 100, 200]\n",
    "d_model = 64\n",
    "\n",
    "print(f\"d_model = {d_model}\\n\")\n",
    "print(\"Seq Length | Time (ms) | Memory (MB) | Complexity\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    # 입력 생성\n",
    "    Q = K = V = np.random.randn(seq_len, d_model)\n",
    "    \n",
    "    # 시간 측정\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _, _ = scaled_dot_product_attention(Q, K, V)\n",
    "    elapsed = (time.time() - start) / 100 * 1000  # ms\n",
    "    \n",
    "    # 메모리 추정 (attention matrix)\n",
    "    memory_mb = (seq_len * seq_len * 8) / (1024 * 1024)  # 8 bytes per float64\n",
    "    \n",
    "    print(f\"{seq_len:10d} | {elapsed:9.2f} | {memory_mb:11.2f} | O(n²)\")\n",
    "\n",
    "print(\"\\n💡 관찰:\")\n",
    "print(\"  - 시퀀스 길이의 제곱에 비례하는 계산량\")\n",
    "print(\"  - 긴 시퀀스에서 메모리 문제 발생 가능\")\n",
    "print(\"  - 이래서 효율적인 Attention 변형들이 연구됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 축하합니다!\n",
    "\n",
    "Attention 메커니즘의 핵심을 모두 구현하고 이해했습니다!\n",
    "\n",
    "### 배운 내용:\n",
    "1. ✅ Scaled Dot-Product Attention\n",
    "2. ✅ Self-Attention과 Cross-Attention\n",
    "3. ✅ Causal Masking\n",
    "4. ✅ Multi-Head Attention\n",
    "5. ✅ Positional Encoding\n",
    "6. ✅ 간단한 Transformer Block\n",
    "\n",
    "### 다음 단계:\n",
    "- Day 4: 완전한 Transformer 구현\n",
    "- Layer Normalization, Residual Connection\n",
    "- Encoder-Decoder 구조\n",
    "\n",
    "\"Attention is literally all you need!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}