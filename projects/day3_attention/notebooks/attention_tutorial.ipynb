{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Attention Mechanism Tutorial\n",
    "\n",
    "이 노트북에서는 Attention 메커니즘을 처음부터 구현하고 실험합니다.\n",
    "\"Attention is All You Need\" 논문의 핵심 개념을 직접 코딩해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 환경 설정 완료!\n",
      "NumPy 버전: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 상위 디렉토리를 path에 추가\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('.')))))\n",
    "\n",
    "# core 모듈 import\n",
    "from core.attention import (\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    "    positional_encoding,\n",
    "    add_positional_encoding,\n",
    "    create_causal_mask,\n",
    "    visualize_attention\n",
    ")\n",
    "\n",
    "print(\"✅ 환경 설정 완료!\")\n",
    "print(f\"NumPy 버전: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention의 직관적 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Attention의 핵심 개념\n",
      "==================================================\n",
      "\n",
      "📝 비유: 도서관에서 책 찾기\n",
      "  Query (질문): '파이썬 프로그래밍 책을 찾고 있어요'\n",
      "  Key (색인): 각 책의 제목과 주제\n",
      "  Value (내용): 실제 책의 내용\n",
      "\n",
      "→ Attention은 Query와 가장 관련있는 Key를 찾아\n",
      "  해당하는 Value를 가져오는 메커니즘입니다.\n",
      "\n",
      "수식: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n"
     ]
    }
   ],
   "source": [
    "# Attention의 핵심: Query, Key, Value\n",
    "print(\"🔍 Attention의 핵심 개념\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"📝 비유: 도서관에서 책 찾기\")\n",
    "print(\"  Query (질문): '파이썬 프로그래밍 책을 찾고 있어요'\")\n",
    "print(\"  Key (색인): 각 책의 제목과 주제\")\n",
    "print(\"  Value (내용): 실제 책의 내용\")\n",
    "print()\n",
    "print(\"→ Attention은 Query와 가장 관련있는 Key를 찾아\")\n",
    "print(\"  해당하는 Value를 가져오는 메커니즘입니다.\")\n",
    "print()\n",
    "print(\"수식: Attention(Q,K,V) = softmax(QK^T/√d_k)V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Simple Attention Example\n",
      "==================================================\n",
      "Q shape: (3, 4)\n",
      "K shape: (3, 4)\n",
      "V shape: (3, 4)\n",
      "\n",
      "출력 shape: (3, 4)\n",
      "Attention weights shape: (3, 3)\n",
      "\n",
      "Attention weights:\n",
      "[[0.39285909 0.16818537 0.43895554]\n",
      " [0.23089671 0.28342933 0.48567396]\n",
      " [0.22547439 0.55874566 0.21577995]]\n",
      "\n",
      "각 행의 합 (should be 1): [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 간단한 예제로 시작\n",
    "print(\"📊 Simple Attention Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 3개의 단어, 각 4차원 벡터\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Query, Key, Value 생성\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "# Attention 계산\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\n출력 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights:\\n{attention_weights}\")\n",
    "print(f\"\\n각 행의 합 (should be 1): {attention_weights.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Attention 계산 단계별 분해\n",
      "==================================================\n",
      "Step 1 - Scores (QK^T):\n",
      "[[-1.58886576 -3.28563423 -1.36697149]\n",
      " [-2.76421799 -2.35423325 -1.27708386]\n",
      " [-0.09043303  1.72454259 -0.1783279 ]]\n",
      "Shape: (3, 3)\n",
      "\n",
      "Step 2 - Scaled scores (÷√4):\n",
      "[[-0.79443288 -1.64281711 -0.68348575]\n",
      " [-1.382109   -1.17711663 -0.63854193]\n",
      " [-0.04521651  0.8622713  -0.08916395]]\n",
      "\n",
      "Step 3 - Attention weights (softmax):\n",
      "[[0.39285909 0.16818537 0.43895554]\n",
      " [0.23089671 0.28342933 0.48567396]\n",
      " [0.22547439 0.55874566 0.21577995]]\n",
      "\n",
      "Step 4 - Output (weighted sum of V):\n",
      "[[-0.32080902 -0.46976975 -0.19231589 -0.07677361]\n",
      " [-0.30249005 -0.57076556 -0.03681329  0.01880544]\n",
      " [-0.46126105 -0.36620523 -0.41823183  0.8562291 ]]\n",
      "\n",
      "✅ 수동 계산과 함수 결과 동일: True\n"
     ]
    }
   ],
   "source": [
    "# Attention 계산 단계별 분해\n",
    "print(\"🔧 Attention 계산 단계별 분해\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Q와 K의 내적\n",
    "scores = Q @ K.T\n",
    "print(\"Step 1 - Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\\n\")\n",
    "\n",
    "# Step 2: Scaling\n",
    "d_k = K.shape[-1]\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "print(f\"Step 2 - Scaled scores (÷√{d_k}):\")\n",
    "print(scaled_scores)\n",
    "print()\n",
    "\n",
    "# Step 3: Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights_manual = softmax(scaled_scores)\n",
    "print(\"Step 3 - Attention weights (softmax):\")\n",
    "print(attention_weights_manual)\n",
    "print()\n",
    "\n",
    "# Step 4: 가중합\n",
    "output_manual = attention_weights_manual @ V\n",
    "print(\"Step 4 - Output (weighted sum of V):\")\n",
    "print(output_manual)\n",
    "\n",
    "# 검증\n",
    "print(f\"\\n✅ 수동 계산과 함수 결과 동일: {np.allclose(output, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 문장에서 Self-Attention\n",
      "==================================================\n",
      "문장: 'The cat sat'\n",
      "토큰: ['The', 'cat', 'sat']\n",
      "\n",
      "임베딩 shape: (3, 8)\n",
      "임베딩: [[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
      "  -1.72491783 -0.56228753]\n",
      " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
      "   0.0675282  -1.42474819]]\n",
      "\n",
      "Attention Matrix:\n",
      "            The     cat     sat\n",
      "    The   0.954   0.025   0.021\n",
      "    cat   0.012   0.833   0.155\n",
      "    sat   0.009   0.145   0.845\n",
      "\n",
      "💡 해석:\n",
      "  'The'이 가장 주목하는 단어: 'The' (weight: 0.954)\n",
      "  'cat'이 가장 주목하는 단어: 'cat' (weight: 0.833)\n",
      "  'sat'이 가장 주목하는 단어: 'sat' (weight: 0.845)\n"
     ]
    }
   ],
   "source": [
    "# 문장에서 Self-Attention\n",
    "print(\"📝 문장에서 Self-Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 간단한 문장\n",
    "sentence = \"The cat sat\"\n",
    "tokens = sentence.split()\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\\n\")\n",
    "\n",
    "# 각 단어를 임의의 벡터로 표현\n",
    "np.random.seed(42)\n",
    "word_embeddings = {\n",
    "    \"The\": np.random.randn(8),\n",
    "    \"cat\": np.random.randn(8),\n",
    "    \"sat\": np.random.randn(8)\n",
    "}\n",
    "\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "print(f\"임베딩 shape: {X.shape}\")\n",
    "print(f\"임베딩: {X}\")\n",
    "\n",
    "# Self-attention (Q=K=V=X)\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X)\n",
    "\n",
    "# Attention 시각화\n",
    "print(\"\\nAttention Matrix:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        weight = attention_weights[i, j]\n",
    "        print(f\"{weight:8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# 해석\n",
    "print(\"\\n💡 해석:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    max_idx = np.argmax(attention_weights[i])\n",
    "    print(f\"  '{token}'이 가장 주목하는 단어: '{tokens[max_idx]}' \"\n",
    "          f\"(weight: {attention_weights[i, max_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Causal Attention (GPT 스타일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Causal Attention (미래를 볼 수 없음)\n",
      "==================================================\n",
      "문장: 'I think therefore I am'\n",
      "토큰: ['I', 'think', 'therefore', 'I', 'am']\n",
      "\n",
      "Causal Mask:\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "Causal Attention Weights:\n",
      "                I     think therefore         I        am\n",
      "      I     1.000         -         -         -         -\n",
      "  think     0.012     0.988         -         -         -\n",
      "therefore     0.020     0.080     0.900         -         -\n",
      "      I     0.165     0.019     0.040     0.776         -\n",
      "     am     0.030     0.002     0.002     0.006     0.959\n",
      "\n",
      "💡 특징:\n",
      "  - 하삼각 행렬 형태\n",
      "  - 각 토큰은 자신과 이전 토큰만 볼 수 있음\n",
      "  - GPT와 같은 생성 모델에서 사용\n"
     ]
    }
   ],
   "source": [
    "# Causal mask 생성 및 적용\n",
    "print(\"🔮 Causal Attention (미래를 볼 수 없음)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 더 긴 문장\n",
    "sentence = \"I think therefore I am\"\n",
    "tokens = sentence.split()\n",
    "seq_len = len(tokens)\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\\n\")\n",
    "\n",
    "# 임베딩\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(seq_len, 16)\n",
    "\n",
    "# Causal mask 생성\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "print()\n",
    "\n",
    "# Causal attention 적용\n",
    "output, attention_weights = scaled_dot_product_attention(X, X, X, mask=causal_mask)\n",
    "\n",
    "# 시각화\n",
    "print(\"Causal Attention Weights:\")\n",
    "print(\"       \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>10}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>7}\", end=\"\")\n",
    "    for j in range(seq_len):\n",
    "        weight = attention_weights[i, j]\n",
    "        if weight < 0.001:\n",
    "            print(\"         -\", end=\"\")\n",
    "        else:\n",
    "            print(f\"{weight:10.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n💡 특징:\")\n",
    "print(\"  - 하삼각 행렬 형태\")\n",
    "print(\"  - 각 토큰은 자신과 이전 토큰만 볼 수 있음\")\n",
    "print(\"  - GPT와 같은 생성 모델에서 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Multi-Head Attention\n",
      "==================================================\n",
      "Model dimension: 64\n",
      "Number of heads: 8\n",
      "Dimension per head: 8\n",
      "\n",
      "입력 shape: (10, 64)\n",
      "출력 shape: (10, 64)\n",
      "Attention weights shape: (8, 10, 10)\n",
      "  (num_heads, seq_len, seq_len)\n",
      "\n",
      "각 Head의 특성:\n",
      "  Head 1: 대각선 강도=0.101, 분산=0.031\n",
      "  Head 2: 대각선 강도=0.041, 분산=0.016\n",
      "  Head 3: 대각선 강도=0.093, 분산=0.024\n",
      "  Head 4: 대각선 강도=0.130, 분산=0.032\n",
      "  Head 5: 대각선 강도=0.138, 분산=0.022\n",
      "  Head 6: 대각선 강도=0.046, 분산=0.020\n",
      "  Head 7: 대각선 강도=0.050, 분산=0.022\n",
      "  Head 8: 대각선 강도=0.032, 분산=0.024\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head Attention 실습\n",
    "print(\"🎭 Multi-Head Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 설정\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {d_model // num_heads}\\n\")\n",
    "\n",
    "# Multi-Head Attention 생성\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 입력 데이터\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "print(f\"입력 shape: {X.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attention_weights = mha.forward(X, X, X)\n",
    "\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  (num_heads, seq_len, seq_len)\\n\")\n",
    "\n",
    "# 각 head의 attention 패턴 분석\n",
    "print(\"각 Head의 특성:\")\n",
    "for head_idx in range(num_heads):\n",
    "    head_weights = attention_weights[head_idx]\n",
    "    \n",
    "    # 대각선 강도 (자기 자신에 대한 attention)\n",
    "    diagonal_strength = np.mean(np.diag(head_weights))\n",
    "    \n",
    "    # 분산 (attention의 집중도)\n",
    "    variance = np.var(head_weights)\n",
    "    \n",
    "    print(f\"  Head {head_idx + 1}: \"\n",
    "          f\"대각선 강도={diagonal_strength:.3f}, \"\n",
    "          f\"분산={variance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본:\n",
      "[[[ 1  2  3  4]\n",
      "  [ 5  6  7  8]\n",
      "  [ 9 10 11 12]]\n",
      "\n",
      " [[13 14 15 16]\n",
      "  [17 18 19 20]\n",
      "  [21 22 23 24]]]\n",
      "\n",
      "Reshape 후:\n",
      "[[[[ 1  2]\n",
      "   [ 3  4]]\n",
      "\n",
      "  [[ 5  6]\n",
      "   [ 7  8]]\n",
      "\n",
      "  [[ 9 10]\n",
      "   [11 12]]]\n",
      "\n",
      "\n",
      " [[[13 14]\n",
      "   [15 16]]\n",
      "\n",
      "  [[17 18]\n",
      "   [19 20]]\n",
      "\n",
      "  [[21 22]\n",
      "   [23 24]]]]\n"
     ]
    }
   ],
   "source": [
    "# 간단한 예시\n",
    "x = np.array([\n",
    "    # 배치 0\n",
    "    [\n",
    "        [1, 2, 3, 4],      # 단어 0의 특징\n",
    "        [5, 6, 7, 8],      # 단어 1의 특징\n",
    "        [9, 10, 11, 12],   # 단어 2의 특징\n",
    "    ],\n",
    "    # 배치 1  \n",
    "    [\n",
    "        [13, 14, 15, 16],  # 단어 0의 특징\n",
    "        [17, 18, 19, 20],  # 단어 1의 특징\n",
    "        [21, 22, 23, 24],  # 단어 2의 특징\n",
    "    ]\n",
    "])\n",
    "# x.shape = (2, 3, 4)\n",
    "\n",
    "# 2개 head로 분할 (d_k = 2)\n",
    "x_reshaped = x.reshape(2, 3, 2, 2)\n",
    "# x_reshaped.shape = (2, 3, 2, 2)\n",
    "\n",
    "print(\"원본:\")\n",
    "print(x)\n",
    "print(\"\\nReshape 후:\")\n",
    "print(x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Multi-Head의 장점\n",
      "==================================================\n",
      "\n",
      "Single Head vs Multi-Head 비교:\n",
      "\n",
      "1. Single Head:\n",
      "   - 1개의 64차원 attention\n",
      "   - 하나의 관점만 학습\n",
      "   - Attention 분산: 0.0264\n",
      "\n",
      "2. Multi-Head (8 heads):\n",
      "   - 8개의 8차원 attention\n",
      "   - 다양한 관점 동시 학습\n",
      "   - Head별 분산: ['0.0425', '0.0211', '0.0274', '0.0339'] ...\n",
      "   - 평균 분산: 0.0329\n",
      "\n",
      "→ Multi-Head는 다양한 패턴을 동시에 학습할 수 있습니다!\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head의 장점 시연\n",
    "print(\"💡 Multi-Head의 장점\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Single Head vs Multi-Head 비교:\")\n",
    "print()\n",
    "\n",
    "# Single Head (큰 차원)\n",
    "single_head = MultiHeadAttention(d_model=64, num_heads=1)\n",
    "\n",
    "# Multi-Head (여러 작은 차원)\n",
    "multi_head = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# 동일한 입력\n",
    "X = np.random.randn(10, 64)\n",
    "\n",
    "# Forward pass\n",
    "output_single, weights_single = single_head.forward(X, X, X)\n",
    "output_multi, weights_multi = multi_head.forward(X, X, X)\n",
    "\n",
    "print(\"1. Single Head:\")\n",
    "print(f\"   - 1개의 64차원 attention\")\n",
    "print(f\"   - 하나의 관점만 학습\")\n",
    "print(f\"   - Attention 분산: {np.var(weights_single):.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"2. Multi-Head (8 heads):\")\n",
    "print(f\"   - 8개의 8차원 attention\")\n",
    "print(f\"   - 다양한 관점 동시 학습\")\n",
    "\n",
    "# 각 head의 다양성\n",
    "head_variances = [np.var(weights_multi[i]) for i in range(8)]\n",
    "print(f\"   - Head별 분산: {[f'{v:.4f}' for v in head_variances[:4]]} ...\")\n",
    "print(f\"   - 평균 분산: {np.mean(head_variances):.4f}\")\n",
    "print()\n",
    "print(\"→ Multi-Head는 다양한 패턴을 동시에 학습할 수 있습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 생성 및 분석\n",
    "print(\"📍 Positional Encoding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PE 생성\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "print(f\"PE shape: {pe.shape}\")\n",
    "print(f\"PE 값 범위: [{pe.min():.3f}, {pe.max():.3f}]\\n\")\n",
    "\n",
    "# 처음 몇 개 위치의 패턴\n",
    "print(\"처음 5개 위치의 처음 8차원:\")\n",
    "print(\"Pos  \", end=\"\")\n",
    "for dim in range(8):\n",
    "    print(f\"Dim{dim:2d}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "for pos in range(5):\n",
    "    print(f\"{pos:3d}  \", end=\"\")\n",
    "    for dim in range(8):\n",
    "        val = pe[pos, dim]\n",
    "        print(f\"{val:6.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# 주파수 특성\n",
    "print(\"\\n📊 차원별 주파수 특성:\")\n",
    "for dim in [0, 32, 64, 127]:\n",
    "    # 주기 계산\n",
    "    if dim % 2 == 0:\n",
    "        wavelength = 2 * np.pi * (10000 ** (dim / d_model))\n",
    "        print(f\"  Dim {dim:3d}: 파장 ≈ {wavelength:.1f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기화:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "위치 인덱스:\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "주파수 항:\n",
      "[1.         0.04641589 0.00215443]\n",
      "\n",
      "최종 Positional Encoding:\n",
      "[[ 0.          1.          0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.04639922  0.99892298  0.00215443  0.99999768]\n",
      " [ 0.90929743 -0.41614684  0.0926985   0.99569422  0.00430886  0.99999072]\n",
      " [ 0.14112001 -0.9899925   0.1387981   0.9903207   0.00646326  0.99997911]\n",
      " [-0.7568025  -0.65364362  0.18459872  0.98281398  0.00861763  0.99996287]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 5\n",
    "d_model = 6\n",
    "base = 10000\n",
    "\n",
    "# 1. 초기화\n",
    "pe = np.zeros((5, 6))\n",
    "print(\"초기화:\")\n",
    "print(pe)\n",
    "print()\n",
    "\n",
    "# 2. 위치 인덱스\n",
    "position = np.arange(5)[:, np.newaxis]\n",
    "print(\"위치 인덱스:\")\n",
    "print(position)\n",
    "print()\n",
    "\n",
    "# 3. 주파수 항\n",
    "div_term = np.exp(np.arange(0, 6, 2) * -(np.log(10000) / 6))\n",
    "print(\"주파수 항:\")\n",
    "print(div_term)\n",
    "print()\n",
    "\n",
    "# 4. Sin, Cos 적용\n",
    "pe[:, 0::2] = np.sin(position * div_term)  # 0, 2, 4번째 차원\n",
    "pe[:, 1::2] = np.cos(position * div_term)  # 1, 3, 5번째 차원\n",
    "\n",
    "print(\"최종 Positional Encoding:\")\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Positional Encoding의 효과\n",
      "==================================================\n",
      "문장: 'The cat and the dog'\n",
      "토큰: ['The', 'cat', 'and', 'the', 'dog']\n",
      "  'the'가 위치 0과 3에 나타남\n",
      "\n",
      "1. PE 없이:\n",
      "   'The' 벡터 평균: 0.100\n",
      "   'the' 벡터 평균: 0.100\n",
      "   → 동일한 벡터!\n",
      "\n",
      "2. PE 추가 후:\n",
      "   'The' (pos 0) 벡터 평균: 0.600\n",
      "   'the' (pos 3) 벡터 평균: 0.581\n",
      "   → 다른 벡터!\n",
      "\n",
      "→ PE를 통해 위치 정보가 추가되어 같은 단어도 구별 가능\n"
     ]
    }
   ],
   "source": [
    "# PE의 효과 시연\n",
    "print(\"🔬 Positional Encoding의 효과\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 동일한 단어가 다른 위치에 있을 때\n",
    "sentence = \"The cat and the dog\"\n",
    "tokens = sentence.split()\n",
    "print(f\"문장: '{sentence}'\")\n",
    "print(f\"토큰: {tokens}\")\n",
    "print(f\"  'the'가 위치 0과 3에 나타남\\n\")\n",
    "\n",
    "# 간단한 임베딩 (the는 같은 벡터)\n",
    "embedding_dim = 32\n",
    "word_embeddings = {\n",
    "    \"The\": np.ones(embedding_dim) * 0.1,\n",
    "    \"the\": np.ones(embedding_dim) * 0.1,  # 같은 벡터\n",
    "    \"cat\": np.ones(embedding_dim) * 0.2,\n",
    "    \"and\": np.ones(embedding_dim) * 0.3,\n",
    "    \"dog\": np.ones(embedding_dim) * 0.4\n",
    "}\n",
    "\n",
    "# 임베딩 행렬\n",
    "X = np.array([word_embeddings[token] for token in tokens])\n",
    "\n",
    "# PE 없이\n",
    "print(\"1. PE 없이:\")\n",
    "print(f\"   'The' 벡터 평균: {X[0].mean():.3f}\")\n",
    "print(f\"   'the' 벡터 평균: {X[3].mean():.3f}\")\n",
    "print(f\"   → 동일한 벡터!\\n\")\n",
    "\n",
    "# PE 추가\n",
    "X_with_pe = add_positional_encoding(X)\n",
    "print(\"2. PE 추가 후:\")\n",
    "print(f\"   'The' (pos 0) 벡터 평균: {X_with_pe[0].mean():.3f}\")\n",
    "print(f\"   'the' (pos 3) 벡터 평균: {X_with_pe[3].mean():.3f}\")\n",
    "print(f\"   → 다른 벡터!\")\n",
    "print()\n",
    "print(\"→ PE를 통해 위치 정보가 추가되어 같은 단어도 구별 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실전 예제: 간단한 Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Simple Transformer Block\n",
      "==================================================\n",
      "입력 shape: (10, 64)\n",
      "출력 shape: (10, 64)\n",
      "Attention shape: (8, 10, 10)\n",
      "\n",
      "구성 요소:\n",
      "  1. Multi-Head Attention\n",
      "  2. Residual Connection\n",
      "  3. Layer Normalization\n",
      "  4. Feed-Forward Network\n",
      "  5. Another Residual + LayerNorm\n",
      "\n",
      "→ 이것이 Transformer의 기본 블록입니다!\n"
     ]
    }
   ],
   "source": [
    "# 간단한 Transformer Block 구현\n",
    "class SimpleTransformerBlock:\n",
    "    \"\"\"간단한 Transformer 블록\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Feed-forward network weights\n",
    "        self.ff_w1 = np.random.randn(d_model, d_model * 4) * 0.1\n",
    "        self.ff_w2 = np.random.randn(d_model * 4, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Multi-Head Attention\n",
    "        attn_output, attn_weights = self.attention.forward(x, x, x, mask)\n",
    "        \n",
    "        # 2. Residual connection + Layer Norm (simplified)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # 3. Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # 4. Residual connection + Layer Norm\n",
    "        x = x + ff_output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x, attn_weights\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Position-wise feed-forward network\"\"\"\n",
    "        # Linear -> ReLU -> Linear\n",
    "        hidden = np.maximum(0, x @ self.ff_w1)  # ReLU\n",
    "        output = hidden @ self.ff_w2\n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x, eps=1e-6):\n",
    "        \"\"\"Layer normalization\"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + eps)\n",
    "\n",
    "# Transformer Block 테스트\n",
    "print(\"🏗️ Simple Transformer Block\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 생성\n",
    "transformer = SimpleTransformerBlock(d_model=64, num_heads=8)\n",
    "\n",
    "# 입력 (with PE)\n",
    "seq_len = 10\n",
    "X = np.random.randn(seq_len, 64)\n",
    "X = add_positional_encoding(X)\n",
    "\n",
    "# Forward pass\n",
    "output, attention = transformer.forward(X)\n",
    "\n",
    "print(f\"입력 shape: {X.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print()\n",
    "print(\"구성 요소:\")\n",
    "print(\"  1. Multi-Head Attention\")\n",
    "print(\"  2. Residual Connection\")\n",
    "print(\"  3. Layer Normalization\")\n",
    "print(\"  4. Feed-Forward Network\")\n",
    "print(\"  5. Another Residual + LayerNorm\")\n",
    "print()\n",
    "print(\"→ 이것이 Transformer의 기본 블록입니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 shape: (2, 3, 4)\n",
      "입력 값:\n",
      "[[[-2.04905521  1.47926649  0.89024727  0.37480383]\n",
      "  [-0.15420973 -1.5365512  -0.10442649  0.39583643]\n",
      "  [-1.32714425  0.77379263 -1.77039224  1.80327198]]\n",
      "\n",
      " [[ 0.05684915 -0.11519803  0.4638339  -1.2636555 ]\n",
      "  [ 1.37259032  0.74536726  1.30657607 -0.06256445]\n",
      "  [-1.6181128  -1.38441537  1.03840968 -0.158426  ]]]\n",
      "\n",
      "Attention 출력:\n",
      "[[[ 0.07653179  0.14218127  0.10137514  0.07876944]\n",
      "  [-0.05597804  0.21980439 -0.09463949  0.08145679]\n",
      "  [ 0.03367227  0.05592733 -0.13191882  0.18883682]]\n",
      "\n",
      " [[-0.23215914 -0.25038028 -0.18049511  0.10472162]\n",
      "  [-0.18826984 -0.08945959 -0.00230802  0.04703809]\n",
      "  [-0.04494361  0.15722834 -0.04599182  0.08773165]]]\n",
      "\n",
      "Residual 후:\n",
      "[[[-1.97252342  1.62144776  0.99162241  0.45357327]\n",
      "  [-0.21018777 -1.31674681 -0.19906599  0.47729323]\n",
      "  [-1.29347198  0.82971997 -1.90231106  1.9921088 ]]\n",
      "\n",
      " [[-0.17530999 -0.36557831  0.28333879 -1.15893388]\n",
      "  [ 1.18432048  0.65590767  1.30426805 -0.01552637]\n",
      "  [-1.66305641 -1.22718703  0.99241786 -0.07069435]]]\n",
      "\n",
      "Layer Norm 후:\n",
      "[[[-1.65024847  0.990359    0.52760584  0.13228364]\n",
      "  [ 0.15852683 -1.56145466  0.175814    1.22711383]\n",
      "  [-0.76222718  0.5864203  -1.1489606   1.32476748]]\n",
      "\n",
      " [[ 0.34314012 -0.021987    1.22329248 -1.5444456 ]\n",
      "  [ 0.77145406 -0.24239446  1.00159357 -1.53065318]\n",
      "  [-1.13027896 -0.70954032  1.43301334  0.40680594]]]\n",
      "\n",
      "Feed-Forward 출력:\n",
      "[[[ 0.16255674 -0.02045328 -0.0792261  -0.12532051]\n",
      "  [ 0.01806971  0.09968509  0.00239077 -0.0176617 ]\n",
      "  [-0.20946689  0.01460091 -0.03211118 -0.00763523]]\n",
      "\n",
      " [[-0.04473144  0.1417025   0.01868986  0.00986699]\n",
      "  [ 0.12519341  0.0252157   0.09595406  0.0814471 ]\n",
      "  [ 0.05255168 -0.02913822  0.02825949  0.03076944]]]\n",
      "\n",
      "두 번째 Residual 후:\n",
      "[[[-1.48769174  0.96990571  0.44837974  0.00696313]\n",
      "  [ 0.17659655 -1.46176957  0.17820477  1.20945213]\n",
      "  [-0.97169406  0.60102121 -1.18107178  1.31713225]]\n",
      "\n",
      " [[ 0.29840869  0.1197155   1.24198234 -1.53457861]\n",
      "  [ 0.89664747 -0.21717876  1.09754764 -1.44920608]\n",
      "  [-1.07772727 -0.73867854  1.46127283  0.43757538]]]\n",
      "\n",
      "최종 Layer Norm 후:\n",
      "[[[-1.60759221  1.07623746  0.50670281  0.02465194]\n",
      "  [ 0.15783528 -1.55497139  0.15951658  1.23761953]\n",
      "  [-0.86843842  0.62744886 -1.06758793  1.30857749]]\n",
      "\n",
      " [[ 0.26713038  0.08836782  1.21107036 -1.56656856]\n",
      "  [ 0.80185851 -0.29441819  0.99959328 -1.5070336 ]\n",
      "  [-1.09393851 -0.75624782  1.43489169  0.41529464]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 설정\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# 입력 데이터\n",
    "x = np.random.randn(batch_size, seq_len, d_model)\n",
    "print(f\"입력 shape: {x.shape}\")\n",
    "print(f\"입력 값:\\n{x}\")\n",
    "\n",
    "# 1단계: Self-Attention (가정)\n",
    "attn_output = np.random.randn(batch_size, seq_len, d_model) * 0.1\n",
    "print(f\"\\nAttention 출력:\\n{attn_output}\")\n",
    "\n",
    "# 2단계: Residual + Layer Norm\n",
    "x = x + attn_output\n",
    "print(f\"\\nResidual 후:\\n{x}\")\n",
    "\n",
    "# Layer Norm (간단한 버전)\n",
    "mean = np.mean(x, axis=-1, keepdims=True)\n",
    "var = np.var(x, axis=-1, keepdims=True)\n",
    "x = (x - mean) / np.sqrt(var + 1e-6)\n",
    "print(f\"\\nLayer Norm 후:\\n{x}\")\n",
    "\n",
    "# 3단계: Feed-Forward (가정)\n",
    "ff_output = np.random.randn(batch_size, seq_len, d_model) * 0.1\n",
    "print(f\"\\nFeed-Forward 출력:\\n{ff_output}\")\n",
    "\n",
    "# 4단계: 두 번째 Residual + Layer Norm\n",
    "x = x + ff_output\n",
    "print(f\"\\n두 번째 Residual 후:\\n{x}\")\n",
    "\n",
    "# 최종 Layer Norm\n",
    "mean = np.mean(x, axis=-1, keepdims=True)\n",
    "var = np.var(x, axis=-1, keepdims=True)\n",
    "x = (x - mean) / np.sqrt(var + 1e-6)\n",
    "print(f\"\\n최종 Layer Norm 후:\\n{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention 패턴 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 다양한 Attention 패턴\n",
      "==================================================\n",
      "\n",
      "Identity (자기 자신만):\n",
      " ██··············\n",
      " ··██············\n",
      " ····██··········\n",
      " ······██········\n",
      " ········██······\n",
      "\n",
      "Uniform (균등 분포):\n",
      " ░░░░░░░░░░░░░░░░\n",
      " ░░░░░░░░░░░░░░░░\n",
      " ░░░░░░░░░░░░░░░░\n",
      " ░░░░░░░░░░░░░░░░\n",
      " ░░░░░░░░░░░░░░░░\n",
      "\n",
      "Local (인접 토큰):\n",
      " ▓▓▓▓············\n",
      " ▓▓▓▓▓▓··········\n",
      " ··▓▓▓▓▓▓········\n",
      " ····▓▓▓▓▓▓······\n",
      " ······▓▓▓▓▓▓····\n",
      "\n",
      "Mixed (Global + Local):\n",
      " ▓▓▓▓············\n",
      " ▓▓▓▓▓▓··········\n",
      " ··▓▓▓▓▓▓········\n",
      " ····▓▓▓▓▓▓······\n",
      " ······▓▓▓▓▓▓····\n"
     ]
    }
   ],
   "source": [
    "# 다양한 Attention 패턴 생성 및 비교\n",
    "print(\"🎨 다양한 Attention 패턴\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 8\n",
    "\n",
    "# 1. Identity Attention (자기 자신만)\n",
    "identity_attention = np.eye(seq_len)\n",
    "\n",
    "# 2. Uniform Attention (균등)\n",
    "uniform_attention = np.ones((seq_len, seq_len)) / seq_len\n",
    "\n",
    "# 3. Local Attention (인접 토큰)\n",
    "local_attention = np.zeros((seq_len, seq_len))\n",
    "for i in range(seq_len):\n",
    "    for j in range(max(0, i-1), min(seq_len, i+2)):\n",
    "        local_attention[i, j] = 1/3\n",
    "\n",
    "# 4. Global + Local (혼합)\n",
    "mixed_attention = 0.7 * local_attention + 0.3 * uniform_attention\n",
    "\n",
    "# 시각화\n",
    "patterns = [\n",
    "    (\"Identity (자기 자신만)\", identity_attention),\n",
    "    (\"Uniform (균등 분포)\", uniform_attention),\n",
    "    (\"Local (인접 토큰)\", local_attention),\n",
    "    (\"Mixed (Global + Local)\", mixed_attention)\n",
    "]\n",
    "\n",
    "for name, pattern in patterns:\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i in range(min(5, seq_len)):\n",
    "        print(\" \", end=\"\")\n",
    "        for j in range(min(8, seq_len)):\n",
    "            val = pattern[i, j]\n",
    "            if val > 0.5:\n",
    "                print(\"██\", end=\"\")\n",
    "            elif val > 0.2:\n",
    "                print(\"▓▓\", end=\"\")\n",
    "            elif val > 0.05:\n",
    "                print(\"░░\", end=\"\")\n",
    "            else:\n",
    "                print(\"··\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Attention 계산 복잡도\n",
      "==================================================\n",
      "d_model = 64\n",
      "\n",
      "Seq Length | Time (ms) | Memory (MB) | Complexity\n",
      "-------------------------------------------------------\n",
      "        10 |      0.01 |        0.00 | O(n²)\n",
      "        50 |      0.05 |        0.02 | O(n²)\n",
      "       100 |      0.11 |        0.08 | O(n²)\n",
      "       200 |      0.31 |        0.31 | O(n²)\n",
      "\n",
      "💡 관찰:\n",
      "  - 시퀀스 길이의 제곱에 비례하는 계산량\n",
      "  - 긴 시퀀스에서 메모리 문제 발생 가능\n",
      "  - 이래서 효율적인 Attention 변형들이 연구됨\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Attention 계산 복잡도 분석\n",
    "print(\"⚡ Attention 계산 복잡도\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 다양한 시퀀스 길이에서 테스트\n",
    "seq_lengths = [10, 50, 100, 200]\n",
    "d_model = 64\n",
    "\n",
    "print(f\"d_model = {d_model}\\n\")\n",
    "print(\"Seq Length | Time (ms) | Memory (MB) | Complexity\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    # 입력 생성\n",
    "    Q = K = V = np.random.randn(seq_len, d_model)\n",
    "    \n",
    "    # 시간 측정\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _, _ = scaled_dot_product_attention(Q, K, V)\n",
    "    elapsed = (time.time() - start) / 100 * 1000  # ms\n",
    "    \n",
    "    # 메모리 추정 (attention matrix)\n",
    "    memory_mb = (seq_len * seq_len * 8) / (1024 * 1024)  # 8 bytes per float64\n",
    "    \n",
    "    print(f\"{seq_len:10d} | {elapsed:9.2f} | {memory_mb:11.2f} | O(n²)\")\n",
    "\n",
    "print(\"\\n💡 관찰:\")\n",
    "print(\"  - 시퀀스 길이의 제곱에 비례하는 계산량\")\n",
    "print(\"  - 긴 시퀀스에서 메모리 문제 발생 가능\")\n",
    "print(\"  - 이래서 효율적인 Attention 변형들이 연구됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 45716 (\\N{HANGUL SYLLABLE NEUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 54617 (\\N{HANGUL SYLLABLE HAG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 44052 (\\N{HANGUL SYLLABLE GASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/seaborn/utils.py:61: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 45716 (\\N{HANGUL SYLLABLE NEUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 54617 (\\N{HANGUL SYLLABLE HAG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 44052 (\\N{HANGUL SYLLABLE GASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/anaconda3/envs/ai/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_attention(attention_weights, tokens):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attention_weights, \n",
    "                xticklabels=tokens, \n",
    "                yticklabels=tokens,\n",
    "                annot=True, \n",
    "                cmap='Blues')\n",
    "    plt.title('Attention Weights Visualization')\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.show()\n",
    "\n",
    "# 예시 데이터\n",
    "tokens = [\"나는\", \"학교에\", \"갔다\"]\n",
    "attention_weights = [\n",
    "    [0.8, 0.15, 0.05],  # \"나는\"의 attention\n",
    "    [0.2, 0.6, 0.2],    # \"학교에\"의 attention\n",
    "    [0.1, 0.2, 0.7]     # \"갔다\"의 attention\n",
    "]\n",
    "\n",
    "visualize_attention(attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 축하합니다!\n",
    "\n",
    "Attention 메커니즘의 핵심을 모두 구현하고 이해했습니다!\n",
    "\n",
    "### 배운 내용:\n",
    "1. ✅ Scaled Dot-Product Attention\n",
    "2. ✅ Self-Attention과 Cross-Attention\n",
    "3. ✅ Causal Masking\n",
    "4. ✅ Multi-Head Attention\n",
    "5. ✅ Positional Encoding\n",
    "6. ✅ 간단한 Transformer Block\n",
    "\n",
    "### 다음 단계:\n",
    "- Day 4: 완전한 Transformer 구현\n",
    "- Layer Normalization, Residual Connection\n",
    "- Encoder-Decoder 구조\n",
    "\n",
    "\"Attention is literally all you need!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
