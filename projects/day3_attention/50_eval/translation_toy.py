"""
ğŸŒ Translation Toy: ê°„ë‹¨í•œ ë²ˆì—­ íƒœìŠ¤í¬

Attentionì„ ì‚¬ìš©í•œ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ í•™ìŠµì˜ ê°„ë‹¨í•œ ì˜ˆì œì…ë‹ˆë‹¤.
ìˆ«ìë¥¼ ì˜ì–´ì—ì„œ í”„ë‘ìŠ¤ì–´ë¡œ "ë²ˆì—­"í•˜ëŠ” íƒœìŠ¤í¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from core.attention import (
    MultiHeadAttention,
    add_positional_encoding,
    create_causal_mask,
    visualize_attention
)


# ê°„ë‹¨í•œ "ë²ˆì—­" ë°ì´í„°ì…‹
TRANSLATION_DATA = {
    "one": "un",
    "two": "deux", 
    "three": "trois",
    "four": "quatre",
    "five": "cinq",
    "six": "six",
    "seven": "sept",
    "eight": "huit",
    "nine": "neuf",
    "ten": "dix"
}

# ë” ë³µì¡í•œ ì˜ˆì œ
PHRASE_DATA = {
    "hello world": "bonjour monde",
    "good morning": "bon matin",
    "thank you": "merci",
    "see you": "Ã  bientÃ´t",
    "how are you": "comment allez vous"
}


class SimpleSeq2SeqWithAttention:
    """
    Attentionì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ Sequence-to-Sequence ëª¨ë¸
    """
    
    def __init__(self, vocab_size, d_model=64, num_heads=4):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        
        # ë‹¨ì–´ ì„ë² ë”© (ëœë¤ ì´ˆê¸°í™”)
        self.embedding = np.random.randn(vocab_size, d_model) * 0.1
        
        # Encoder self-attention
        self.encoder_attention = MultiHeadAttention(d_model, num_heads)
        
        # Decoder self-attention
        self.decoder_self_attention = MultiHeadAttention(d_model, num_heads)
        
        # Decoder cross-attention (encoder ì¶œë ¥ ì°¸ì¡°)
        self.decoder_cross_attention = MultiHeadAttention(d_model, num_heads)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_projection = np.random.randn(d_model, vocab_size) * 0.1
        
        # ì–´íœ˜ ì‚¬ì „
        self.word_to_idx = {}
        self.idx_to_word = {}
        self._build_vocab()
    
    def _build_vocab(self):
        """ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•"""
        words = set()
        
        # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
        for en, fr in {**TRANSLATION_DATA, **PHRASE_DATA}.items():
            words.update(en.split())
            words.update(fr.split())
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        words.add("<PAD>")
        words.add("<START>")
        words.add("<END>")
        
        # ì¸ë±ìŠ¤ ë§¤í•‘
        for idx, word in enumerate(sorted(words)):
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
    
    def encode_sentence(self, sentence):
        """ë¬¸ì¥ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        tokens = sentence.split()
        indices = [self.word_to_idx.get(token, 0) for token in tokens]
        return indices
    
    def decode_indices(self, indices):
        """ì¸ë±ìŠ¤ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
        tokens = [self.idx_to_word.get(idx, "<UNK>") for idx in indices]
        return " ".join(tokens)
    
    def embed_tokens(self, token_indices):
        """í† í° ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        for idx in token_indices:
            if idx < len(self.embedding):
                embeddings.append(self.embedding[idx])
            else:
                embeddings.append(np.zeros(self.d_model))
        return np.array(embeddings)
    
    def encode(self, source_sentence):
        """
        Encoder: ì†ŒìŠ¤ ë¬¸ì¥ ì¸ì½”ë”©
        """
        # í† í°í™” ë° ì„ë² ë”©
        indices = self.encode_sentence(source_sentence)
        embeddings = self.embed_tokens(indices)
        
        # Positional encoding ì¶”ê°€
        embeddings = add_positional_encoding(embeddings)
        
        # Self-attention
        encoded, enc_attention = self.encoder_attention.forward(
            embeddings, embeddings, embeddings
        )
        
        return encoded, enc_attention, indices
    
    def decode(self, target_sentence, encoder_output):
        """
        Decoder: íƒ€ê²Ÿ ë¬¸ì¥ ë””ì½”ë”©
        """
        # í† í°í™” ë° ì„ë² ë”©
        indices = self.encode_sentence(target_sentence)
        embeddings = self.embed_tokens(indices)
        
        # Positional encoding ì¶”ê°€
        embeddings = add_positional_encoding(embeddings)
        
        # Causal mask (ë¯¸ë˜ ë‹¨ì–´ ì°¨ë‹¨)
        seq_len = len(indices)
        causal_mask = create_causal_mask(seq_len)
        
        # Self-attention (with causal mask)
        self_attn_out, self_attn_weights = self.decoder_self_attention.forward(
            embeddings, embeddings, embeddings, mask=causal_mask
        )
        
        # Cross-attention (encoder ì°¸ì¡°)
        cross_attn_out, cross_attn_weights = self.decoder_cross_attention.forward(
            self_attn_out, encoder_output, encoder_output
        )
        
        # ì¶œë ¥ projection
        logits = np.matmul(cross_attn_out, self.output_projection)
        
        return logits, self_attn_weights, cross_attn_weights, indices
    
    def translate(self, source_sentence, visualize=True):
        """
        ë²ˆì—­ ìˆ˜í–‰
        """
        print(f"\n{'=' * 60}")
        print(f"ì†ŒìŠ¤ (ì˜ì–´): {source_sentence}")
        
        # Encoding
        encoder_output, enc_attention, src_indices = self.encode(source_sentence)
        src_tokens = source_sentence.split()
        
        # ì‹¤ì œ ë²ˆì—­ (ì—¬ê¸°ì„œëŠ” ì •ë‹µ ì‚¬ìš©)
        if source_sentence in TRANSLATION_DATA:
            target_sentence = TRANSLATION_DATA[source_sentence]
        elif source_sentence in PHRASE_DATA:
            target_sentence = PHRASE_DATA[source_sentence]
        else:
            target_sentence = "unknown"
        
        print(f"íƒ€ê²Ÿ (í”„ë‘ìŠ¤ì–´): {target_sentence}")
        
        # Decoding
        logits, self_attn, cross_attn, tgt_indices = self.decode(
            target_sentence, encoder_output
        )
        tgt_tokens = target_sentence.split()
        
        # Attention ì‹œê°í™”
        if visualize:
            print("\nğŸ“Š Encoder Self-Attention:")
            self._visualize_attention(enc_attention.mean(axis=0), src_tokens, src_tokens)
            
            print("\nğŸ“Š Decoder Self-Attention:")
            self._visualize_attention(self_attn.mean(axis=0), tgt_tokens, tgt_tokens)
            
            print("\nğŸ“Š Cross-Attention (Decoder â†’ Encoder):")
            self._visualize_attention(cross_attn.mean(axis=0), tgt_tokens, src_tokens)
        
        return target_sentence
    
    def _visualize_attention(self, weights, query_tokens, key_tokens):
        """Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        print("\n      ", end="")
        for token in key_tokens:
            print(f"{token[:6]:^8}", end="")
        print()
        
        for i, q_token in enumerate(query_tokens):
            print(f"{q_token[:6]:>6}", end="")
            for j, k_token in enumerate(key_tokens):
                weight = weights[i, j] if i < len(weights) and j < len(weights[0]) else 0
                
                if weight > 0.5:
                    print("  â–ˆâ–ˆâ–ˆâ–ˆ  ", end="")
                elif weight > 0.3:
                    print("  â–“â–“â–“  ", end="")
                elif weight > 0.15:
                    print("  â–’â–’  ", end="")
                elif weight > 0.05:
                    print("  â–‘â–‘  ", end="")
                else:
                    print("  Â·Â·  ", end="")
            print()


def demonstrate_translation():
    """
    ë²ˆì—­ ë°ëª¨
    """
    print("=" * 60)
    print("ğŸŒ Attentionì„ ì‚¬ìš©í•œ ë²ˆì—­ ë°ëª¨")
    print("=" * 60)
    
    # ëª¨ë¸ ìƒì„±
    model = SimpleSeq2SeqWithAttention(vocab_size=50, d_model=32, num_heads=4)
    
    print("\nğŸ“š ë²ˆì—­ ë°ì´í„°ì…‹:")
    print("-" * 40)
    for en, fr in list(TRANSLATION_DATA.items())[:5]:
        print(f"  {en:10} â†’ {fr}")
    print("  ...")
    
    # ë‹¨ìˆœ ë‹¨ì–´ ë²ˆì—­
    print("\n" + "=" * 60)
    print("1ï¸âƒ£ ë‹¨ì¼ ë‹¨ì–´ ë²ˆì—­")
    print("=" * 60)
    
    for word in ["one", "five", "ten"]:
        model.translate(word, visualize=True)
    
    # êµ¬ë¬¸ ë²ˆì—­
    print("\n" + "=" * 60)
    print("2ï¸âƒ£ êµ¬ë¬¸ ë²ˆì—­")
    print("=" * 60)
    
    for phrase in ["hello world", "thank you"]:
        model.translate(phrase, visualize=True)


def analyze_attention_patterns():
    """
    Attention íŒ¨í„´ ë¶„ì„
    """
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Attention íŒ¨í„´ ë¶„ì„")
    print("=" * 60)
    
    model = SimpleSeq2SeqWithAttention(vocab_size=50, d_model=32, num_heads=4)
    
    # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ë¬¸ì¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_sentences = [
        ("one", "un"),
        ("one two", "un deux"),
        ("one two three", "un deux trois")
    ]
    
    for src, tgt in test_sentences:
        print(f"\nê¸¸ì´ {len(src.split())} â†’ {len(tgt.split())} ë²ˆì—­:")
        print(f"  {src} â†’ {tgt}")
        
        # Encoding
        encoder_output, _, _ = model.encode(src)
        
        # Decoding
        _, _, cross_attn, _ = model.decode(tgt, encoder_output)
        
        # Cross-attention ë¶„ì„
        avg_cross_attn = cross_attn.mean(axis=0)
        
        print("\nCross-Attention íŒ¨í„´:")
        src_tokens = src.split()
        tgt_tokens = tgt.split()
        
        for i, t_tok in enumerate(tgt_tokens):
            if i < len(avg_cross_attn):
                max_idx = np.argmax(avg_cross_attn[i])
                if max_idx < len(src_tokens):
                    print(f"  {t_tok} â†’ {src_tokens[max_idx]} "
                          f"(attention: {avg_cross_attn[i, max_idx]:.3f})")


def interactive_translation():
    """
    ëŒ€í™”í˜• ë²ˆì—­
    """
    print("\n" + "=" * 60)
    print("ğŸ® ëŒ€í™”í˜• ë²ˆì—­ ì‹¤í—˜")
    print("=" * 60)
    
    model = SimpleSeq2SeqWithAttention(vocab_size=50, d_model=32, num_heads=4)
    
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ë‹¨ì–´:")
    print("-" * 40)
    print("ì˜ì–´:", ", ".join(list(TRANSLATION_DATA.keys())[:10]))
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ë¬¸:")
    print("ì˜ì–´:", ", ".join(list(PHRASE_DATA.keys())[:5]))
    
    while True:
        print("\n" + "-" * 60)
        user_input = input("\nì˜ì–´ ì…ë ¥ (ì¢…ë£Œ: 'quit'): ").strip().lower()
        
        if user_input == 'quit':
            break
        
        if user_input in TRANSLATION_DATA or user_input in PHRASE_DATA:
            model.translate(user_input, visualize=True)
        else:
            print("âš ï¸ ë¯¸ë“±ë¡ ë‹¨ì–´/êµ¬ë¬¸ì…ë‹ˆë‹¤. ë“±ë¡ëœ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ" * 30)
    print("Translation with Attention".center(60))
    print("ğŸŒ" * 30)
    
    # 1. ê¸°ë³¸ ë²ˆì—­ ë°ëª¨
    demonstrate_translation()
    
    # 2. Attention íŒ¨í„´ ë¶„ì„
    analyze_attention_patterns()
    
    # 3. ëŒ€í™”í˜• ì‹¤í—˜
    print("\n" + "=" * 60)
    response = input("\nëŒ€í™”í˜• ë²ˆì—­ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() == 'y':
        interactive_translation()
    
    print("\n" + "ğŸ‰" * 20)
    print("ë²ˆì—­ ë°ëª¨ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
    print("Cross-Attentionì´ ì–´ë–»ê²Œ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì„ ì—°ê²°í•˜ëŠ”ì§€ ë³´ì…¨ë‚˜ìš”?")
    print("ğŸ‰" * 20)


if __name__ == "__main__":
    main()