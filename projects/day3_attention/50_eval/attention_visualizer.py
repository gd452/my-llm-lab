"""
ğŸ‘ï¸ Attention Visualizer: Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”

Attentionì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ì–´ë–»ê²Œ ì£¼ëª©í•˜ëŠ”ì§€ ë´…ë‹ˆë‹¤.
"""

import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from core.attention import (
    scaled_dot_product_attention,
    MultiHeadAttention,
    positional_encoding,
    add_positional_encoding,
    create_causal_mask
)


def create_word_embeddings(words, embedding_dim=64):
    """
    ê°„ë‹¨í•œ ë‹¨ì–´ ì„ë² ë”© ìƒì„± (ëœë¤)
    ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ì„ë² ë”©ì„ ì‚¬ìš©
    """
    np.random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•´
    vocab = {}
    embeddings = []
    
    for word in words:
        if word not in vocab:
            vocab[word] = np.random.randn(embedding_dim)
        embeddings.append(vocab[word])
    
    return np.array(embeddings), vocab


def visualize_attention_heatmap(attention_weights, tokens, title="Attention Weights"):
    """
    Attention ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
    """
    print(f"\n{'=' * 60}")
    print(f"{title:^60}")
    print('=' * 60)
    
    seq_len = len(tokens)
    
    # í—¤ë” (Query)
    print("\n" + " " * 12 + "Keys (ì–´ë””ë¥¼ ë³¼ ê²ƒì¸ê°€?)")
    print(" " * 10, end="")
    for token in tokens:
        print(f"{token[:8]:^10}", end="")
    print()
    print(" " * 10 + "â”€" * (10 * seq_len))
    
    # íˆíŠ¸ë§µ
    for i, query_token in enumerate(tokens):
        print(f"{query_token[:8]:>8} â”‚", end="")
        
        for j, key_token in enumerate(tokens):
            weight = attention_weights[i, j]
            
            # ìƒ‰ìƒ ê°•ë„ë¡œ í‘œí˜„
            if weight > 0.5:
                symbol = "â–ˆâ–ˆâ–ˆâ–ˆ"
            elif weight > 0.3:
                symbol = "â–“â–“â–“â–“"
            elif weight > 0.15:
                symbol = "â–’â–’â–’â–’"
            elif weight > 0.05:
                symbol = "â–‘â–‘â–‘â–‘"
            else:
                symbol = "Â·Â·Â·Â·"
            
            # ëŒ€ê°ì„  (ìê¸° ìì‹ ) ê°•ì¡°
            if i == j:
                print(f" [{symbol}] ", end="")
            else:
                print(f"  {symbol}  ", end="")
        
        print(f" â”‚ {query_token}")
    
    print(" " * 10 + "â”€" * (10 * seq_len))
    print("\nQueries")
    print("(ë¬´ì—‡ì„")
    print("ì°¾ëŠ”ê°€?)")
    
    # ë²”ë¡€
    print("\në²”ë¡€: â–ˆâ–ˆâ–ˆâ–ˆ (>0.5) â–“â–“â–“â–“ (>0.3) â–’â–’â–’â–’ (>0.15) â–‘â–‘â–‘â–‘ (>0.05) Â·Â·Â·Â· (<0.05)")


def demonstrate_self_attention():
    """
    Self-Attention ë°ëª¨
    """
    print("\n" + "ğŸ” Self-Attention ë°ëª¨ ".center(60, "="))
    
    # ì˜ˆì œ ë¬¸ì¥
    sentence = "The cat sat on the mat"
    tokens = sentence.split()
    print(f"\në¬¸ì¥: '{sentence}'")
    print(f"í† í°: {tokens}")
    
    # ì„ë² ë”© ìƒì„±
    embeddings, vocab = create_word_embeddings(tokens, embedding_dim=32)
    print(f"\nì„ë² ë”© shape: {embeddings.shape}")
    
    # Positional encoding ì¶”ê°€
    embeddings_with_pe = add_positional_encoding(embeddings)
    
    # Self-attention ê³„ì‚°
    output, attention_weights = scaled_dot_product_attention(
        embeddings_with_pe, 
        embeddings_with_pe, 
        embeddings_with_pe
    )
    
    # ì‹œê°í™”
    visualize_attention_heatmap(attention_weights, tokens, 
                               "Self-Attention (ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œë¥¼ ë³¼ ìˆ˜ ìˆìŒ)")
    
    # ë¶„ì„
    print("\nğŸ“Š Attention ë¶„ì„:")
    for i, token in enumerate(tokens):
        top_indices = np.argsort(attention_weights[i])[-3:][::-1]
        top_tokens = [tokens[idx] for idx in top_indices]
        top_weights = [attention_weights[i, idx] for idx in top_indices]
        
        print(f"  '{token}' ì£¼ëª© â†’ ", end="")
        for t, w in zip(top_tokens, top_weights):
            print(f"{t}({w:.2f}) ", end="")
        print()


def demonstrate_causal_attention():
    """
    Causal Attention ë°ëª¨ (GPT ìŠ¤íƒ€ì¼)
    """
    print("\n" + "ğŸ”® Causal Attention ë°ëª¨ ".center(60, "="))
    
    # ì˜ˆì œ ë¬¸ì¥
    sentence = "I think therefore I am"
    tokens = sentence.split()
    print(f"\në¬¸ì¥: '{sentence}'")
    print(f"í† í°: {tokens}")
    
    # ì„ë² ë”© ìƒì„±
    embeddings, _ = create_word_embeddings(tokens, embedding_dim=32)
    embeddings_with_pe = add_positional_encoding(embeddings)
    
    # Causal mask ìƒì„±
    seq_len = len(tokens)
    causal_mask = create_causal_mask(seq_len)
    
    # Causal self-attention
    output, attention_weights = scaled_dot_product_attention(
        embeddings_with_pe,
        embeddings_with_pe,
        embeddings_with_pe,
        mask=causal_mask
    )
    
    # ì‹œê°í™”
    visualize_attention_heatmap(attention_weights, tokens,
                               "Causal Attention (ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ)")
    
    print("\nğŸ’¡ íŠ¹ì§•:")
    print("  - ê° ë‹¨ì–´ëŠ” ìì‹ ê³¼ ì´ì „ ë‹¨ì–´ë“¤ë§Œ ë³¼ ìˆ˜ ìˆìŒ")
    print("  - í•˜ì‚¼ê° í–‰ë ¬ í˜•íƒœ")
    print("  - GPTì™€ ê°™ì€ ìƒì„± ëª¨ë¸ì—ì„œ ì‚¬ìš©")


def demonstrate_multi_head_attention():
    """
    Multi-Head Attention ë°ëª¨
    """
    print("\n" + "ğŸ­ Multi-Head Attention ë°ëª¨ ".center(60, "="))
    
    # ì˜ˆì œ ë¬¸ì¥
    sentence = "Time flies like an arrow"
    tokens = sentence.split()
    print(f"\në¬¸ì¥: '{sentence}'")
    print(f"í† í°: {tokens}")
    
    # ì„ë² ë”© ìƒì„±
    embeddings, _ = create_word_embeddings(tokens, embedding_dim=64)
    embeddings_with_pe = add_positional_encoding(embeddings)
    
    # Multi-head attention
    num_heads = 4
    mha = MultiHeadAttention(d_model=64, num_heads=num_heads)
    
    output, attention_weights = mha.forward(
        embeddings_with_pe,
        embeddings_with_pe,
        embeddings_with_pe
    )
    
    print(f"\n{num_heads}ê°œì˜ Attention Head:")
    print("ê° headê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.")
    
    # ê° head ì‹œê°í™”
    for head_idx in range(min(2, num_heads)):  # ì²˜ìŒ 2ê°œ headë§Œ
        print(f"\n{'â”€' * 60}")
        visualize_attention_heatmap(
            attention_weights[head_idx],
            tokens,
            f"Head {head_idx + 1}/{num_heads}"
        )
    
    # í‰ê·  attention
    avg_attention = attention_weights.mean(axis=0)
    print(f"\n{'â”€' * 60}")
    visualize_attention_heatmap(
        avg_attention,
        tokens,
        "í‰ê·  Attention (ëª¨ë“  headì˜ í‰ê· )"
    )


def analyze_positional_encoding():
    """
    Positional Encoding ë¶„ì„
    """
    print("\n" + "ğŸ“ Positional Encoding ë¶„ì„ ".center(60, "="))
    
    seq_len = 20
    d_model = 64
    
    pe = positional_encoding(seq_len, d_model)
    
    print(f"\nPositional Encoding shape: {pe.shape}")
    print(f"ê°’ ë²”ìœ„: [{pe.min():.3f}, {pe.max():.3f}]")
    
    # ìœ„ì¹˜ë³„ íŒ¨í„´ ì‹œê°í™”
    print("\nìœ„ì¹˜ë³„ ì¸ì½”ë”© íŒ¨í„´ (ì²˜ìŒ 8ì°¨ì›):")
    print("Pos ", end="")
    for dim in range(8):
        print(f" Dim{dim:2d}", end="")
    print()
    print("â”€" * 50)
    
    for pos in range(min(10, seq_len)):
        print(f"{pos:3d} ", end="")
        for dim in range(8):
            val = pe[pos, dim]
            if val > 0.5:
                print("  â–ˆâ–ˆ ", end="")
            elif val > 0:
                print("  â–“â–“ ", end="")
            elif val > -0.5:
                print("  â–‘â–‘ ", end="")
            else:
                print("  Â·Â· ", end="")
        print()
    
    # ì£¼íŒŒìˆ˜ ë¶„ì„
    print("\nğŸ“Š ì°¨ì›ë³„ ì£¼íŒŒìˆ˜:")
    for dim in [0, d_model//4, d_model//2, d_model-1]:
        # ì£¼ê¸° ê³„ì‚°
        if dim % 2 == 0:
            freq = 1.0 / (10000 ** (dim / d_model))
            wavelength = 2 * np.pi / freq
            print(f"  Dim {dim:3d}: íŒŒì¥ â‰ˆ {wavelength:.1f} positions")


def interactive_attention():
    """
    ëŒ€í™”í˜• Attention ì‹¤í—˜
    """
    print("\n" + "ğŸ® ëŒ€í™”í˜• Attention ì‹¤í—˜ ".center(60, "="))
    print("\nìì‹ ë§Œì˜ ë¬¸ì¥ìœ¼ë¡œ Attentionì„ ì‹¤í—˜í•´ë³´ì„¸ìš”!")
    
    while True:
        print("\n" + "â”€" * 60)
        user_input = input("\në¬¸ì¥ ì…ë ¥ (ì¢…ë£Œ: 'quit'): ").strip()
        
        if user_input.lower() == 'quit':
            break
        
        if not user_input:
            continue
        
        tokens = user_input.split()
        if len(tokens) < 2:
            print("ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        
        if len(tokens) > 10:
            print("10ê°œ ì´í•˜ì˜ ë‹¨ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        
        # ì„ë² ë”© ìƒì„±
        embeddings, _ = create_word_embeddings(tokens, embedding_dim=32)
        embeddings_with_pe = add_positional_encoding(embeddings)
        
        # Attention ê³„ì‚°
        output, attention_weights = scaled_dot_product_attention(
            embeddings_with_pe,
            embeddings_with_pe,
            embeddings_with_pe
        )
        
        # ì‹œê°í™”
        visualize_attention_heatmap(attention_weights, tokens, "Your Attention")
        
        # ê°€ì¥ ê°•í•œ ì—°ê²° ì°¾ê¸°
        print("\nğŸ”— ê°€ì¥ ê°•í•œ ì—°ê²° (ìê¸° ìì‹  ì œì™¸):")
        for i in range(len(tokens)):
            attention_weights[i, i] = 0  # ìê¸° ìì‹  ì œì™¸
        
        max_idx = np.unravel_index(attention_weights.argmax(), attention_weights.shape)
        max_weight = attention_weights[max_idx]
        
        print(f"  '{tokens[max_idx[0]]}' â†’ '{tokens[max_idx[1]]}' "
              f"(weight: {max_weight:.3f})")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ‘ï¸  Attention Mechanism Visualizer")
    print("=" * 60)
    
    # 1. Self-Attention ë°ëª¨
    demonstrate_self_attention()
    
    # 2. Causal Attention ë°ëª¨
    demonstrate_causal_attention()
    
    # 3. Multi-Head Attention ë°ëª¨
    demonstrate_multi_head_attention()
    
    # 4. Positional Encoding ë¶„ì„
    analyze_positional_encoding()
    
    # 5. ëŒ€í™”í˜• ì‹¤í—˜
    print("\n" + "=" * 60)
    response = input("\nëŒ€í™”í˜• ì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() == 'y':
        interactive_attention()
    
    print("\n" + "ğŸ‰" * 20)
    print("Attention ì‹œê°í™”ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
    print("ì´ì œ Attentionì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì´í•´í•˜ì…¨ë‚˜ìš”?")
    print("ğŸ‰" * 20)


if __name__ == "__main__":
    main()