"""
ðŸ§ª Attention ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸
"""

import pytest
import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from core.attention import (
    scaled_dot_product_attention,
    MultiHeadAttention,
    positional_encoding,
    create_causal_mask,
    create_padding_mask,
    add_positional_encoding
)


class TestScaledDotProductAttention:
    """Scaled Dot-Product Attention í…ŒìŠ¤íŠ¸"""
    
    def test_basic_attention(self):
        """ê¸°ë³¸ attention ë™ìž‘ í…ŒìŠ¤íŠ¸"""
        Q = np.random.randn(10, 64)
        K = np.random.randn(10, 64)
        V = np.random.randn(10, 128)
        
        output, weights = scaled_dot_product_attention(Q, K, V)
        
        # Shape í™•ì¸
        assert output.shape == (10, 128)
        assert weights.shape == (10, 10)
        
        # Attention weights í•©ì´ 1
        np.testing.assert_almost_equal(weights.sum(axis=1), np.ones(10))
        
        # ëª¨ë“  weightsê°€ 0~1 ì‚¬ì´
        assert np.all(weights >= 0) and np.all(weights <= 1)
    
    def test_batch_attention(self):
        """ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        batch_size = 4
        seq_len = 8
        d_k = 32
        d_v = 16
        
        Q = np.random.randn(batch_size, seq_len, d_k)
        K = np.random.randn(batch_size, seq_len, d_k)
        V = np.random.randn(batch_size, seq_len, d_v)
        
        output, weights = scaled_dot_product_attention(Q, K, V)
        
        assert output.shape == (batch_size, seq_len, d_v)
        assert weights.shape == (batch_size, seq_len, seq_len)
    
    def test_with_mask(self):
        """ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸"""
        seq_len = 5
        Q = K = V = np.random.randn(seq_len, 16)
        
        # Causal mask
        mask = create_causal_mask(seq_len)
        output, weights = scaled_dot_product_attention(Q, K, V, mask=mask)
        
        # ë¯¸ëž˜ ìœ„ì¹˜ì˜ attentionì´ 0ì¸ì§€ í™•ì¸
        for i in range(seq_len):
            for j in range(i + 1, seq_len):
                assert weights[i, j] < 1e-8
    
    def test_attention_pattern(self):
        """íŠ¹ì • attention íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        # Identity attention (ìžê¸° ìžì‹ ì—ë§Œ ì£¼ëª©)
        seq_len = 4
        d = 8
        
        # Qì™€ Kë¥¼ ì§êµí•˜ê²Œ ì„¤ì •
        Q = np.eye(seq_len, d)
        K = np.eye(seq_len, d)
        V = np.random.randn(seq_len, d)
        
        output, weights = scaled_dot_product_attention(Q, K, V)
        
        # ëŒ€ê°ì„ ì´ ê°€ìž¥ í° ê°’
        for i in range(seq_len):
            assert np.argmax(weights[i]) == i


class TestMultiHeadAttention:
    """Multi-Head Attention í…ŒìŠ¤íŠ¸"""
    
    def test_multi_head_shapes(self):
        """Multi-head ì¶œë ¥ shape í…ŒìŠ¤íŠ¸"""
        d_model = 512
        num_heads = 8
        seq_len = 20
        
        mha = MultiHeadAttention(d_model, num_heads)
        x = np.random.randn(seq_len, d_model)
        
        output, weights = mha.forward(x, x, x)
        
        assert output.shape == (seq_len, d_model)
        assert weights.shape == (num_heads, seq_len, seq_len)
    
    def test_head_dimension_split(self):
        """Head ì°¨ì› ë¶„í•  í…ŒìŠ¤íŠ¸"""
        d_model = 256
        num_heads = 4
        d_k = d_model // num_heads  # 64
        
        mha = MultiHeadAttention(d_model, num_heads)
        assert mha.d_k == d_k
    
    def test_batch_multi_head(self):
        """ë°°ì¹˜ ì²˜ë¦¬ multi-head í…ŒìŠ¤íŠ¸"""
        batch_size = 2
        seq_len = 10
        d_model = 128
        num_heads = 4
        
        mha = MultiHeadAttention(d_model, num_heads)
        x = np.random.randn(batch_size, seq_len, d_model)
        
        output, weights = mha.forward(x, x, x)
        
        assert output.shape == (batch_size, seq_len, d_model)
        assert weights.shape == (batch_size, num_heads, seq_len, seq_len)
    
    def test_cross_attention(self):
        """Cross-attention í…ŒìŠ¤íŠ¸"""
        d_model = 64
        num_heads = 2
        
        mha = MultiHeadAttention(d_model, num_heads)
        
        # ë‹¤ë¥¸ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤
        query = np.random.randn(5, d_model)  # 5 queries
        key = np.random.randn(8, d_model)    # 8 keys
        value = np.random.randn(8, d_model)  # 8 values
        
        output, weights = mha.forward(query, key, value)
        
        assert output.shape == (5, d_model)
        assert weights.shape == (num_heads, 5, 8)


class TestPositionalEncoding:
    """Positional Encoding í…ŒìŠ¤íŠ¸"""
    
    def test_pe_shape(self):
        """PE shape í…ŒìŠ¤íŠ¸"""
        seq_len = 100
        d_model = 512
        
        pe = positional_encoding(seq_len, d_model)
        assert pe.shape == (seq_len, d_model)
    
    def test_pe_range(self):
        """PE ê°’ ë²”ìœ„ í…ŒìŠ¤íŠ¸"""
        pe = positional_encoding(50, 128)
        
        # Sin/Cos ê°’ì€ -1~1 ì‚¬ì´
        assert pe.min() >= -1.0
        assert pe.max() <= 1.0
    
    def test_pe_pattern(self):
        """PE íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        pe = positional_encoding(100, 4)
        
        # ì²« ë²ˆì§¸ ìœ„ì¹˜ëŠ” íŠ¹ì • íŒ¨í„´
        # PE(0, 0) = sin(0) = 0
        assert abs(pe[0, 0]) < 1e-10
        
        # PE(0, 1) = cos(0) = 1
        assert abs(pe[0, 1] - 1.0) < 1e-10
    
    def test_add_pe(self):
        """PE ì¶”ê°€ í…ŒìŠ¤íŠ¸"""
        # 2D ìž…ë ¥
        x_2d = np.random.randn(10, 64)
        x_with_pe = add_positional_encoding(x_2d)
        assert x_with_pe.shape == x_2d.shape
        
        # 3D ìž…ë ¥ (ë°°ì¹˜)
        x_3d = np.random.randn(2, 10, 64)
        x_with_pe = add_positional_encoding(x_3d)
        assert x_with_pe.shape == x_3d.shape


class TestMasking:
    """ë§ˆìŠ¤í‚¹ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    
    def test_causal_mask(self):
        """Causal mask í…ŒìŠ¤íŠ¸"""
        mask = create_causal_mask(4)
        
        expected = np.array([
            [0, 1, 1, 1],
            [0, 0, 1, 1],
            [0, 0, 0, 1],
            [0, 0, 0, 0]
        ])
        
        np.testing.assert_array_equal(mask, expected)
    
    def test_padding_mask(self):
        """Padding mask í…ŒìŠ¤íŠ¸"""
        # ì‹œí€€ìŠ¤ with padding (0ì´ padding)
        seq = np.array([
            [1, 2, 3, 0, 0],
            [1, 2, 0, 0, 0]
        ])
        
        mask = create_padding_mask(seq, pad_idx=0)
        
        assert mask.shape == (2, 1, 1, 5)
        
        # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤: ë§ˆì§€ë§‰ 2ê°œê°€ padding
        assert mask[0, 0, 0, 3] == 1
        assert mask[0, 0, 0, 4] == 1
        
        # ë‘ ë²ˆì§¸ ì‹œí€€ìŠ¤: ë§ˆì§€ë§‰ 3ê°œê°€ padding
        assert mask[1, 0, 0, 2] == 1
        assert mask[1, 0, 0, 3] == 1
        assert mask[1, 0, 0, 4] == 1


class TestAttentionProperties:
    """Attentionì˜ ìˆ˜í•™ì  íŠ¹ì„± í…ŒìŠ¤íŠ¸"""
    
    def test_attention_sum_to_one(self):
        """Attention ê°€ì¤‘ì¹˜ í•©ì´ 1ì¸ì§€ í…ŒìŠ¤íŠ¸"""
        for _ in range(10):
            Q = np.random.randn(5, 32)
            K = np.random.randn(5, 32)
            V = np.random.randn(5, 16)
            
            _, weights = scaled_dot_product_attention(Q, K, V)
            
            # ê° queryì˜ attention í•©ì´ 1
            np.testing.assert_almost_equal(
                weights.sum(axis=1), 
                np.ones(5),
                decimal=6
            )
    
    def test_self_attention_permutation(self):
        """Self-attention ìˆœì—´ ë¶ˆë³€ì„± í…ŒìŠ¤íŠ¸"""
        seq_len = 6
        d_model = 32
        
        mha = MultiHeadAttention(d_model, num_heads=4)
        
        # ì›ë³¸ ìž…ë ¥
        x = np.random.randn(seq_len, d_model)
        output1, _ = mha.forward(x, x, x)
        
        # ìˆœì—´ ì ìš©
        perm = np.random.permutation(seq_len)
        x_perm = x[perm]
        output2, _ = mha.forward(x_perm, x_perm, x_perm)
        
        # ì¶œë ¥ë„ ê°™ì€ ìˆœì—´ ë”°ë¦„
        output2_restored = output2[np.argsort(perm)]
        
        # ê±°ì˜ ê°™ì•„ì•¼ í•¨ (ìˆ˜ì¹˜ ì˜¤ì°¨ í—ˆìš©)
        np.testing.assert_almost_equal(output1, output2_restored, decimal=5)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])