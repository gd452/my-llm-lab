# ğŸ”§ Self-Attentionì˜ ìˆ˜í•™ì  êµ¬í˜„

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- Self-Attentionì˜ ìˆ˜ì‹ ì´í•´
- Query, Key, Value í–‰ë ¬ ê³„ì‚°
- Scaled Dot-Product Attention êµ¬í˜„
- ì‹¤ì œ ì½”ë“œë¡œ ì‘ì„±í•˜ê¸°

## 1. Self-Attention ìˆ˜ì‹

### í•µì‹¬ ê³µì‹

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

ê° êµ¬ì„± ìš”ì†Œ:
- **Q** (Query): ë‚´ê°€ ì°¾ê³  ìˆëŠ” ê²ƒ
- **K** (Key): ê° ìœ„ì¹˜ê°€ ì œê³µí•  ìˆ˜ ìˆëŠ” ì •ë³´
- **V** (Value): ì‹¤ì œ ì „ë‹¬ë  ì •ë³´
- **d_k**: Key ë²¡í„°ì˜ ì°¨ì› (ìŠ¤ì¼€ì¼ë§ íŒ©í„°)

## 2. ë‹¨ê³„ë³„ ê³„ì‚°

### Step 0: ì…ë ¥ ì„ë² ë”©

```python
# ì…ë ¥: "The cat sat"
# ê° ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì˜ˆ: d_model=4)

X = [[0.1, 0.2, 0.3, 0.4],  # "The"
     [0.5, 0.6, 0.7, 0.8],  # "cat"  
     [0.9, 1.0, 1.1, 1.2]]  # "sat"

# Shape: (seq_len=3, d_model=4)
```

### Step 1: Q, K, V ìƒì„±

```python
# í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬
W_Q = [[...], [...], ...]  # (d_model, d_k)
W_K = [[...], [...], ...]  # (d_model, d_k)
W_V = [[...], [...], ...]  # (d_model, d_v)

# ì„ í˜• ë³€í™˜
Q = X @ W_Q  # (seq_len, d_k)
K = X @ W_K  # (seq_len, d_k)
V = X @ W_V  # (seq_len, d_v)
```

### Step 2: Attention Score ê³„ì‚°

```python
# Queryì™€ Keyì˜ ë‚´ì 
scores = Q @ K.T  # (seq_len, seq_len)

# ì˜ˆì‹œ ê²°ê³¼:
#        The  cat  sat
# The  [[2.0, 1.5, 0.8],
# cat   [1.5, 3.0, 2.1],
# sat   [0.8, 2.1, 2.5]]
```

### Step 3: Scaling

```python
# âˆšd_kë¡œ ë‚˜ëˆ„ê¸° (ì•ˆì •ì„±ì„ ìœ„í•´)
d_k = Q.shape[-1]  # Key ì°¨ì›
scores = scores / sqrt(d_k)

# ì™œ ìŠ¤ì¼€ì¼ë§?
# d_kê°€ í¬ë©´ ë‚´ì  ê°’ì´ ì»¤ì ¸ì„œ softmaxê°€ saturateë¨
# ì˜ˆ: [100, 1, 1] â†’ softmax â†’ [0.99999, 0.00001, 0.00001]
#     [10, 1, 1] â†’ softmax â†’ [0.99, 0.005, 0.005]
```

### Step 4: Softmax

```python
# ê° í–‰ë³„ë¡œ softmax ì ìš©
attention_weights = softmax(scores, axis=-1)

# ê²°ê³¼:
#        The   cat   sat
# The  [[0.50, 0.30, 0.20],  # TheëŠ” ìì‹ ì— 50% ì£¼ëª©
# cat   [0.20, 0.60, 0.20],  # catì€ ìì‹ ì— 60% ì£¼ëª©
# sat   [0.15, 0.35, 0.50]]  # satì€ ìì‹ ì— 50% ì£¼ëª©

# ê° í–‰ì˜ í•© = 1.0
```

### Step 5: Value ê°€ì¤‘í•©

```python
# Attention ê°€ì¤‘ì¹˜ë¡œ Value ê°€ì¤‘í•©
output = attention_weights @ V  # (seq_len, d_v)

# ê° ìœ„ì¹˜ì˜ ì¶œë ¥ = ëª¨ë“  Valueì˜ ê°€ì¤‘ í‰ê· 
# output[0] = 0.50*V[0] + 0.30*V[1] + 0.20*V[2]
```

## 3. êµ¬ì²´ì ì¸ ì˜ˆì œ

### ì™„ì „í•œ Self-Attention êµ¬í˜„

```python
import numpy as np

def self_attention(X, W_Q, W_K, W_V):
    """
    Self-Attention ê³„ì‚°
    
    Args:
        X: ì…ë ¥ (seq_len, d_model)
        W_Q, W_K, W_V: ê°€ì¤‘ì¹˜ í–‰ë ¬
    
    Returns:
        ì¶œë ¥ (seq_len, d_v)
    """
    # 1. Q, K, V ê³„ì‚°
    Q = X @ W_Q
    K = X @ W_K  
    V = X @ W_V
    
    # 2. Attention scores
    scores = Q @ K.T
    
    # 3. Scaling
    d_k = K.shape[-1]
    scores = scores / np.sqrt(d_k)
    
    # 4. Softmax
    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
    
    # 5. Value ê°€ì¤‘í•©
    output = attention_weights @ V
    
    return output, attention_weights
```

### ì‹¤í–‰ ì˜ˆì œ

```python
# ì„¤ì •
seq_len = 4
d_model = 8
d_k = d_v = 4

# ì…ë ¥ ìƒì„±
X = np.random.randn(seq_len, d_model)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
W_Q = np.random.randn(d_model, d_k) * 0.1
W_K = np.random.randn(d_model, d_k) * 0.1
W_V = np.random.randn(d_model, d_v) * 0.1

# Self-Attention ì‹¤í–‰
output, attention_weights = self_attention(X, W_Q, W_K, W_V)

print(f"ì…ë ¥ shape: {X.shape}")
print(f"ì¶œë ¥ shape: {output.shape}")
print(f"Attention weights shape: {attention_weights.shape}")
print(f"\nAttention weights:\n{attention_weights}")
```

## 4. Masked Self-Attention (Causal)

### GPT ìŠ¤íƒ€ì¼: ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨

```python
def causal_self_attention(X, W_Q, W_K, W_V):
    """
    Causal Self-Attention (ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ)
    """
    seq_len = X.shape[0]
    
    # ê¸°ë³¸ attention ê³„ì‚°
    Q = X @ W_Q
    K = X @ W_K
    V = X @ W_V
    
    scores = Q @ K.T / np.sqrt(K.shape[-1])
    
    # Causal mask ì ìš©
    mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    scores = scores - mask * 1e10  # ë¯¸ë˜ ìœ„ì¹˜ì— í° ìŒìˆ˜
    
    # Softmax
    attention_weights = softmax(scores, axis=-1)
    
    # Output
    output = attention_weights @ V
    
    return output, attention_weights

# Causal mask ì˜ˆì‹œ:
# [[0, -âˆ, -âˆ, -âˆ],  # ìœ„ì¹˜ 0ì€ ìì‹ ë§Œ ë³¼ ìˆ˜ ìˆìŒ
#  [0,  0, -âˆ, -âˆ],  # ìœ„ì¹˜ 1ì€ 0,1ì„ ë³¼ ìˆ˜ ìˆìŒ
#  [0,  0,  0, -âˆ],  # ìœ„ì¹˜ 2ëŠ” 0,1,2ë¥¼ ë³¼ ìˆ˜ ìˆìŒ
#  [0,  0,  0,  0]]  # ìœ„ì¹˜ 3ì€ ëª¨ë‘ ë³¼ ìˆ˜ ìˆìŒ
```

## 5. íš¨ìœ¨ì ì¸ êµ¬í˜„ íŒ

### ë°°ì¹˜ ì²˜ë¦¬

```python
def batch_self_attention(X, W_Q, W_K, W_V):
    """
    ë°°ì¹˜ ë‹¨ìœ„ Self-Attention
    
    Args:
        X: (batch_size, seq_len, d_model)
    """
    # Q, K, V ê³„ì‚° (broadcasting í™œìš©)
    Q = X @ W_Q  # (batch, seq_len, d_k)
    K = X @ W_K
    V = X @ W_V
    
    # ë°°ì¹˜ í–‰ë ¬ê³±
    scores = np.matmul(Q, K.transpose(0, 2, 1))
    scores = scores / np.sqrt(K.shape[-1])
    
    # Softmax (ë§ˆì§€ë§‰ ì°¨ì›)
    attention_weights = softmax(scores, axis=-1)
    
    # Output
    output = np.matmul(attention_weights, V)
    
    return output
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì  êµ¬í˜„

```python
def memory_efficient_attention(Q, K, V, chunk_size=32):
    """
    ì²­í¬ ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    """
    seq_len = Q.shape[0]
    outputs = []
    
    for i in range(0, seq_len, chunk_size):
        q_chunk = Q[i:i+chunk_size]
        
        # ì²­í¬ë³„ attention
        scores = q_chunk @ K.T / np.sqrt(K.shape[-1])
        weights = softmax(scores, axis=-1)
        output_chunk = weights @ V
        
        outputs.append(output_chunk)
    
    return np.concatenate(outputs, axis=0)
```

## 6. Attention Pattern ë¶„ì„

### ë‹¤ì–‘í•œ Attention íŒ¨í„´

```python
# 1. ìê¸° ìì‹ ì— ì§‘ì¤‘
attention_diagonal = np.eye(seq_len)

# 2. ê· ë“± ë¶„í¬
attention_uniform = np.ones((seq_len, seq_len)) / seq_len

# 3. ì²« í† í°ì— ì§‘ì¤‘ (CLS token style)
attention_first = np.zeros((seq_len, seq_len))
attention_first[:, 0] = 1.0

# 4. ì¸ì ‘ í† í° ì§‘ì¤‘
attention_local = np.zeros((seq_len, seq_len))
for i in range(seq_len):
    for j in range(max(0, i-1), min(seq_len, i+2)):
        attention_local[i, j] = 1/3
```

## 7. ì‹¤ì „ ë””ë²„ê¹…

### Attention ê°€ì¤‘ì¹˜ ê²€ì¦

```python
def validate_attention(attention_weights):
    """Attention ê°€ì¤‘ì¹˜ ê²€ì¦"""
    
    # 1. ê° í–‰ì˜ í•©ì´ 1ì¸ì§€
    row_sums = attention_weights.sum(axis=-1)
    assert np.allclose(row_sums, 1.0), "í–‰ í•©ì´ 1ì´ ì•„ë‹˜"
    
    # 2. ëª¨ë“  ê°’ì´ 0~1 ì‚¬ì´ì¸ì§€
    assert (attention_weights >= 0).all(), "ìŒìˆ˜ ê°€ì¤‘ì¹˜"
    assert (attention_weights <= 1).all(), "1ë³´ë‹¤ í° ê°€ì¤‘ì¹˜"
    
    # 3. NaNì´ë‚˜ Infê°€ ì—†ëŠ”ì§€
    assert not np.isnan(attention_weights).any(), "NaN ë°œê²¬"
    assert not np.isinf(attention_weights).any(), "Inf ë°œê²¬"
    
    print("âœ… Attention ê°€ì¤‘ì¹˜ ì •ìƒ")
```

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

1. **Q, K, VëŠ” í•™ìŠµë˜ëŠ” ë³€í™˜**
   - ê°™ì€ ì…ë ¥ì—ì„œ ë‹¤ë¥¸ ì—­í• ë¡œ ë³€í™˜

2. **Scalingì€ í•„ìˆ˜**
   - í° ì°¨ì›ì—ì„œ gradient vanishing ë°©ì§€

3. **Softmaxë¡œ í™•ë¥  ë¶„í¬í™”**
   - ê°€ì¤‘ì¹˜ì˜ í•´ì„ ê°€ëŠ¥ì„±

4. **í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ íš¨ìœ¨ì„±**
   - ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ê³„ì‚°

## ğŸ” ì´í•´ë„ ì²´í¬

1. Q @ K.Tì˜ ê²°ê³¼ shapeì€?
2. Scaling factor âˆšd_kê°€ í•„ìš”í•œ ì´ìœ ëŠ”?
3. Causal maskëŠ” ì–¸ì œ í•„ìš”í•œê°€?
4. Attention weightsì˜ íŠ¹ì„± 3ê°€ì§€ëŠ”?

## ğŸ“ ì—°ìŠµ ë¬¸ì œ

1. d_model=512, d_k=64ì¼ ë•Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
2. seq_len=1000ì¼ ë•Œ attention í–‰ë ¬ í¬ê¸°ëŠ”?
3. Relative position encoding êµ¬í˜„í•´ë³´ê¸°

## ë‹¤ìŒ ë‹¨ê³„

Single headë¥¼ Multi-headë¡œ í™•ì¥í•´ë´…ì‹œë‹¤!
â†’ [03_multi_head.md](03_multi_head.md)