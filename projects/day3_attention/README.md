# ğŸ¯ Day 3: Attention Mechanism - LLMì˜ ì‹¬ì¥

> "Attention is All You Need" - Transformer ë…¼ë¬¸ì˜ í•µì‹¬ ë©”ì‹œì§€

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ë“œë””ì–´ LLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ Attention ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤!
ì´ê²ƒì´ ë°”ë¡œ GPT, BERT, LLaMA ë“± ëª¨ë“  í˜„ëŒ€ LLMì˜ ê¸°ì´ˆì…ë‹ˆë‹¤.

### í•µì‹¬ ê°œë…
- âœ… Self-Attentionì˜ ì›ë¦¬ ì´í•´
- âœ… Query, Key, Value í–‰ë ¬ì˜ ì—­í• 
- âœ… Scaled Dot-Product Attention êµ¬í˜„
- âœ… Multi-Head Attentionìœ¼ë¡œ í™•ì¥
- âœ… Positional Encoding ì¶”ê°€

## ğŸ“š í”„ë¡œì íŠ¸ êµ¬ì¡°

```
day3_attention/
â”œâ”€â”€ README.md                    # í”„ë¡œì íŠ¸ ê°œìš”
â”œâ”€â”€ requirements.txt            # ì˜ì¡´ì„±
â”œâ”€â”€ Makefile                   # ë¹Œë“œ/í…ŒìŠ¤íŠ¸ ìë™í™”
â”‚
â”œâ”€â”€ study_notes/               # ğŸ“– í•™ìŠµ ìë£Œ
â”‚   â”œâ”€â”€ 01_attention_intuition.md   # Attention ì§ê´€ì  ì´í•´
â”‚   â”œâ”€â”€ 02_self_attention.md        # Self-Attention ë©”ì»¤ë‹ˆì¦˜
â”‚   â”œâ”€â”€ 03_multi_head.md           # Multi-Head Attention
â”‚   â””â”€â”€ 04_positional_encoding.md   # ìœ„ì¹˜ ì¸ì½”ë”©
â”‚
â”œâ”€â”€ notebooks/                 # ğŸ”¬ ì‹¤ìŠµ ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ attention_tutorial.ipynb
â”‚
â”œâ”€â”€ tests/                     # ğŸ§ª í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_attention.py
â”‚
â””â”€â”€ 50_eval/                   # ğŸ¯ í‰ê°€/ë°ëª¨
    â”œâ”€â”€ attention_visualizer.py    # Attention ì‹œê°í™”
    â””â”€â”€ translation_toy.py         # ê°„ë‹¨í•œ ë²ˆì—­ íƒœìŠ¤í¬
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •
```bash
cd projects/day3_attention
pip install -r requirements.txt
```

### 2. í•™ìŠµ ìˆœì„œ
1. **ì´ë¡  í•™ìŠµ**: `study_notes/` ìˆœì„œëŒ€ë¡œ ì½ê¸°
2. **ì‹¤ìŠµ**: `notebooks/attention_tutorial.ipynb` ë”°ë¼í•˜ê¸°
3. **êµ¬í˜„**: `core/attention.py` ì™„ì„±
4. **í…ŒìŠ¤íŠ¸**: `make test`ë¡œ ê²€ì¦
5. **ì‹œê°í™”**: `50_eval/attention_visualizer.py` ì‹¤í–‰

### 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
make test  # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
make visualize  # Attention ì‹œê°í™”
```

## ğŸ“ í•™ìŠµ ë‚´ìš©

### 1. Attentionì˜ ì§ê´€
- ë¬¸ì¥ì˜ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ "ì£¼ëª©"í•˜ëŠ”ì§€
- "The cat sat on the mat" - 'cat'ì€ 'sat'ê³¼ 'the'ì— ì£¼ëª©
- ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜

### 2. Self-Attention ìˆ˜ì‹
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V

Q: Query - ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?
K: Key - ë¬´ì—‡ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?
V: Value - ì‹¤ì œ ì •ë³´
```

### 3. Multi-Head Attention
```python
# ì—¬ëŸ¬ ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### 4. Positional Encoding
```python
# ìœ„ì¹˜ ì •ë³´ë¥¼ sine/cosineìœ¼ë¡œ ì¸ì½”ë”©
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

## ğŸ’» êµ¬í˜„í•  ì£¼ìš” í•¨ìˆ˜

### core/attention.py
```python
# êµ¬í˜„í•  í•¨ìˆ˜ë“¤
- scaled_dot_product_attention()  # ê¸°ë³¸ attention
- MultiHeadAttention class        # Multi-head êµ¬í˜„
- positional_encoding()           # ìœ„ì¹˜ ì¸ì½”ë”©
- causal_mask()                  # ë¯¸ë˜ ë§ˆìŠ¤í‚¹ (GPTìš©)
```

## ğŸ”¥ ë„ì „ ê³¼ì œ

### ê¸°ë³¸ ê³¼ì œ
1. âœ… Self-Attention êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
2. âœ… Multi-Head Attention ì™„ì„±
3. âœ… Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”
4. âœ… ê°„ë‹¨í•œ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ íƒœìŠ¤í¬

### ì‹¬í™” ê³¼ì œ
1. ğŸŒŸ Relative Positional Encoding
2. ğŸŒŸ Flash Attention (íš¨ìœ¨ì  êµ¬í˜„)
3. ğŸŒŸ Cross-Attention êµ¬í˜„
4. ğŸŒŸ Attention with Linear Complexity

## ğŸ“Š ì´í•´ë„ ì²´í¬

### ê°œë… ì´í•´
- [ ] Q, K, Vì˜ ì—­í• ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] Scaling factor (âˆšd_k)ê°€ í•„ìš”í•œ ì´ìœ ë¥¼ ì•ˆë‹¤
- [ ] Multi-Headê°€ Single-Headë³´ë‹¤ ì¢‹ì€ ì´ìœ ë¥¼ ì•ˆë‹¤
- [ ] Positional Encodingì´ í•„ìš”í•œ ì´ìœ ë¥¼ ì•ˆë‹¤

### êµ¬í˜„ ëŠ¥ë ¥
- [ ] NumPyë¡œ Self-Attentionì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- [ ] Attention ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤
- [ ] Causal Maskë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] Multi-Headë¡œ í™•ì¥í•  ìˆ˜ ìˆë‹¤

## ğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸

1. **ë³‘ë ¬ ì²˜ë¦¬**: RNNê³¼ ë‹¬ë¦¬ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬
2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ê±°ë¦¬ì™€ ë¬´ê´€í•˜ê²Œ ì •ë³´ ì „ë‹¬
3. **í•´ì„ ê°€ëŠ¥ì„±**: Attention ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ë™ì‘ ì´í•´
4. **ê³„ì‚° íš¨ìœ¨ì„±**: í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ GPU ìµœì í™”

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Self-Attention ì´í•´ ë° êµ¬í˜„
- [ ] Multi-Head Attention êµ¬í˜„
- [ ] Positional Encoding ì ìš©
- [ ] Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”
- [ ] í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼
- [ ] ë²ˆì—­ íƒœìŠ¤í¬ ì‹¤í–‰

## ğŸ”— ì°¸ê³  ìë£Œ

- [Attention Is All You Need (ì› ë…¼ë¬¸)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Visualizing Attention](https://distill.pub/2016/augmented-rnns/)

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

Day 3ë¥¼ ì™„ë£Œí•˜ë©´ Day 4 (Transformer Block)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
ì—¬ëŸ¬ Attention ë ˆì´ì–´ë¥¼ ìŒ“ì•„ ì™„ì „í•œ Transformerë¥¼ ë§Œë“­ë‹ˆë‹¤!

---

**"Attention is literally all you need to understand modern AI!"**