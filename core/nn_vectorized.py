"""
ğŸš€ Vectorized Neural Network: ë²¡í„°í™”ëœ ì‹ ê²½ë§

ì´ íŒŒì¼ì—ì„œ êµ¬í˜„í•  ê²ƒ:
1. ë²¡í„°í™”ëœ Linear Layer
2. ë²¡í„°í™”ëœ MLP
3. íš¨ìœ¨ì ì¸ forward/backward pass

NumPyì˜ Broadcastingì„ í™œìš©í•˜ì—¬
ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

if __name__ == "__main__":
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from core.tensor_ops import relu, sigmoid, tanh, softmax


class LinearLayer:
    """
    ë²¡í„°í™”ëœ ì™„ì „ ì—°ê²°ì¸µ
    
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ íš¨ìœ¨ì ì¸ í–‰ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, input_dim, output_dim, activation='relu'):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_dim: ì…ë ¥ ì°¨ì›
            output_dim: ì¶œë ¥ ì°¨ì›
            activation: í™œì„±í™” í•¨ìˆ˜ ('relu', 'sigmoid', 'tanh', 'none')
        """
        # Xavier/He ì´ˆê¸°í™”
        if activation == 'relu':
            # He ì´ˆê¸°í™” (ReLUì— ì í•©)
            self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2 / input_dim)
        else:
            # Xavier ì´ˆê¸°í™”
            self.W = np.random.randn(input_dim, output_dim) * np.sqrt(1 / input_dim)
        
        self.b = np.zeros(output_dim)
        
        # í™œì„±í™” í•¨ìˆ˜ ì„¤ì •
        self.activation = activation
        self.activation_fn = {
            'relu': relu,
            'sigmoid': sigmoid,
            'tanh': tanh,
            'none': lambda x: x
        }.get(activation, relu)
        
        # ìºì‹œ (ì—­ì „íŒŒìš©)
        self.cache = {}
        
        # ê·¸ë˜ë””ì–¸íŠ¸
        self.dW = None
        self.db = None
    
    def forward(self, X):
        """
        ìˆœì „íŒŒ
        
        Args:
            X: ì…ë ¥ (batch_size, input_dim)
        
        Returns:
            ì¶œë ¥ (batch_size, output_dim)
        
        TODO: êµ¬í˜„
        1. ì„ í˜• ë³€í™˜: Z = XW + b
        2. í™œì„±í™” í•¨ìˆ˜ ì ìš©
        3. ì—­ì „íŒŒë¥¼ ìœ„í•´ ê°’ë“¤ ì €ì¥
        """
        # ì„ í˜• ë³€í™˜
        Z = X @ self.W + self.b  # Broadcastingìœ¼ë¡œ bias ì¶”ê°€
        
        # í™œì„±í™”
        A = self.activation_fn(Z)
        
        # ì—­ì „íŒŒë¥¼ ìœ„í•´ ì €ì¥
        self.cache = {
            'X': X,
            'Z': Z,
            'A': A
        }
        
        return A
    
    def backward(self, dA):
        """
        ì—­ì „íŒŒ
        
        Args:
            dA: ì¶œë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ (batch_size, output_dim)
        
        Returns:
            ì…ë ¥ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ (batch_size, input_dim)
        
        TODO: êµ¬í˜„
        """
        X = self.cache['X']
        Z = self.cache['Z']
        batch_size = X.shape[0]
        
        # í™œì„±í™” í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸
        if self.activation == 'relu':
            dZ = dA * (Z > 0)
        elif self.activation == 'sigmoid':
            A = self.cache['A']
            dZ = dA * A * (1 - A)
        elif self.activation == 'tanh':
            A = self.cache['A']
            dZ = dA * (1 - A ** 2)
        else:  # none
            dZ = dA
        
        # íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸
        self.dW = (X.T @ dZ) / batch_size
        self.db = np.sum(dZ, axis=0) / batch_size
        
        # ì…ë ¥ ê·¸ë˜ë””ì–¸íŠ¸
        dX = dZ @ self.W.T
        
        return dX
    
    def update_params(self, learning_rate):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        self.W -= learning_rate * self.dW
        self.b -= learning_rate * self.db
    
    def get_params(self):
        """íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return {'W': self.W, 'b': self.b}
    
    def set_params(self, params):
        """íŒŒë¼ë¯¸í„° ì„¤ì •"""
        self.W = params['W']
        self.b = params['b']


class MLPVectorized:
    """
    ë²¡í„°í™”ëœ Multi-Layer Perceptron
    
    ì—¬ëŸ¬ LinearLayerë¥¼ ì¡°í•©í•œ ê¹Šì€ ì‹ ê²½ë§
    """
    
    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu'):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_dim: ì…ë ¥ ì°¨ì›
            hidden_dims: ì€ë‹‰ì¸µ ì°¨ì›ë“¤ (ë¦¬ìŠ¤íŠ¸)
            output_dim: ì¶œë ¥ ì°¨ì›
            activation: ì€ë‹‰ì¸µ í™œì„±í™” í•¨ìˆ˜
        
        Example:
            >>> mlp = MLPVectorized(784, [256, 128], 10)
            >>> # 784 -> 256 -> 128 -> 10 êµ¬ì¡°
        """
        self.layers = []
        
        # ëª¨ë“  ì°¨ì›ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ
        dims = [input_dim] + hidden_dims + [output_dim]
        
        # ë ˆì´ì–´ ìƒì„±
        for i in range(len(dims) - 1):
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ
            if i == len(dims) - 2:
                layer = LinearLayer(dims[i], dims[i+1], activation='none')
            else:
                layer = LinearLayer(dims[i], dims[i+1], activation=activation)
            
            self.layers.append(layer)
    
    def forward(self, X):
        """
        ìˆœì „íŒŒ
        
        Args:
            X: ì…ë ¥ (batch_size, input_dim)
        
        Returns:
            ì¶œë ¥ (batch_size, output_dim)
        """
        out = X
        for layer in self.layers:
            out = layer.forward(out)
        return out
    
    def backward(self, dout):
        """
        ì—­ì „íŒŒ
        
        Args:
            dout: ì¶œë ¥ ê·¸ë˜ë””ì–¸íŠ¸ (batch_size, output_dim)
        
        Returns:
            ì…ë ¥ ê·¸ë˜ë””ì–¸íŠ¸ (batch_size, input_dim)
        """
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout
    
    def update_params(self, learning_rate):
        """ëª¨ë“  ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        for layer in self.layers:
            layer.update_params(learning_rate)
    
    def get_all_params(self):
        """ëª¨ë“  íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return [layer.get_params() for layer in self.layers]
    
    def predict(self, X):
        """
        ì˜ˆì¸¡ (ì¶”ë¡  ëª¨ë“œ)
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        return self.forward(X)
    
    def predict_proba(self, X):
        """
        í™•ë¥  ì˜ˆì¸¡ (ë¶„ë¥˜ìš©)
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
        
        Returns:
            í´ë˜ìŠ¤ í™•ë¥ 
        """
        logits = self.forward(X)
        return softmax(logits)


class SGDOptimizer:
    """
    Stochastic Gradient Descent Optimizer
    """
    
    def __init__(self, model, learning_rate=0.01, momentum=0.0):
        """
        ì´ˆê¸°í™”
        
        Args:
            model: MLPVectorized ëª¨ë¸
            learning_rate: í•™ìŠµë¥ 
            momentum: ëª¨ë©˜í…€ ê³„ìˆ˜
        """
        self.model = model
        self.lr = learning_rate
        self.momentum = momentum
        
        # ëª¨ë©˜í…€ì„ ìœ„í•œ ì†ë„ ì €ì¥
        if momentum > 0:
            self.velocities = []
            for layer in model.layers:
                self.velocities.append({
                    'W': np.zeros_like(layer.W),
                    'b': np.zeros_like(layer.b)
                })
        else:
            self.velocities = None
    
    def step(self):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        if self.momentum > 0:
            # ëª¨ë©˜í…€ SGD
            for layer, velocity in zip(self.model.layers, self.velocities):
                # ì†ë„ ì—…ë°ì´íŠ¸
                velocity['W'] = self.momentum * velocity['W'] - self.lr * layer.dW
                velocity['b'] = self.momentum * velocity['b'] - self.lr * layer.db
                
                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
                layer.W += velocity['W']
                layer.b += velocity['b']
        else:
            # ì¼ë°˜ SGD
            self.model.update_params(self.lr)


# ============================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================

if __name__ == "__main__":
    print("ğŸ§ª Vectorized Neural Network í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    # ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X = np.random.randn(32, 10)  # 32ê°œ ìƒ˜í”Œ, 10ì°¨ì›
    y = np.random.randint(0, 3, 32)  # 3ê°œ í´ë˜ìŠ¤
    
    # ëª¨ë¸ ìƒì„±
    model = MLPVectorized(
        input_dim=10,
        hidden_dims=[20, 15],
        output_dim=3
    )
    
    print("ëª¨ë¸ êµ¬ì¡°: 10 -> 20 -> 15 -> 3")
    
    # Forward pass
    output = model.forward(X)
    print(f"ì¶œë ¥ shape: {output.shape}")
    
    # Softmax ì ìš©
    probs = model.predict_proba(X)
    print(f"í™•ë¥  í•©: {probs.sum(axis=1)[:5]}")  # ì²˜ìŒ 5ê°œë§Œ
    
    # Backward pass í…ŒìŠ¤íŠ¸
    from core.tensor_ops import cross_entropy
    
    # ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸ (ì‹¤ì œë¡œëŠ” lossì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì‚¬ìš©)
    dout = probs.copy()
    dout[np.arange(32), y] -= 1
    dout /= 32
    
    din = model.backward(dout)
    print(f"ì…ë ¥ ê·¸ë˜ë””ì–¸íŠ¸ shape: {din.shape}")
    
    # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    optimizer = SGDOptimizer(model, learning_rate=0.1, momentum=0.9)
    optimizer.step()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")


