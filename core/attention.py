"""
ğŸ¯ Attention Mechanism: LLMì˜ í•µì‹¬

ì´ íŒŒì¼ì—ì„œ êµ¬í˜„í•  ê²ƒ:
1. Scaled Dot-Product Attention
2. Multi-Head Attention
3. Positional Encoding
4. Causal Masking

"Attention is All You Need" ë…¼ë¬¸ì˜ í•µì‹¬ êµ¬í˜„ì…ë‹ˆë‹¤.
"""

import numpy as np


# ============================================
# Scaled Dot-Product Attention
# ============================================

def scaled_dot_product_attention(Q, K, V, mask=None, dropout_rate=0.0):
    """
    Scaled Dot-Product Attention ê³„ì‚°
    
    Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
    
    Args:
        Q: Query matrix (batch_size, seq_len_q, d_k) or (seq_len_q, d_k)
        K: Key matrix (batch_size, seq_len_k, d_k) or (seq_len_k, d_k)
        V: Value matrix (batch_size, seq_len_v, d_v) or (seq_len_v, d_v)
        mask: ë§ˆìŠ¤í‚¹ í–‰ë ¬ (batch_size, seq_len_q, seq_len_k) or (seq_len_q, seq_len_k)
        dropout_rate: Dropout ë¹„ìœ¨ (í›ˆë ¨ ì‹œ)
    
    Returns:
        output: Attention ì¶œë ¥ (batch_size, seq_len_q, d_v)
        attention_weights: Attention ê°€ì¤‘ì¹˜ (batch_size, seq_len_q, seq_len_k)
    
    Example:
        >>> Q = np.random.randn(10, 64)  # 10 queries, 64 dims
        >>> K = np.random.randn(10, 64)  # 10 keys
        >>> V = np.random.randn(10, 128) # 10 values, 128 dims
        >>> output, weights = scaled_dot_product_attention(Q, K, V)
    """
    # Qì™€ Kì˜ ë‚´ì  ê³„ì‚°
    d_k = K.shape[-1]
    
    # Kì˜ ì „ì¹˜ (ë§ˆì§€ë§‰ ë‘ ì°¨ì›ë§Œ)
    if K.ndim == 3:
        K_T = K.transpose(0, 2, 1)  # (batch, d_k, seq_len_k)
    else:
        K_T = K.T  # (d_k, seq_len_k)
    
    # Attention scores ê³„ì‚°
    scores = np.matmul(Q, K_T)  # (batch, seq_len_q, seq_len_k)
    
    # Scaling (ì¤‘ìš”!)
    scores = scores / np.sqrt(d_k)
    
    # Masking (optional)
    if mask is not None:
        # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì— í° ìŒìˆ˜ ê°’ ì¶”ê°€
        scores = scores + (mask * -1e9)
    
    # Softmax
    attention_weights = softmax(scores, axis=-1)
    
    # Dropout (training mode)
    if dropout_rate > 0:
        dropout_mask = np.random.binomial(1, 1 - dropout_rate, attention_weights.shape)
        attention_weights = attention_weights * dropout_mask / (1 - dropout_rate)
    
    # Valueì™€ ê°€ì¤‘í•©
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights


def softmax(x, axis=-1):
    """ì•ˆì •ì ì¸ softmax êµ¬í˜„"""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)


# ============================================
# Multi-Head Attention
# ============================================

class MultiHeadAttention:
    """
    Multi-Head Attention êµ¬í˜„
    
    ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬
    ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— í•™ìŠµí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, d_model, num_heads, dropout_rate=0.0):
        """
        ì´ˆê¸°í™”
        
        Args:
            d_model: ëª¨ë¸ì˜ ì°¨ì› (ì˜ˆ: 512)
            num_heads: Attention head ê°œìˆ˜ (ì˜ˆ: 8)
            dropout_rate: Dropout ë¹„ìœ¨
        """
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # ê° headì˜ ì°¨ì›
        self.dropout_rate = dropout_rate
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.W_Q = self._init_weight()
        self.W_K = self._init_weight()
        self.W_V = self._init_weight()
        self.W_O = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
    
    def _init_weight(self):
        """Xavier ì´ˆê¸°í™”"""
        return np.random.randn(self.d_model, self.d_model) * np.sqrt(2.0 / self.d_model)
    
    def forward(self, query, key, value, mask=None):
        """
        Multi-Head Attention forward pass
        
        Args:
            query: (batch_size, seq_len_q, d_model) or (seq_len_q, d_model)
            key: (batch_size, seq_len_k, d_model) or (seq_len_k, d_model)
            value: (batch_size, seq_len_v, d_model) or (seq_len_v, d_model)
            mask: Optional mask
        
        Returns:
            output: (batch_size, seq_len_q, d_model)
            attention_weights: (batch_size, num_heads, seq_len_q, seq_len_k)
        """
        # ë°°ì¹˜ ì°¨ì› ì²˜ë¦¬
        if query.ndim == 2:
            query = query[np.newaxis, :]
            key = key[np.newaxis, :]
            value = value[np.newaxis, :]
            squeeze_output = True
        else:
            squeeze_output = False
        
        batch_size = query.shape[0]
        seq_len_q = query.shape[1]
        seq_len_k = key.shape[1]
        
        # 1. Linear projections
        Q = np.matmul(query, self.W_Q)  # (batch, seq_len_q, d_model)
        K = np.matmul(key, self.W_K)    # (batch, seq_len_k, d_model)
        V = np.matmul(value, self.W_V)  # (batch, seq_len_v, d_model)
        
        # 2. Reshape for multi-head
        Q = self._split_heads(Q, batch_size)  # (batch, num_heads, seq_len_q, d_k)
        K = self._split_heads(K, batch_size)
        V = self._split_heads(V, batch_size)
        
        # 3. Scaled dot-product attention for each head
        attention_output, attention_weights = self._attention(Q, K, V, mask)
        
        # 4. Concatenate heads
        attention_output = self._combine_heads(attention_output, batch_size)
        
        # 5. Final linear projection
        output = np.matmul(attention_output, self.W_O)
        
        if squeeze_output:
            output = output.squeeze(0)
            attention_weights = attention_weights.squeeze(0)
        
        return output, attention_weights
    
    def _split_heads(self, x, batch_size):
        """Split into multiple heads"""
        seq_len = x.shape[1]
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)  # (batch, num_heads, seq_len, d_k)
    
    def _combine_heads(self, x, batch_size):
        """Combine multiple heads"""
        x = x.transpose(0, 2, 1, 3)  # (batch, seq_len, num_heads, d_k)
        seq_len = x.shape[1]
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def _attention(self, Q, K, V, mask=None):
        """Apply attention to all heads"""
        # (batch, num_heads, seq_len_q, d_k) @ (batch, num_heads, d_k, seq_len_k)
        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.d_k)
        
        if mask is not None:
            # Expand mask for heads
            if mask.ndim == 2:
                mask = mask[np.newaxis, np.newaxis, :, :]
            elif mask.ndim == 3:
                mask = mask[:, np.newaxis, :, :]
            scores = scores + (mask * -1e9)
        
        attention_weights = softmax(scores, axis=-1)
        
        # Apply dropout
        if self.dropout_rate > 0:
            dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, 
                                            attention_weights.shape)
            attention_weights = attention_weights * dropout_mask / (1 - self.dropout_rate)
        
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights


# ============================================
# Positional Encoding
# ============================================

def positional_encoding(seq_len, d_model, base=10000):
    """
    Sinusoidal Positional Encoding
    
    PE(pos, 2i) = sin(pos / base^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / base^(2i/d_model))
    
    Args:
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        d_model: ëª¨ë¸ ì°¨ì›
        base: ì£¼íŒŒìˆ˜ ë² ì´ìŠ¤ (ê¸°ë³¸ 10000)
    
    Returns:
        PE matrix (seq_len, d_model)
    
    Example:
        >>> pe = positional_encoding(100, 512)
        >>> print(pe.shape)  # (100, 512)
    """
    pe = np.zeros((seq_len, d_model))
    position = np.arange(seq_len)[:, np.newaxis]
    
    # ê° ì°¨ì›ì˜ ì£¼íŒŒìˆ˜ ê³„ì‚°
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(base) / d_model))
    
    # Sin, Cos ì ìš©
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    
    return pe


def add_positional_encoding(x, max_len=5000):
    """
    ì…ë ¥ì— Positional Encoding ì¶”ê°€
    
    Args:
        x: ì…ë ¥ í…ì„œ (batch_size, seq_len, d_model) or (seq_len, d_model)
        max_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    
    Returns:
        PEê°€ ì¶”ê°€ëœ ì…ë ¥
    """
    if x.ndim == 2:
        seq_len, d_model = x.shape
        pe = positional_encoding(min(seq_len, max_len), d_model)
        return x + pe[:seq_len]
    else:
        batch_size, seq_len, d_model = x.shape
        pe = positional_encoding(min(seq_len, max_len), d_model)
        return x + pe[np.newaxis, :seq_len, :]


# ============================================
# Masking Functions
# ============================================

def create_padding_mask(seq, pad_idx=0):
    """
    íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±
    
    Args:
        seq: ì…ë ¥ ì‹œí€€ìŠ¤ (batch_size, seq_len)
        pad_idx: íŒ¨ë”© í† í°ì˜ ì¸ë±ìŠ¤
    
    Returns:
        ë§ˆìŠ¤í¬ (batch_size, 1, 1, seq_len)
    """
    # íŒ¨ë”© ìœ„ì¹˜ëŠ” 1, ì•„ë‹ˆë©´ 0
    mask = (seq == pad_idx).astype(np.float32)
    
    # Shape ì¡°ì • for broadcasting
    return mask[:, np.newaxis, np.newaxis, :]


def create_causal_mask(seq_len):
    """
    Causal mask ìƒì„± (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)
    
    GPTì™€ ê°™ì€ autoregressive ëª¨ë¸ì—ì„œ ì‚¬ìš©
    
    Args:
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
    
    Returns:
        Causal mask (seq_len, seq_len)
    
    Example:
        >>> mask = create_causal_mask(4)
        >>> print(mask)
        [[0. 1. 1. 1.]
         [0. 0. 1. 1.]
         [0. 0. 0. 1.]
         [0. 0. 0. 0.]]
    """
    # Upper triangular matrix (k=1ì€ ëŒ€ê°ì„  ì œì™¸)
    mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    return mask


def create_combined_mask(seq, pad_idx=0):
    """
    íŒ¨ë”© + Causal ë§ˆìŠ¤í¬ ê²°í•©
    
    Args:
        seq: ì…ë ¥ ì‹œí€€ìŠ¤
        pad_idx: íŒ¨ë”© ì¸ë±ìŠ¤
    
    Returns:
        ê²°í•©ëœ ë§ˆìŠ¤í¬
    """
    seq_len = seq.shape[1]
    
    # Padding mask
    padding_mask = create_padding_mask(seq, pad_idx)
    
    # Causal mask
    causal_mask = create_causal_mask(seq_len)
    causal_mask = causal_mask[np.newaxis, np.newaxis, :, :]
    
    # Combine (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§ˆìŠ¤í¬ë©´ ë§ˆìŠ¤í¬)
    combined_mask = np.maximum(padding_mask, causal_mask)
    
    return combined_mask


# ============================================
# Attention Utilities
# ============================================

def visualize_attention(attention_weights, tokens=None):
    """
    Attention ê°€ì¤‘ì¹˜ ì‹œê°í™” (í…ìŠ¤íŠ¸ ê¸°ë°˜)
    
    Args:
        attention_weights: (seq_len, seq_len) or (num_heads, seq_len, seq_len)
        tokens: í† í° ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
    """
    if attention_weights.ndim == 3:
        # Multi-headì¸ ê²½ìš° í‰ê· 
        attention_weights = attention_weights.mean(axis=0)
    
    seq_len = attention_weights.shape[0]
    
    # í† í° ë¼ë²¨
    if tokens is None:
        tokens = [f"T{i}" for i in range(seq_len)]
    
    # í—¤ë”
    print("\nAttention Weights Heatmap:")
    print("       ", end="")
    for token in tokens:
        print(f"{token:>6}", end="")
    print()
    
    # íˆíŠ¸ë§µ
    for i, token in enumerate(tokens):
        print(f"{token:>6} ", end="")
        for j in range(seq_len):
            weight = attention_weights[i, j]
            if weight > 0.5:
                print("  â–ˆâ–ˆ  ", end="")
            elif weight > 0.3:
                print("  â–“â–“  ", end="")
            elif weight > 0.1:
                print("  â–‘â–‘  ", end="")
            else:
                print("  ..  ", end="")
        print()


# ============================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================

if __name__ == "__main__":
    print("ğŸ§ª Attention ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    # 1. Scaled Dot-Product Attention
    print("\n1ï¸âƒ£ Scaled Dot-Product Attention")
    Q = np.random.randn(4, 8)  # 4 queries, 8 dims
    K = np.random.randn(4, 8)  # 4 keys
    V = np.random.randn(4, 16) # 4 values, 16 dims
    
    output, weights = scaled_dot_product_attention(Q, K, V)
    print(f"ì¶œë ¥ shape: {output.shape}")
    print(f"Attention weights shape: {weights.shape}")
    print(f"Weights sum per query: {weights.sum(axis=1)}")
    
    # 2. Multi-Head Attention
    print("\n2ï¸âƒ£ Multi-Head Attention")
    mha = MultiHeadAttention(d_model=64, num_heads=8)
    
    x = np.random.randn(10, 64)  # 10 tokens, 64 dims
    output, weights = mha.forward(x, x, x)  # Self-attention
    print(f"ì¶œë ¥ shape: {output.shape}")
    print(f"Attention weights shape: {weights.shape}")
    
    # 3. Positional Encoding
    print("\n3ï¸âƒ£ Positional Encoding")
    pe = positional_encoding(100, 128)
    print(f"PE shape: {pe.shape}")
    print(f"PE ë²”ìœ„: [{pe.min():.2f}, {pe.max():.2f}]")
    
    # 4. Causal Mask
    print("\n4ï¸âƒ£ Causal Mask")
    mask = create_causal_mask(5)
    print("Causal mask (5x5):")
    print(mask)
    
    # 5. Attention ì‹œê°í™”
    print("\n5ï¸âƒ£ Attention ì‹œê°í™”")
    tokens = ["The", "cat", "sat", "on", "mat"]
    random_weights = np.random.rand(5, 5)
    random_weights = random_weights / random_weights.sum(axis=1, keepdims=True)
    visualize_attention(random_weights, tokens)
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")