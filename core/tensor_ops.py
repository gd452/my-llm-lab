"""
ðŸ”¢ Tensor Operations: ë²¡í„°/í–‰ë ¬ ì—°ì‚°ì˜ í•µì‹¬

ì´ íŒŒì¼ì—ì„œ êµ¬í˜„í•  ê²ƒ:
1. íš¨ìœ¨ì ì¸ í–‰ë ¬ ì—°ì‚°
2. Softmaxì™€ í™œì„±í™” í•¨ìˆ˜ë“¤
3. ì†ì‹¤ í•¨ìˆ˜ë“¤
4. ë°°ì¹˜ ì •ê·œí™”

NumPyë¥¼ í™œìš©í•œ ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ 
ìŠ¤ì¹¼ë¼ ì—°ì‚°ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import numpy as np


# ============================================
# ê¸°ë³¸ ì—°ì‚°
# ============================================

def matmul(a, b):
    """
    í–‰ë ¬ê³± ì—°ì‚°
    
    Args:
        a: (..., m, k) shapeì˜ ë°°ì—´
        b: (..., k, n) shapeì˜ ë°°ì—´
    
    Returns:
        (..., m, n) shapeì˜ ê²°ê³¼
    
    Example:
        >>> A = np.random.randn(32, 784)  # batch_size=32, features=784
        >>> W = np.random.randn(784, 128)  # 784 -> 128 ë³€í™˜
        >>> result = matmul(A, W)  # (32, 128)
    """
    return np.matmul(a, b)


# ============================================
# í™œì„±í™” í•¨ìˆ˜ (Vectorized)
# ============================================

def relu(x):
    """
    ReLU í™œì„±í™” í•¨ìˆ˜
    
    Args:
        x: ìž…ë ¥ ë°°ì—´
    
    Returns:
        max(0, x)
    """
    return np.maximum(0, x)


def sigmoid(x):
    """
    Sigmoid í™œì„±í™” í•¨ìˆ˜ (ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •)
    
    Args:
        x: ìž…ë ¥ ë°°ì—´
    
    Returns:
        1 / (1 + exp(-x))
    """
    # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
    x = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x))


def tanh(x):
    """
    Tanh í™œì„±í™” í•¨ìˆ˜
    
    Args:
        x: ìž…ë ¥ ë°°ì—´
    
    Returns:
        tanh(x)
    """
    return np.tanh(x)


def softmax(x, axis=-1):
    """
    Softmax í•¨ìˆ˜ (ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •)
    
    Args:
        x: ìž…ë ¥ ë°°ì—´
        axis: softmaxë¥¼ ì ìš©í•  ì¶•
    
    Returns:
        í™•ë¥  ë¶„í¬ (í•©ì´ 1)
    
    Example:
        >>> logits = np.random.randn(32, 10)  # 32ê°œ ìƒ˜í”Œ, 10ê°œ í´ëž˜ìŠ¤
        >>> probs = softmax(logits)
        >>> np.allclose(probs.sum(axis=1), 1)  # True
    """
    # TODO: êµ¬í˜„
    # ížŒíŠ¸:
    # 1. ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ“ê°’ ë¹¼ê¸°
    # 2. exp ê³„ì‚°
    # 3. ì •ê·œí™”
    
    # ìµœëŒ“ê°’ ë¹¼ê¸° (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    
    # ì •ê·œí™”
    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)
    return exp_x / sum_exp_x


# ============================================
# ì†ì‹¤ í•¨ìˆ˜
# ============================================

def mse_loss(y_pred, y_true):
    """
    Mean Squared Error Loss
    
    Args:
        y_pred: ì˜ˆì¸¡ê°’ (batch_size, output_dim)
        y_true: ì‹¤ì œê°’ (batch_size, output_dim)
    
    Returns:
        ìŠ¤ì¹¼ë¼ ì†ì‹¤ê°’
    """
    return np.mean((y_pred - y_true) ** 2)


def cross_entropy(y_pred, y_true, eps=1e-8):
    """
    Cross Entropy Loss
    
    Args:
        y_pred: ì˜ˆì¸¡ í™•ë¥  (batch_size, num_classes)
        y_true: ì •ë‹µ ì¸ë±ìŠ¤ (batch_size,) ë˜ëŠ” one-hot (batch_size, num_classes)
        eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ìž‘ì€ ê°’
    
    Returns:
        ìŠ¤ì¹¼ë¼ ì†ì‹¤ê°’
    
    TODO: êµ¬í˜„
    ížŒíŠ¸:
    1. y_trueê°€ ì¸ë±ìŠ¤ì¸ì§€ one-hotì¸ì§€ í™•ì¸
    2. log(0) ë°©ì§€ë¥¼ ìœ„í•´ clipping
    3. í‰ê·  ì†ì‹¤ ë°˜í™˜
    """
    # Clipping for numerical stability
    y_pred = np.clip(y_pred, eps, 1 - eps)
    
    # y_trueê°€ one-hotì¸ì§€ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
    if y_true.ndim == 1 or y_true.shape[1] == 1:
        # ì¸ë±ìŠ¤ í˜•íƒœ
        batch_size = y_pred.shape[0]
        log_probs = -np.log(y_pred[np.arange(batch_size), y_true.astype(int)])
    else:
        # One-hot í˜•íƒœ
        log_probs = -np.sum(y_true * np.log(y_pred), axis=1)
    
    return np.mean(log_probs)


def softmax_cross_entropy(logits, y_true):
    """
    Softmax + Cross Entropy (ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •)
    
    Args:
        logits: ë¡œì§“ (batch_size, num_classes)
        y_true: ì •ë‹µ ì¸ë±ìŠ¤ (batch_size,)
    
    Returns:
        ìŠ¤ì¹¼ë¼ ì†ì‹¤ê°’
    """
    # LogSumExp trick for numerical stability
    logits_max = np.max(logits, axis=1, keepdims=True)
    log_sum_exp = logits_max + np.log(np.sum(np.exp(logits - logits_max), axis=1, keepdims=True))
    
    batch_size = logits.shape[0]
    log_probs = logits - log_sum_exp
    
    # Cross entropy
    ce = -log_probs[np.arange(batch_size), y_true.astype(int)]
    
    return np.mean(ce)


# ============================================
# ë°°ì¹˜ ì •ê·œí™”
# ============================================

def batch_norm(x, gamma, beta, eps=1e-5, momentum=0.9, 
               running_mean=None, running_var=None, training=True):
    """
    Batch Normalization
    
    Args:
        x: ìž…ë ¥ (batch_size, features)
        gamma: ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° (features,)
        beta: ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„° (features,)
        eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ìž‘ì€ ê°’
        momentum: running statistics ì—…ë°ì´íŠ¸ ë¹„ìœ¨
        running_mean: ì¶”ë¡ ìš© í‰ê·  (features,)
        running_var: ì¶”ë¡ ìš© ë¶„ì‚° (features,)
        training: í•™ìŠµ/ì¶”ë¡  ëª¨ë“œ
    
    Returns:
        ì •ê·œí™”ëœ ì¶œë ¥, (í‰ê· , ë¶„ì‚°) - ì—­ì „íŒŒìš©
    
    TODO: êµ¬í˜„
    """
    if training:
        # ë°°ì¹˜ í†µê³„ ê³„ì‚°
        mean = np.mean(x, axis=0)
        var = np.var(x, axis=0)
        
        # Running statistics ì—…ë°ì´íŠ¸
        if running_mean is not None:
            running_mean[:] = momentum * running_mean + (1 - momentum) * mean
        if running_var is not None:
            running_var[:] = momentum * running_var + (1 - momentum) * var
    else:
        # ì¶”ë¡  ì‹œ running statistics ì‚¬ìš©
        mean = running_mean if running_mean is not None else np.mean(x, axis=0)
        var = running_var if running_var is not None else np.var(x, axis=0)
    
    # ì •ê·œí™”
    x_norm = (x - mean) / np.sqrt(var + eps)
    
    # ìŠ¤ì¼€ì¼ê³¼ ì‹œí”„íŠ¸
    out = gamma * x_norm + beta
    
    return out, (mean, var)


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def one_hot(indices, num_classes):
    """
    One-hot ì¸ì½”ë”©
    
    Args:
        indices: í´ëž˜ìŠ¤ ì¸ë±ìŠ¤ (batch_size,)
        num_classes: ì „ì²´ í´ëž˜ìŠ¤ ìˆ˜
    
    Returns:
        One-hot ë²¡í„° (batch_size, num_classes)
    """
    batch_size = len(indices)
    one_hot_matrix = np.zeros((batch_size, num_classes))
    one_hot_matrix[np.arange(batch_size), indices] = 1
    return one_hot_matrix


def accuracy(y_pred, y_true):
    """
    ë¶„ë¥˜ ì •í™•ë„ ê³„ì‚°
    
    Args:
        y_pred: ì˜ˆì¸¡ í™•ë¥  (batch_size, num_classes) ë˜ëŠ” í´ëž˜ìŠ¤ (batch_size,)
        y_true: ì •ë‹µ í´ëž˜ìŠ¤ (batch_size,)
    
    Returns:
        ì •í™•ë„ (0~1)
    """
    if y_pred.ndim == 2:
        # í™•ë¥ ì—ì„œ í´ëž˜ìŠ¤ ì¶”ì¶œ
        y_pred = np.argmax(y_pred, axis=1)
    
    return np.mean(y_pred == y_true)


def clip_gradients(gradients, max_norm=1.0):
    """
    ê·¸ëž˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    Args:
        gradients: ê·¸ëž˜ë””ì–¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        max_norm: ìµœëŒ€ norm
    
    Returns:
        í´ë¦¬í•‘ëœ ê·¸ëž˜ë””ì–¸íŠ¸
    """
    total_norm = np.sqrt(sum(np.sum(g ** 2) for g in gradients))
    
    if total_norm > max_norm:
        scale = max_norm / total_norm
        gradients = [g * scale for g in gradients]
    
    return gradients


# ============================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================

if __name__ == "__main__":
    print("ðŸ§ª Tensor Operations í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    # Softmax í…ŒìŠ¤íŠ¸
    logits = np.random.randn(4, 3)
    probs = softmax(logits)
    print(f"Softmax í•©: {probs.sum(axis=1)}")  # [1, 1, 1, 1]
    
    # Cross Entropy í…ŒìŠ¤íŠ¸
    y_true = np.array([0, 1, 2, 1])
    loss = cross_entropy(probs, y_true)
    print(f"Cross Entropy Loss: {loss:.4f}")
    
    # Batch Norm í…ŒìŠ¤íŠ¸
    x = np.random.randn(32, 10)
    gamma = np.ones(10)
    beta = np.zeros(10)
    x_norm, _ = batch_norm(x, gamma, beta)
    print(f"Batch Norm - Mean: {x_norm.mean(axis=0).mean():.4f}, "
          f"Var: {x_norm.var(axis=0).mean():.4f}")
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")