# Day 3: Attention Mechanism í”„ë¡œì íŠ¸ ìŠ¤í™

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ
LLMì˜ í•µì‹¬ì¸ Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì™„ì „íˆ ì´í•´í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“š í•™ìŠµ ëª©í‘œ
1. Attentionì˜ ì§ê´€ì  ì´í•´ì™€ ìˆ˜í•™ì  ì›ë¦¬
2. Self-Attentionê³¼ Cross-Attention êµ¬í˜„
3. Multi-Head Attentionì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„
4. Positional Encodingì˜ ì—­í• ê³¼ êµ¬í˜„
5. Transformerì˜ ê¸°ì´ˆ ì´í•´

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
projects/day3_attention/
â”œâ”€â”€ README.md                       # í”„ë¡œì íŠ¸ ê°œìš”
â”œâ”€â”€ requirements.txt               # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â”œâ”€â”€ Makefile                      # ë¹Œë“œ ìë™í™”
â”‚
â”œâ”€â”€ study_notes/                  # ğŸ“– í•™ìŠµ ìë£Œ
â”‚   â”œâ”€â”€ 01_attention_intuition.md    # Attention ì§ê´€ì  ì´í•´
â”‚   â”œâ”€â”€ 02_self_attention.md         # Self-Attention ë©”ì»¤ë‹ˆì¦˜
â”‚   â”œâ”€â”€ 03_multi_head.md            # Multi-Head Attention
â”‚   â””â”€â”€ 04_positional_encoding.md    # ìœ„ì¹˜ ì¸ì½”ë”©
â”‚
â”œâ”€â”€ notebooks/                    # ğŸ”¬ ì‹¤ìŠµ ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ attention_tutorial.ipynb
â”‚
â”œâ”€â”€ tests/                        # ğŸ§ª í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_attention.py
â”‚
â””â”€â”€ 50_eval/                      # ğŸ¯ í‰ê°€/ë°ëª¨
    â”œâ”€â”€ attention_visualizer.py  # Attention ì‹œê°í™”
    â””â”€â”€ translation_toy.py       # ê°„ë‹¨í•œ ë²ˆì—­ íƒœìŠ¤í¬

core/                            # í•µì‹¬ êµ¬í˜„ (ìµœìƒìœ„)
â””â”€â”€ attention.py                 # Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
```

## ğŸ“‹ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Core ëª¨ë“ˆ (`core/attention.py`)
- [x] `scaled_dot_product_attention()`: ê¸°ë³¸ attention
- [x] `MultiHeadAttention` class: Multi-head êµ¬í˜„
  - [x] Q, K, V projection
  - [x] Head splitting/combining
  - [x] Output projection
- [x] `positional_encoding()`: Sinusoidal PE
- [x] `add_positional_encoding()`: PE ì¶”ê°€
- [x] `create_causal_mask()`: GPT ìŠ¤íƒ€ì¼ ë§ˆìŠ¤í‚¹
- [x] `create_padding_mask()`: íŒ¨ë”© ë§ˆìŠ¤í‚¹
- [x] `visualize_attention()`: ê°€ì¤‘ì¹˜ ì‹œê°í™”

### í…ŒìŠ¤íŠ¸ (`tests/test_attention.py`)
- [x] Scaled dot-product attention í…ŒìŠ¤íŠ¸
- [x] Multi-head attention í…ŒìŠ¤íŠ¸
- [x] Positional encoding í…ŒìŠ¤íŠ¸
- [x] Masking í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
- [x] Attention ì†ì„± ê²€ì¦

### í‰ê°€ (`50_eval/`)
- [x] Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”
  - [x] Self-attention íŒ¨í„´
  - [x] Causal attention íŒ¨í„´
  - [x] Multi-head ë¹„êµ
- [x] ê°„ë‹¨í•œ ë²ˆì—­ íƒœìŠ¤í¬
  - [x] Encoder-Decoder attention
  - [x] Cross-attention ì‹œê°í™”

## ğŸ“ í•µì‹¬ ê°œë…

### 1. Attention ìˆ˜ì‹
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V

Q: Query - ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?
K: Key - ë¬´ì—‡ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?
V: Value - ì‹¤ì œ ì •ë³´
```

### 2. Multi-Head Attention
```python
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### 3. Positional Encoding
```python
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

## ğŸ”¥ ë„ì „ ê³¼ì œ

### í•„ìˆ˜ ê³¼ì œ
1. âœ… Self-Attention êµ¬í˜„ ë° ì‹œê°í™”
2. âœ… Multi-Head Attention êµ¬í˜„
3. âœ… Causal masking ì ìš©
4. âœ… Positional encoding ì¶”ê°€

### ì„ íƒ ê³¼ì œ
1. â¬œ Relative positional encoding
2. â¬œ Efficient attention (Linear complexity)
3. â¬œ Flash Attention ì´í•´
4. â¬œ RoPE (Rotary PE) êµ¬í˜„

## ğŸ“Š ì´í•´ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë… ì´í•´
- [ ] Q, K, Vì˜ ì—­í• ì„ ëª…í™•íˆ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] Scaling factor (âˆšd_k)ê°€ í•„ìš”í•œ ì´ìœ ë¥¼ ì•ˆë‹¤
- [ ] Multi-Headê°€ Single-Headë³´ë‹¤ ì¢‹ì€ ì´ìœ ë¥¼ ì•ˆë‹¤
- [ ] Positional Encodingì´ í•„ìš”í•œ ì´ìœ ë¥¼ ì•ˆë‹¤
- [ ] Self-Attention vs Cross-Attention ì°¨ì´ë¥¼ ì•ˆë‹¤

### êµ¬í˜„ ëŠ¥ë ¥
- [ ] NumPyë¡œ Attentionì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- [ ] Attention weightsë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤
- [ ] Causal maskë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] Multi-Headë¡œ í™•ì¥í•  ìˆ˜ ìˆë‹¤
- [ ] PEë¥¼ ì¶”ê°€í•˜ê³  íš¨ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤

## ğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸

### Attentionì˜ ì¥ì 
1. **ë³‘ë ¬ ì²˜ë¦¬**: RNNê³¼ ë‹¬ë¦¬ ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ì²˜ë¦¬
2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ê±°ë¦¬ì™€ ë¬´ê´€í•˜ê²Œ ì •ë³´ ì „ë‹¬
3. **í•´ì„ ê°€ëŠ¥ì„±**: Attention ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ì´í•´
4. **íš¨ìœ¨ì„±**: í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ GPU ìµœì í™”

### Multi-Headì˜ ì´ì 
1. **ë‹¤ì–‘ì„±**: ê° headê°€ ë‹¤ë¥¸ íŒ¨í„´ í•™ìŠµ
2. **ì•™ìƒë¸” íš¨ê³¼**: ì—¬ëŸ¬ ê´€ì  ë™ì‹œ ê³ ë ¤
3. **í‘œí˜„ë ¥**: ë” í’ë¶€í•œ íŠ¹ì§• ì¶”ì¶œ

### Positional Encoding
1. **ìˆœì„œ ì •ë³´**: Attentionì€ ìˆœì„œë¥¼ ëª¨ë¦„
2. **Sinusoidal**: í•™ìŠµ ë¶ˆí•„ìš”, ì„ì˜ ê¸¸ì´ ì²˜ë¦¬
3. **ìµœì‹  ê¸°ë²•**: RoPE, ALiBi ë“±

## ğŸ’¡ êµ¬í˜„ íŒ

1. **ìˆ˜ì¹˜ ì•ˆì •ì„±**
   ```python
   # Softmax ì „ ìµœëŒ“ê°’ ë¹¼ê¸°
   scores = scores - np.max(scores, axis=-1, keepdims=True)
   ```

2. **íš¨ìœ¨ì ì¸ í–‰ë ¬ ì—°ì‚°**
   ```python
   # ë°°ì¹˜ ì²˜ë¦¬
   # (batch, seq, d) @ (batch, d, seq) = (batch, seq, seq)
   ```

3. **Shape ë””ë²„ê¹…**
   ```python
   print(f"Q: {Q.shape}, K: {K.shape}, V: {V.shape}")
   print(f"Attention: {attention.shape}")
   ```

## ğŸ› ì¼ë°˜ì ì¸ ì‹¤ìˆ˜

1. **Scaling ë¹¼ë¨¹ê¸°**: âˆšd_kë¡œ ë‚˜ëˆ„ì§€ ì•Šìœ¼ë©´ gradient ë¬¸ì œ
2. **ì°¨ì› í˜¼ë™**: seq_len vs d_model êµ¬ë¶„
3. **Mask ì ìš© ì˜¤ë¥˜**: -inf ëŒ€ì‹  í° ìŒìˆ˜ ì‚¬ìš©

## ğŸ“š ì°¸ê³  ìë£Œ

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [Visualizing Attention](https://distill.pub/2016/augmented-rnns/)

## âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ (`make test`)
- [ ] Attention ì‹œê°í™” ì‹¤í–‰ (`make visualize`)
- [ ] ë²ˆì—­ ë°ëª¨ ì‹¤í–‰ (`make translate`)
- [ ] í•™ìŠµ ë…¸íŠ¸ ì™„ë… ë° ì´í•´
- [ ] ë…¸íŠ¸ë¶ íŠœí† ë¦¬ì–¼ ì™„ë£Œ

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

Day 3ë¥¼ ì™„ë£Œí•˜ë©´ Day 4 (Transformer Block)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
ì—¬ëŸ¬ Attention ë ˆì´ì–´ë¥¼ ìŒ“ì•„ ì™„ì „í•œ Transformerë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤!