# ğŸ“… ì¼ì¼ ì‹¤ìŠµ ê³„íš

## Week 1: ê¸°ì´ˆ ë‹¤ì§€ê¸°

### Day 1 (ì›”): ë…¸íŠ¸ë¶ ì²« ì‹¤í–‰
**ëª©í‘œ**: ì „ì²´ íë¦„ íŒŒì•…

**ì˜¤ì „ (30ë¶„)**
```python
# 1. Jupyter ë…¸íŠ¸ë¶ ì—´ê¸°
jupyter notebook tiny_autograd_tutorial.ipynb

# 2. ëª¨ë“  ì…€ ì‹¤í–‰ (Run All)
# 3. ê²°ê³¼ ê´€ì°°
```

**ì˜¤í›„ (30ë¶„)**
- ì´í•´ ì•ˆ ë˜ëŠ” ë¶€ë¶„ ë©”ëª¨
- `study_notes/my_questions.md` íŒŒì¼ ìƒì„±
- 3ê°€ì§€ ì£¼ìš” ì§ˆë¬¸ ì‘ì„±

### Day 2 (í™”): Value í´ë˜ìŠ¤ ë¶„ì„
**ëª©í‘œ**: ë°ì´í„° êµ¬ì¡° ì´í•´

**ì‹¤ìŠµ**
```python
# ìƒˆ ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í—˜
from _10_core.autograd_tiny.value import Value

# 1. ê°„ë‹¨í•œ ì—°ì‚°
a = Value(2.0)
b = Value(3.0)
c = a + b

# 2. ì†ì„± í™•ì¸
print(f"c.data = {c.data}")
print(f"c._prev = {c._prev}")
print(f"c._op = {c._op}")

# 3. ê·¸ë˜í”„ êµ¬ì¡° ì´í•´
def print_graph(v, level=0):
    print("  " * level + f"Value({v.data}, op={v._op})")
    for child in v._prev:
        print_graph(child, level + 1)

print_graph(c)
```

### Day 3 (ìˆ˜): Forward Pass ì´í•´
**ëª©í‘œ**: ì—°ì‚° ê·¸ë˜í”„ êµ¬ì¶• ê³¼ì •

**ì‹¤ìŠµ ê³¼ì œ**
```python
# ë³µì¡í•œ í•¨ìˆ˜ ë§Œë“¤ê¸°
x = Value(2.0)
y = Value(3.0)

# TODO: ë‹¤ìŒ í•¨ìˆ˜ êµ¬í˜„
# f(x,y) = xÂ² + 2xy + yÂ²
# íŒíŠ¸: (x+y)Â² = xÂ² + 2xy + yÂ²

z = # ì—¬ê¸° êµ¬í˜„

print(f"Result: {z.data}")
# Expected: 25.0 (5Â²)
```

### Day 4 (ëª©): Backward Pass ì´í•´
**ëª©í‘œ**: ì—­ì „íŒŒ ê³¼ì • ì¶”ì 

**ë””ë²„ê¹… ì‹¤ìŠµ**
```python
# backward() ê³¼ì • ì¶”ì 
class DebugValue(Value):
    def backward(self):
        print("=== Starting Backward Pass ===")
        topo = []
        visited = set()
        
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                print(f"Visiting: Value({v.data})")
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        
        build_topo(self)
        print(f"\nTopological order: {[v.data for v in topo]}")
        
        self.grad = 1.0
        for v in reversed(topo):
            print(f"\nProcessing: Value({v.data})")
            print(f"  Before: grad={v.grad}")
            v._backward()
            print(f"  After: grad={v.grad}")

# í…ŒìŠ¤íŠ¸
x = DebugValue(2.0)
y = DebugValue(3.0)
z = x * y
z.backward()
```

### Day 5 (ê¸ˆ): ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë¹„êµ
**ëª©í‘œ**: êµ¬í˜„ ê²€ì¦

**ê²€ì¦ ì½”ë“œ**
```python
def numerical_diff(f, x, h=1e-5):
    """ìˆ˜ì¹˜ ë¯¸ë¶„ ê³„ì‚°"""
    return (f(x + h) - f(x - h)) / (2 * h)

def verify_gradient(func_name, func, x_val):
    """ìë™ë¯¸ë¶„ê³¼ ìˆ˜ì¹˜ë¯¸ë¶„ ë¹„êµ"""
    # ìë™ë¯¸ë¶„
    x = Value(x_val)
    y = func(x)
    y.backward()
    auto_grad = x.grad
    
    # ìˆ˜ì¹˜ë¯¸ë¶„
    num_grad = numerical_diff(lambda v: func(Value(v)).data, x_val)
    
    # ë¹„êµ
    error = abs(auto_grad - num_grad)
    print(f"{func_name}:")
    print(f"  Auto: {auto_grad:.6f}")
    print(f"  Numerical: {num_grad:.6f}")
    print(f"  Error: {error:.2e}")
    print(f"  âœ… Pass" if error < 1e-4 else f"  âŒ Fail")

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
verify_gradient("xÂ²", lambda x: x * x, 3.0)
verify_gradient("xÂ³", lambda x: x * x * x, 2.0)
verify_gradient("tanh(x)", lambda x: x.tanh(), 0.5)
```

### Weekend Project: ë¯¸ë‹ˆ ì‹ ê²½ë§
```python
# ì£¼ë§ ê³¼ì œ: 1ê°œ ë‰´ëŸ° êµ¬í˜„
class SimpleNeuron:
    def __init__(self):
        self.w = Value(0.5)  # weight
        self.b = Value(0.1)  # bias
    
    def forward(self, x):
        # TODO: wx + b êµ¬í˜„
        pass
    
    def train_step(self, x_data, y_target, lr=0.01):
        # Forward
        y_pred = self.forward(Value(x_data))
        
        # Loss (MSE)
        loss = (y_pred - Value(y_target)) ** 2
        
        # Backward
        loss.backward()
        
        # Update (gradient descent)
        self.w.data -= lr * self.w.grad
        self.b.data -= lr * self.b.grad
        
        # Reset gradients
        self.w.grad = 0
        self.b.grad = 0
        
        return loss.data

# ì„ í˜• í•¨ìˆ˜ í•™ìŠµ: y = 2x + 1
neuron = SimpleNeuron()
for epoch in range(100):
    loss = neuron.train_step(1.0, 3.0)  # x=1, y=3
    if epoch % 20 == 0:
        print(f"Epoch {epoch}: Loss={loss:.4f}, w={neuron.w.data:.2f}, b={neuron.b.data:.2f}")
```

## Week 2: ì‹¬í™” í•™ìŠµ

### Day 6-7: ìƒˆë¡œìš´ ì—°ì‚° ì¶”ê°€
```python
# êµ¬í˜„í•  ì—°ì‚°ë“¤
1. __sub__ (ë¹¼ê¸°)
2. __truediv__ (ë‚˜ëˆ„ê¸°)
3. __pow__ (ê±°ë“­ì œê³±)
4. sigmoid í™œì„±í™” í•¨ìˆ˜
5. log, exp í•¨ìˆ˜
```

### Day 8-9: ë³µì¡í•œ í•¨ìˆ˜
```python
# êµ¬í˜„í•  í•¨ìˆ˜ë“¤
1. Rosenbrock: f(x,y) = (1-x)Â² + 100(y-xÂ²)Â²
2. Himmelblau: f(x,y) = (xÂ²+y-11)Â² + (x+yÂ²-7)Â²
3. Beale: f(x,y) = (1.5-x+xy)Â² + (2.25-x+xyÂ²)Â² + (2.625-x+xyÂ³)Â²
```

### Day 10: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
```python
class SGD:
    """Stochastic Gradient Descent"""
    def __init__(self, params, lr=0.01):
        self.params = params
        self.lr = lr
    
    def step(self):
        for p in self.params:
            p.data -= self.lr * p.grad
    
    def zero_grad(self):
        for p in self.params:
            p.grad = 0

# ì‚¬ìš© ì˜ˆì‹œ
params = [w1, w2, b]
optimizer = SGD(params, lr=0.001)

for epoch in range(100):
    # Forward
    loss = compute_loss()
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    
    # Update
    optimizer.step()
```

## Week 3: í”„ë¡œì íŠ¸

### Mini Project 1: XOR ë¬¸ì œ
```python
# 2ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ XOR í•´ê²°
# Input: (0,0), (0,1), (1,0), (1,1)
# Output: 0, 1, 1, 0
```

### Mini Project 2: íšŒê·€ ë¬¸ì œ
```python
# sin í•¨ìˆ˜ ê·¼ì‚¬
# ë°ì´í„° ìƒì„±
import numpy as np
X = np.linspace(-np.pi, np.pi, 100)
Y = np.sin(X)

# 3ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµ
```

### Mini Project 3: ë¶„ë¥˜ ë¬¸ì œ
```python
# 2D ë‚˜ì„ í˜• ë°ì´í„° ë¶„ë¥˜
# 2ê°œ í´ë˜ìŠ¤, ë‚˜ì„ í˜•ìœ¼ë¡œ ë¶„í¬
```

## ğŸ“Š ì§„ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ ì´í•´ ê°œë…
- [ ] Forward passì™€ ì—°ì‚° ê·¸ë˜í”„
- [ ] Chain ruleì˜ ì›ë¦¬
- [ ] Backward passì™€ gradient ì „íŒŒ
- [ ] ìœ„ìƒì •ë ¬ì˜ í•„ìš”ì„±
- [ ] Gradient ëˆ„ì  (+=) ì´ìœ 

### êµ¬í˜„ ëŠ¥ë ¥
- [ ] Value í´ë˜ìŠ¤ ì²˜ìŒë¶€í„° êµ¬í˜„
- [ ] ê¸°ë³¸ ì—°ì‚° 4ê°œ êµ¬í˜„
- [ ] í™œì„±í™” í•¨ìˆ˜ 2ê°œ êµ¬í˜„
- [ ] backward() ë©”ì„œë“œ êµ¬í˜„
- [ ] ê°„ë‹¨í•œ ìµœì í™” êµ¬í˜„

### ì‘ìš© ëŠ¥ë ¥
- [ ] ë³µì¡í•œ í•¨ìˆ˜ ë¯¸ë¶„
- [ ] ë‰´ëŸ° í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„
- [ ] í•™ìŠµ ë£¨í”„ ì‘ì„±
- [ ] ìˆ˜ë ´ í™•ì¸

## ğŸ¯ ìµœì¢… ëª©í‘œ

**3ì£¼ í›„ ë„ë‹¬ ëª©í‘œ:**
1. âœ… Autograd ì™„ì „ ì´í•´
2. âœ… ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬í˜„ ê°€ëŠ¥
3. âœ… XOR ë¬¸ì œ í•´ê²°
4. âœ… PyTorch autogradì™€ ë¹„êµ ê°€ëŠ¥
5. âœ… ë‚˜ë§Œì˜ ë¯¸ë‹ˆ í”„ë ˆì„ì›Œí¬ ì œì‘

**í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸:**
- GitHubì— ì •ë¦¬ëœ ì½”ë“œ
- í•™ìŠµ ê³¼ì • ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- êµ¬í˜„í•œ ì‹ ê²½ë§ ë°ëª¨
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼