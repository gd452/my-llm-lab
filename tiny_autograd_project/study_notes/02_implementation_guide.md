# ğŸ› ï¸ Tiny Autograd êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“ í•™ìŠµ ì¼ì§€ í…œí”Œë¦¿

### Day 1: ì „ì²´ êµ¬ì¡° íŒŒì•…
**ë‚ ì§œ**: ___________

**í•™ìŠµ ëª©í‘œ**:
- [ ] ë…¸íŠ¸ë¶ ì „ì²´ ì‹¤í–‰
- [ ] Value í´ë˜ìŠ¤ êµ¬ì¡° ì´í•´
- [ ] ì—°ì‚° ê·¸ë˜í”„ ê°œë… íŒŒì•…

**í•µì‹¬ ê°œë…**:
```python
# ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ì½”ë“œ
```

**ì§ˆë¬¸/ì˜ë¬¸ì **:
1. 
2. 

**ë‚´ì¼ í•  ì¼**:

---

## ğŸ”¬ ë‹¨ê³„ë³„ êµ¬í˜„ ì‹¤ìŠµ

### Step 1: ìµœì†Œ êµ¬í˜„ (Minimal Value)
```python
class SimpleValue:
    """ê°€ì¥ ê°„ë‹¨í•œ Value êµ¬í˜„"""
    def __init__(self, data):
        self.data = data
        self.grad = 0
    
    def __repr__(self):
        return f"Value({self.data})"

# í…ŒìŠ¤íŠ¸
v = SimpleValue(3.0)
print(v)  # Value(3.0)
```

### Step 2: ë§ì…ˆ ì¶”ê°€
```python
class Value:
    def __init__(self, data):
        self.data = data
        self.grad = 0
        self._backward = lambda: None
        self._prev = set()
    
    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data)
        out._prev = {self, other}
        
        def _backward():
            self.grad += out.grad  # âˆ‚out/âˆ‚self = 1
            other.grad += out.grad  # âˆ‚out/âˆ‚other = 1
        
        out._backward = _backward
        return out

# í…ŒìŠ¤íŠ¸
a = Value(2)
b = Value(3)
c = a + b
print(c.data)  # 5
```

### Step 3: ì—­ì „íŒŒ êµ¬í˜„
```python
def backward(self):
    # ìœ„ìƒì •ë ¬
    topo = []
    visited = set()
    
    def build_topo(v):
        if v not in visited:
            visited.add(v)
            for child in v._prev:
                build_topo(child)
            topo.append(v)
    
    build_topo(self)
    
    # ì—­ì „íŒŒ
    self.grad = 1.0
    for v in reversed(topo):
        v._backward()

Value.backward = backward

# í…ŒìŠ¤íŠ¸
a = Value(2)
b = Value(3)
c = a + b
c.backward()
print(f"a.grad={a.grad}, b.grad={b.grad}")  # 1, 1
```

## ğŸ“Š ì‹¤ìŠµ í”„ë¡œì íŠ¸

### Project 1: ê°„ë‹¨í•œ í•¨ìˆ˜ ë¯¸ë¶„
```python
# ëª©í‘œ: f(x) = xÂ² + 2x + 1 ì˜ x=3ì—ì„œ ë¯¸ë¶„ê°’ êµ¬í•˜ê¸°
# ë‹µ: f'(x) = 2x + 2, f'(3) = 8

x = Value(3.0)
# TODO: í•¨ìˆ˜ êµ¬í˜„
# f = ...
# f.backward()
# assert abs(x.grad - 8.0) < 1e-6
```

### Project 2: 2ë³€ìˆ˜ í•¨ìˆ˜
```python
# ëª©í‘œ: f(x,y) = x*y + x ì˜ í¸ë¯¸ë¶„
# âˆ‚f/âˆ‚x = y + 1, âˆ‚f/âˆ‚y = x

def test_two_vars():
    x = Value(2.0)
    y = Value(3.0)
    # TODO: êµ¬í˜„
    pass
```

### Project 3: ë³µì¡í•œ í•¨ìˆ˜
```python
# ëª©í‘œ: f(x,y) = tanh(x*y) * (x + y)

def complex_function():
    x = Value(1.0)
    y = Value(2.0)
    # TODO: êµ¬í˜„
    pass
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‘ì„±í•˜ê¸°

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿
```python
import math

def test_addition():
    """ë§ì…ˆ í…ŒìŠ¤íŠ¸"""
    a = Value(2.0)
    b = Value(3.0)
    c = a + b
    c.backward()
    
    assert c.data == 5.0
    assert a.grad == 1.0
    assert b.grad == 1.0
    print("âœ… Addition test passed")

def test_multiplication():
    """ê³±ì…ˆ í…ŒìŠ¤íŠ¸"""
    a = Value(2.0)
    b = Value(3.0)
    c = a * b
    c.backward()
    
    assert c.data == 6.0
    assert a.grad == 3.0  # âˆ‚(a*b)/âˆ‚a = b
    assert b.grad == 2.0  # âˆ‚(a*b)/âˆ‚b = a
    print("âœ… Multiplication test passed")

def test_tanh():
    """tanh í…ŒìŠ¤íŠ¸"""
    x = Value(0.5)
    y = x.tanh()
    y.backward()
    
    expected_grad = 1 - math.tanh(0.5)**2
    assert abs(x.grad - expected_grad) < 1e-6
    print("âœ… Tanh test passed")

# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    test_addition()
    test_multiplication()
    test_tanh()
    print("ğŸ‰ All tests passed!")
```

## ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„

### ì‹œê°„ ë³µì¡ë„ ë¶„ì„
```python
import time

def benchmark_operations(n):
    """nê°œì˜ ì—°ì‚° ë²¤ì¹˜ë§ˆí¬"""
    
    # ì„ í˜• ê·¸ë˜í”„
    start = time.time()
    x = Value(1.0)
    for _ in range(n):
        x = x + 1
    x.backward()
    linear_time = time.time() - start
    
    # ì´ì§„ íŠ¸ë¦¬ ê·¸ë˜í”„
    start = time.time()
    values = [Value(1.0) for _ in range(n)]
    while len(values) > 1:
        new_values = []
        for i in range(0, len(values)-1, 2):
            new_values.append(values[i] + values[i+1])
        if len(values) % 2 == 1:
            new_values.append(values[-1])
        values = new_values
    values[0].backward()
    tree_time = time.time() - start
    
    print(f"Linear graph ({n} ops): {linear_time:.4f}s")
    print(f"Tree graph ({n} ops): {tree_time:.4f}s")

# í…ŒìŠ¤íŠ¸
benchmark_operations(100)
benchmark_operations(1000)
```

## ğŸ¯ ì²´í¬í¬ì¸íŠ¸

### Week 1 ëª©í‘œ
- [ ] Value í´ë˜ìŠ¤ ì™„ì „ ì´í•´
- [ ] 4ê°€ì§€ ê¸°ë³¸ ì—°ì‚° êµ¬í˜„ (+, -, *, /)
- [ ] backward() ë©”ì„œë“œ ì´í•´
- [ ] ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë¹„êµ ê²€ì¦

### Week 2 ëª©í‘œ
- [ ] í™œì„±í™” í•¨ìˆ˜ 3ê°œ ì¶”ê°€ (sigmoid, relu, leaky_relu)
- [ ] ë³µì¡í•œ í•¨ìˆ˜ 5ê°œ í…ŒìŠ¤íŠ¸
- [ ] ê·¸ë˜í”„ ì‹œê°í™” êµ¬í˜„
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

### Week 3 ëª©í‘œ
- [ ] Neuron í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Layer í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ê°„ë‹¨í•œ MLP êµ¬í˜„
- [ ] XOR ë¬¸ì œ í•´ê²°

## ğŸ’¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### 1. TypeError: unhashable type
```python
# ë¬¸ì œ: Valueë¥¼ setì— ë„£ì„ ìˆ˜ ì—†ìŒ
# í•´ê²°: __hash__ì™€ __eq__ êµ¬í˜„
def __hash__(self):
    return id(self)
```

#### 2. Gradientê°€ 0ìœ¼ë¡œ ìœ ì§€
```python
# ë¬¸ì œ: backward() í›„ì—ë„ gradê°€ 0
# ì²´í¬ì‚¬í•­:
# 1. out.grad = 1.0 ì„¤ì •í–ˆëŠ”ì§€
# 2. _backward í•¨ìˆ˜ê°€ ì œëŒ€ë¡œ ì—°ê²°ë˜ì—ˆëŠ”ì§€
# 3. += ë¡œ gradient ëˆ„ì í•˜ëŠ”ì§€
```

#### 3. ì˜ëª»ëœ gradient ê°’
```python
# ë¬¸ì œ: ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë‹¤ë¥¸ ê°’
# ë””ë²„ê¹…:
def debug_gradient():
    x = Value(2.0)
    y = x * x  # xÂ²
    y.backward()
    
    print(f"Autograd: {x.grad}")  # 4.0ì´ì–´ì•¼ í•¨
    
    # ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ í™•ì¸
    h = 1e-5
    numerical = ((2+h)**2 - (2-h)**2) / (2*h)
    print(f"Numerical: {numerical}")
```

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### ì½ê¸° ìë£Œ
1. **CS231n Backprop Notes**: [ë§í¬](http://cs231n.github.io/optimization-2/)
2. **PyTorch Autograd Tutorial**: [ë§í¬](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)
3. **Micrograd ì›ë³¸**: [GitHub](https://github.com/karpathy/micrograd)

### ë¹„ë””ì˜¤ ê°•ì˜
1. **Andrej Karpathy - micrograd**: 2ì‹œê°„ 30ë¶„
2. **3Blue1Brown - Neural Networks**: ì‹œë¦¬ì¦ˆ 4í¸
3. **Fast.ai - Lesson 8**: From scratch implementation

### ì—°ìŠµ ë¬¸ì œ ì‚¬ì´íŠ¸
1. **Brilliant.org**: ë¯¸ì ë¶„ ì¸í„°ë™í‹°ë¸Œ ì½”ìŠ¤
2. **Khan Academy**: ë‹¤ë³€ìˆ˜ ë¯¸ì ë¶„
3. **MIT OCW 18.06**: ì„ í˜•ëŒ€ìˆ˜

## ğŸ† ë§ˆì¼ìŠ¤í†¤

### Bronze ğŸ¥‰ (1ì£¼ì°¨)
- Value í´ë˜ìŠ¤ ì´í•´
- ê¸°ë³¸ ì—°ì‚° êµ¬í˜„
- ê°„ë‹¨í•œ í•¨ìˆ˜ ë¯¸ë¶„

### Silver ğŸ¥ˆ (2ì£¼ì°¨)
- ëª¨ë“  ì—°ì‚° êµ¬í˜„
- ë³µì¡í•œ í•¨ìˆ˜ ì²˜ë¦¬
- í…ŒìŠ¤íŠ¸ ì‘ì„±

### Gold ğŸ¥‡ (3ì£¼ì°¨)
- ì‹ ê²½ë§ êµ¬í˜„
- XOR í•´ê²°
- ìµœì í™” êµ¬í˜„

### Platinum ğŸ’ (4ì£¼ì°¨+)
- ë²¡í„° ì—°ì‚° í™•ì¥
- CNN/RNN êµ¬í˜„
- ìì²´ í”„ë ˆì„ì›Œí¬ ì œì‘